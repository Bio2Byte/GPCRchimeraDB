{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlottecrauwels/anaconda3/lib/python3.10/site-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests, sys\n",
    "import json\n",
    "import os\n",
    "import urllib.request\n",
    "from Bio import SeqIO, pairwise2, AlignIO\n",
    "import re\n",
    "import html\n",
    "from collections import defaultdict\n",
    "import subprocess\n",
    "from Bio.PDB.DSSP import make_dssp_dict\n",
    "import mapping_uniprot_pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download file with classification info\n",
    "filename_listGPCRdb = \"../data/250220_Classification_GPCRdb.xlsx\"\n",
    "listGPCRdb_df = pd.read_excel(filename_listGPCRdb)\n",
    "\n",
    "#chimeric design info\n",
    "filename_chimeric_designs = \"../data/previous_designs.xlsx\"\n",
    "chimeric_design_df = pd.read_excel(filename_chimeric_designs)\n",
    "\n",
    "#alignment all natural GPCRs\n",
    "MSA = \"../data/MSA_all_mammalian.fasta\"\n",
    "\n",
    "#chimeras\n",
    "chimeric_entry_data = \"../data/all_designs.fasta\"\n",
    "chimeras_record_dict = SeqIO.index(chimeric_entry_data, \"fasta\")\n",
    "\n",
    "# file with the representative experimental 3D structures (lowest resolution independent of activation state)\n",
    "# file generate with the notebook \"reference_structure.ipynb\" updated Feb 2025\n",
    "representative_structures_json = json.load(open(\"../data/250225_representative_structures_exp_pdbID_uniprotID.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "{'OPSD_BOVIN_ACM3_HUMAN_1': 'MNGTEGPNFYVPFSNKTGVVRSPFEAPQYYLAEPWQFSMLAAYMFLLIMLGFPINFLTLYVFKVNKQLKTVLNYILLNLAVADLFMVFGGFTTTLYTSLHGYFVFGPTGCNLEGFFATLGGEIALWSLVVLAIERYVVVTRPLTYRAKRTTKRAIMGVAFTWVMALACAAPPLVGWSRYIPEGMQCSCGIDYYTPHEETNNESFVIYMFVVHFIIPLIVIFFCYGRIYKETEKRTKELAGLQASGTEAETENFVHPTGSSRSCSSYELQQQSMKRSNRRKYGRCHFWFTTKSWKPSSEQMDQDHSSSDSWNNNDAAASLENSASSDEEDIGSETRAIYSIVLKLPGHSTILNSTKLPSSDNLQVPEEELGMVDLERKADKLQAQKSVDDGGSFPKSFSKLPIQLESAVDTAKTSDVNSSVGKSTATLPLSFKEATLAKRFALKTRSQITKRKRMSLVKEKKAARMVIIMVIAFLICWLPYAGVAFYIFTHQGSDFGPIFMTIPAFFAKTSAVYNPVIYIMMNKQFRTTFKMLLLCQCDKKKRRKQQYQQRQSVIFHKRAPEQALTETSQVAPA', 'OPRM_MOUSE_OPRK_HUMAN_1': 'MDSSAGPGNISDCSDPLAPASCSPAPGSWLNLSHVDGNQSDPCGPNRTGLGGSHSLCPQTGSPSMVTAITIMALYSIVCVVGLFGNFLVMYVIVRYTKMKTATNIYIFNLALADALATSTLPFQSVNYLMGTWPFGNILCKIVISIDYYNMFTSIFTLCTMSVDRYIAVCHPVKALDFRTPRNAKIVNVCNWILSSAIGLPVMFMATTKYRQGSIDCTLTFSHPTWYWENLLKICVFIFAFIMPVLIITVCYGLMILRLKSVRLLSGSREKDRNLRRITRMVLVVVAVFIVCWTPIHIYVIIKALITIPETTFQTVSWHFCIALGYTNSCLNPVLYAFLDENFKRCFREFCIPTSSTIEQQNSARIRQNTREHPSTANTVDRTNHQLENLEAETAPLP', 'NTR1_HUMAN_OPRK_HUMAN_1': 'MRLNSSAPGTPGTPAADPFQRAQAGLEEALLAPGFGNASGNASERVLAAPSSELDVNTDIYSKVLVTAVYLALFVVGTVGNTVTLFTLARKKSLQSLQSTVHYHLGSLALSDLLTLLLAMPVELYNFIWVHHPWAFGDAGCRGYYFLRDACTYATALNVASLSVERYLAICHPFKAKTLMSRSRTKKFISAIWLASALLAVPMLFTMGEQNRSADGQHAGGLVCTPTIHTATVKVVIQVNTFMSFIFPMVVISVLYTLMILRLKSVRLLSGSREKDRNLRRITRLVLAVVIAFVVCWLPYHVRRLMFCYISDEQWTPFLYDFYHYFYMVTNALFYVSSTINPILYNLVSANFRHIFLATLACLCPVWRRRRKRPAFSRKADSVSSNHTLSSNATRETLY', 'SSR2_HUMAN_OPRK_HUMAN_1': 'MDMADEPLNGSHTWLSIPFDLNGSVVSTNTSNQTEPYYDLTSNAVLTFIYFVVCIIGLCGNTLVIYVILRYAKMKTITNIYILNLAIADELFMLGLPFLAMQVALVHWPFGKAICRVVMTVDGINQFTSIFCLTVMSIDRYLAVVHPIKSAKWRRPRTAKMITMAVWGVSLLVILPIMIYAGLRSNQWGRSSCTINWPGESGAWYTGFIIYTFILGFLVPLTIICLCYLFIIIKVKSVRLLSGSREKDRNLRRVTRMVSIVVAVFIFCWLPFYIFNVSSVSMAISPTPALKGMFDFVVVLTYANSCANPILYAFLSDNFKKSFQNVLCLVKVSGTDDGERSDSKQDKSRLNETTETQRTLLNGDLQTSI', 'HRH2_HUMAN_OPRK_HUMAN_1': 'MAPNGTASSFCLDSTACKITITVVLAVLILITVAGNVVVCLAVGLNRRLRNLTNCFIVSLAITDLLLGLLVLPFSAIYQLSCKWSFGKVFCNIYTSLDVMLCTASILNLFMISLDRYCAVMDPLRYPVLVTPVRVAISLVLIWVISITLSFLSIHLGWNSRNETSKGNHTTSKCKVQVNEVYGLVDGLVTFYLPLLIMCVCYTLMILRLKSVRLLSGSREKDRNLRRITRLVLVVVAVFVICWFPYFTAFVYRGLRGDDAINEVLEAIVLWLGYANSALNPILYAALNRDFRTGYQQLFCCRLANRNSHKTSLRSNASQLSRTQSREPRQQEEKPLKLQVWSGTEVTAPQGATDR', 'ADA1A_HUMAN_OPRK_HUMAN_1': 'MVFLSGQASDSSQCTQPPAPVQISKAILLGVILGGLILFGVLGNILVILSVACHRHLHSVTHYYIVNLAVADLLLTSTVLPFSAIFEVLGYWAFGRVFCNIWAAVDVLCCTARIWGLCIISIDRYIGVSYPLRYPTIVTQRRGLMALLCVWALSLVISIGPLFGWRQPAPEDETICQINEEPGYVLFSALGSFYLPLAIILVMYTLMILRLKSVRLLSGSREKDRNLRRITRLVLIVVGCFVLCWLPFFLVMPIGSFFPDFKPSETVFKIVFWLGYLNSCINPIIYPCSSQEFKKAFQNVLRIQCLCRKQSSKHALGYTLHPPSQAVEGQHKDMVRIPVGSRETFYRISKTDGVCEWKFFSSMPRGSARITVSKDQSSCTTARVRSKSFLQVCCCVGPSTPSLDKNHQVPTIKVHTISLSENGEEV', 'CXCR3_HUMAN_OPRK_HUMAN_1': 'MVLEVSDHQVLNDAEVAALLENFSSSYDYGENESDSCCTSPPCPQDFSLNFDRAFLPALYSLLFLLGLLGNGAVAAVLLSRRTALSSTDTFLLHLAVADTLLVLTLPLWAVDAAVQWVFGSGLCKVAGALFNINFYAGALLLACISFDRYLNIVHATQLYRRGPPARVTLTCLAVWGLCLLFALPDFIFLSAHHDERLNATHCQYNFPQVGRTALRVLQLVAGFLLPLLVMAYCYAHILARLKSVRLLSGSREKDRNLRRITRLVVVVVVAFALCWTPYHLVVLVDILMDLGALARNCGRESRVDVAKSVTSGLGYMHCCLNPLLYAFVGVKFRERMWMLLLRLGCPNQRGLQRQPSSSRRDSSWSETSEASYSGL', 'ADA1A_HUMAN_OPRK_HUMAN_2': 'MVFLSGQASDSSQCTQPPAPVQISKAILLGVILGGLILFGVLGNILVILSVACHRHLHSVTHYYIVNLAVADLLLTSTVLPFSAIFEVLGYWAFGRVFCNIWAAVDVLCCTARIWGLCIISIDRYIGVSYPLRYPTIVTQRRGLMALLCVWALSLVISIGPLFGWRQPAPEDETICQINEEPGYVLFSALGSFYLPLAIILVMYCRVYVVAKRVRLLSGSREKDRNLRKAAKTLGIVVGCFVLCWLPFFLVMPIGSFFPDFKPSETVFKIVFWLGYLNSCINPIIYPCSSQEFKKAFQNVLRIQCLCRKQSSKHALGYTLHPPSQAVEGQHKDMVRIPVGSRETFYRISKTDGVCEWKFFSSMPRGSARITVSKDQSSCTTARVRSKSFLQVCCCVGPSTPSLDKNHQVPTIKVHTISLSENGEEV'}\n"
     ]
    }
   ],
   "source": [
    "#upload data\n",
    "of_interest=['OPRM_MOUSE_OPRK_HUMAN_1',\"NTR1_HUMAN_OPRK_HUMAN_1\",\"SSR2_HUMAN_OPRK_HUMAN_1\",\"HRH2_HUMAN_OPRK_HUMAN_1\",\"ADA1A_HUMAN_OPRK_HUMAN_1\",\"CXCR3_HUMAN_OPRK_HUMAN_1\",\n",
    "             \"ADA1A_HUMAN_OPRK_HUMAN_2\",\"OPSD_BOVIN_ACM3_HUMAN_1\"]\n",
    "\n",
    "entry_data = \"../data/all_designs.fasta\"\n",
    "entry_uniprotID_seq = {}\n",
    "for record in SeqIO.parse(entry_data,\"fasta\"):\n",
    "    if record.id in of_interest:\n",
    "        entry_uniprotID_seq[record.id]=str(record.seq)\n",
    "\n",
    "print(len(entry_uniprotID_seq))\n",
    "print(entry_uniprotID_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sq_atom_distance(i, j):\n",
    "    \"\"\"Squared euclidean distance between two 3d points\"\"\"\n",
    "    return (i[0] - j[0]) * (i[0] - j[0]) + \\\n",
    "            (i[1] - j[1]) * (i[1] - j[1]) + \\\n",
    "            (i[2] - j[2]) * (i[2] - j[2])\n",
    "\n",
    "def identify_gaps(pdb_file, chain_pdb, offset, end): #code modified from pdb_gap.py file from pdbtools Copyright 2018 João Pedro Rodrigues\n",
    "    fhandle = open(pdb_file, 'r')\n",
    "    centroid = ' CA '  # respect spacing. 'CA  ' != ' CA '\n",
    "    distance_threshold = 4.0 * 4.0\n",
    "    prev_at = (None, None, None, None, (None, None, None))\n",
    "    model = 0\n",
    "    n_gaps = 0\n",
    "    gap = []\n",
    "    for line in fhandle:\n",
    "\n",
    "        if line.startswith('MODEL'):\n",
    "            model = int(line[10:14])\n",
    "\n",
    "        elif line.startswith('ATOM'):\n",
    "            atom_name = line[12:16]\n",
    "            if atom_name != centroid:\n",
    "                continue\n",
    "\n",
    "            resn = line[17:20]\n",
    "            resi = int(line[22:26])\n",
    "            chain = line[21]\n",
    "            x = float(line[30:38])\n",
    "            y = float(line[38:46])\n",
    "            z = float(line[46:54])\n",
    "\n",
    "            at_uid = (model, chain, resi, resn, atom_name, (x, y, z))\n",
    "            if prev_at[0] == at_uid[0] and prev_at[1] == at_uid[1]:\n",
    "                d = calculate_sq_atom_distance(at_uid[5], prev_at[5])\n",
    "                if d > distance_threshold:\n",
    "                    gap.append([prev_at[1],prev_at[2],at_uid[1],at_uid[2]])\n",
    "                    # sys.stdout.write(fmt_GAPd.format(prev_at, at_uid, d**0.5))\n",
    "                    n_gaps += 1\n",
    "                elif prev_at[2] + 1 != at_uid[2]:\n",
    "                    # sys.stdout.write(fmt_GAPs.format(prev_at, at_uid))\n",
    "                    gap.append([prev_at[1],prev_at[2],at_uid[1],at_uid[2]])\n",
    "                    n_gaps += 1\n",
    "\n",
    "            prev_at = at_uid\n",
    "\n",
    "    gaps_cleaned = []\n",
    "    start = offset\n",
    "    for section in gap:\n",
    "        if section[0] == chain_pdb and section[2] == chain_pdb:\n",
    "            stop = section[1]\n",
    "            if start < 1000 and stop < 1000:\n",
    "                gaps_cleaned.append([start,stop])\n",
    "            start = section[3]\n",
    "    gaps_cleaned.append([start,end])\n",
    "    return gaps_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_duplicates(dicts):\n",
    "    # Step 1: Group dictionaries by the 'start' key\n",
    "    grouped = defaultdict(list)\n",
    "    for d in dicts:\n",
    "        grouped[d['start']].append(d)\n",
    "    \n",
    "    result = []\n",
    "    conflicts = []\n",
    "\n",
    "    # Step 2: Process each group\n",
    "    for start, items in grouped.items():\n",
    "        if len(items) > 1:\n",
    "            # Check if all 'type' values are the same\n",
    "            types = set(d['type'] for d in items)\n",
    "            if len(types) == 1:\n",
    "                # Merge 'reference' values\n",
    "                merged_references = \"\"\n",
    "                for d in items:\n",
    "                    merged_references += d['description']\n",
    "                # Create a new dictionary with merged references\n",
    "                new_dict = items[0].copy()\n",
    "                new_dict['description'] = merged_references[:-1]\n",
    "                new_dict['reference'] = \"https://www.ebi.ac.uk/pdbe/pisa/\"\n",
    "                result.append(new_dict)\n",
    "            else:\n",
    "                # Print dictionaries with different 'type' values\n",
    "                for d in items:\n",
    "                    conflicts.append(d)\n",
    "        else:\n",
    "            result.append(items[0])\n",
    "    \n",
    "    return result, conflicts\n",
    "\n",
    "def retrieve_interacting_residues_PDB(pdb_id, chain_pdb,mapping_uniprot_PDB_dict,pdb_file_path):\n",
    "    mapping_PDB_uniprot = {v: k for k, v in mapping_uniprot_PDB_dict.items()} #gives position of a aligned res in unaligned seq\n",
    "\n",
    "    # retrieve the interacting residues in the PDBs from PISA, need to make sure it doesn't take into accound the interactions between 2 sym GPCRs\n",
    "    # interacting residues is defined by a bsa > 0\n",
    "    #https://github.com/PDBe-KB/pdbe-pisa-json/blob/main/PISA-APIs.ipynb\n",
    "\n",
    "    interacting_residues_list = []\n",
    "    binders_chain= []\n",
    "    # for pdb_id, chain_pdb,uniprot_pdb_start,pdb_start in zip(pdb_ids,chain_pdbs,uniprot_pdb_starts,pdb_starts):\n",
    "    # try: #when its just 1 chain or 1 chain and a ligand PISA doesn't work\n",
    "        \n",
    "    if \"a\"==\"a\":\n",
    "        \n",
    "        response = requests.get(f\"https://www.ebi.ac.uk/pdbe/api/pisa/assembly/{pdb_id.lower()}/1\")\n",
    "        interface_count = response.json()[pdb_id.lower()][\"assembly\"][\"interface_count\"]\n",
    "        for i in range(1,interface_count+1):\n",
    "            interacting_residues = []\n",
    "            response_single_interface = requests.get(f\"https://www.ebi.ac.uk/pdbe/api/pisa/interface/{pdb_id.lower()}/1/{i}/\")\n",
    "            data = response_single_interface.json()\n",
    "            if \"/\" in chain_pdb:\n",
    "                chain_pdb = chain_pdb.split(\"/\")\n",
    "            for j in range(len(data[\"molecules\"])):\n",
    "                if isinstance(chain_pdb,str):\n",
    "                    if data[\"molecules\"][j][\"chain_id\"]==chain_pdb:\n",
    "                        for bsa,position in zip(data[\"molecules\"][j][\"buried_surface_areas\"],data[\"molecules\"][j]['residue_seq_ids']):\n",
    "                            if bsa > 0.0:\n",
    "                                try:\n",
    "                                    interacting_residues.append(mapping_PDB_uniprot[int(position)])\n",
    "                                except:\n",
    "                                    continue\n",
    "                        if j == 0: #there is supposed to be only 2 molecules, the GPCR and the interacting molecule\n",
    "                            chain_interacting_molecule = data[\"molecules\"][1][\"chain_id\"]\n",
    "                        else: \n",
    "                            chain_interacting_molecule = data[\"molecules\"][0][\"chain_id\"]\n",
    "                        binders_chain.append(extract_name_binders(chain_interacting_molecule,pdb_file_path))\n",
    "                        interacting_residues_list.append(list(set(interacting_residues)))\n",
    "\n",
    "                elif isinstance(chain_pdb,list):\n",
    "                    if chain_pdb[0] in data[\"molecules\"][j][\"chain_id\"] and chain_pdb[1] in data[\"molecules\"][j+1][\"chain_id\"]:\n",
    "                        break\n",
    "                    else:\n",
    "                        if chain_pdb[0] in data[\"molecules\"][j][\"chain_id\"] or chain_pdb[1] in data[\"molecules\"][j][\"chain_id\"]:\n",
    "                            for bsa,position in zip(data[\"molecules\"][j][\"buried_surface_areas\"],data[\"molecules\"][j]['residue_seq_ids']):\n",
    "                                if bsa >0.0:\n",
    "                                    try:\n",
    "                                        interacting_residues.append(mapping_PDB_uniprot[int(position)])\n",
    "                                    except:\n",
    "                                        continue\n",
    "                        if j == 0: #there is supposed to be only 2 molecules, the GPCR and the interacting molecule\n",
    "                            chain_interacting_molecule = data[\"molecules\"][1][\"chain_id\"]\n",
    "                        else: \n",
    "                            chain_interacting_molecule = data[\"molecules\"][0][\"chain_id\"]\n",
    "                        binders_chain.append(extract_name_binders(chain_interacting_molecule,pdb_file_path))\n",
    "                        interacting_residues_list.append(list(set(interacting_residues)))\n",
    "\n",
    "        return interacting_residues_list,binders_chain\n",
    "    # except:\n",
    "    #     return [],\"\"\n",
    "\n",
    "\n",
    "def extract_name_binders(chain_of_interest,pdb_file_path):\n",
    "\n",
    "    molecule_name = None\n",
    "    current_molecule = \"\"\n",
    "    reading_molecule = False\n",
    "    found_chain = False\n",
    "    \n",
    "    # Open and read the PDB file\n",
    "    with open(pdb_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith(\"COMPND\"):\n",
    "            # Start reading the molecule name if \"MOLECULE\" is in the line\n",
    "            if \"MOLECULE\" in line:\n",
    "                reading_molecule = True\n",
    "                current_molecule = line.split(\":\")[1].strip().rstrip(\";\")  # Extract initial part of the molecule name\n",
    "            # If the molecule name is being read and it continues on the next line\n",
    "            elif reading_molecule and \"CHAIN\" not in line:\n",
    "                pattern = r\"\\d+\\s+(.+)\"\n",
    "                match = re.search(pattern, line)\n",
    "                current_molecule += \" \"+match.group(1).strip().rstrip(\";\")\n",
    "            # Once we reach the chain of interest\n",
    "            if bool(re.search(rf\"CHAIN:\\s*(?:[^,]*,\\s*)*{chain_of_interest}\\b\", line)):\n",
    "                found_chain = True\n",
    "            # If molecule and chain have been found, stop reading\n",
    "            if found_chain and current_molecule and \";\" in line:\n",
    "                molecule_name = current_molecule\n",
    "                break\n",
    "    return molecule_name\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import PDB\n",
    "\n",
    "def extract_resolution_and_method(file_path):\n",
    "    \"\"\"\n",
    "    Extracts resolution and experimental method from a PDB or mmCIF file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the structure file (.pdb or .cif).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the resolution and experimental method.\n",
    "    \"\"\"\n",
    "    method_abb = {\"ELECTRON MICROSCOPY\":\"EM\",\"X-RAY DIFFRACTION\":\"X-RAY\",\"SOLUTION NMR\":\"NMR\", \"SOLID-STATE NMR\":\"NMR\",\n",
    "                  \"ELECTRON CRYSTALLOGRAPHY\":\"EM\"}\n",
    "    \n",
    "    if file_path.endswith(\".cif\"):\n",
    "        parser = PDB.MMCIFParser(QUIET=True)\n",
    "    elif file_path.endswith(\".pdb\"):\n",
    "        parser = PDB.PDBParser(QUIET=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a .pdb or .cif file.\")\n",
    "\n",
    "    structure = parser.get_structure(\"structure\", file_path)\n",
    "\n",
    "    # Extracting metadata from the structure header\n",
    "    header = structure.header\n",
    "\n",
    "    resolution = str(header.get(\"resolution\", \"Not available\"))+\"Å\"\n",
    "    experimental_method = method_abb[header.get(\"structure_method\", \"Not available\").upper()]\n",
    "    if experimental_method == \"NMR\":\n",
    "        resolution = \"\"\n",
    "\n",
    "    return resolution,experimental_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pdb_dsbonds_chimera_info(structures_pdb,uniprot_id,sequence):\n",
    "    structures = []\n",
    "    ds_bonds = []\n",
    "    pdbs = []\n",
    "    chains = []\n",
    "    uniprot_pdb_starts = []\n",
    "    pdb_starts = []\n",
    "    binders_list= []\n",
    "    interacting_residues_list = []\n",
    "\n",
    "    for pdb_info in structures_pdb:\n",
    "        pdb_id = pdb_info[\"PDB id\"]\n",
    "        chain_pdb = pdb_info[\"chain\"]\n",
    "        state = pdb_info[\"state\"]\n",
    "        if not os.path.exists('../data/tmp/'):\n",
    "            os.mkdir('../data/tmp/')\n",
    "        try:\n",
    "            mmcif=False\n",
    "            pdb_file_path = f'../data/tmp/{pdb_id}.pdb'\n",
    "            urllib.request.urlretrieve(f'https://files.rcsb.org/download/{pdb_id}.pdb', pdb_file_path)\n",
    "        except:\n",
    "            mmcif=True\n",
    "            # pdb_file_path = f'../data/tmp/{pdb_id}.cif'\n",
    "            # urllib.request.urlretrieve(f'https://files.rcsb.org/download/{pdb_id}.cif', pdb_file_path)\n",
    "        \n",
    "        if not mmcif:\n",
    "            #get uniprot to pdb mapping\n",
    "            folder_mapping_json = \"../examples/3Dstructures/uniprot_pdb_mapping/\"\n",
    "            mapping_file = folder_mapping_json+pdb_id+\".json\"\n",
    "            if not os.path.exists(folder_mapping_json+pdb_id+\".json\"):\n",
    "                mapping_uniprot_pdb_dict= mapping_uniprot_pdb.map_PDB_uniprot(pdb_id,pdb_file_path,chain_pdb,uniprot_id,sequence,folder_mapping_json, type_gpcr = \"chimera\")\n",
    "            else:\n",
    "                mapping_uniprot_pdb_dict = json.load(open(mapping_file))\n",
    "                mapping_uniprot_pdb_dict = {int(k): v for k, v in mapping_uniprot_pdb_dict.items()} #keys are strings\n",
    "\n",
    "\n",
    "            # #this is needed to find the interactions within the pdb file\n",
    "            pdbs.append(pdb_id)\n",
    "\n",
    "            #get offset and gaps in structure\n",
    "            resolution,method =  extract_resolution_and_method(pdb_file_path)\n",
    "            if mmcif:\n",
    "                url = f\"https://files.rcsb.org/download/{pdb_id}.cif\"\n",
    "            else:\n",
    "                url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\n",
    "            \n",
    "            structures.append({\"offset\":  0, \"gaps\": [],\"value\":pdb_id,\"chain\": chain_pdb, \"state\":state, \"mapping\": f\"file:///examples/3Dstructures/uniprot_pdb_mapping/{pdb_id}.json\", \"resolution\": resolution, \"method\": method, \"url\":url, \"reference\":f\"https://www.rcsb.org/structure/{pdb_id.upper()}\", \"date\":\"\"})\n",
    "            \n",
    "            #find interacting residues at ligand binding site and G protein binding site\n",
    "            interacting_residues,binders = retrieve_interacting_residues_PDB(pdb_id,chain_pdb,mapping_uniprot_pdb_dict,pdb_file_path)\n",
    "            if len(interacting_residues)>0:\n",
    "                interacting_residues_list.append(interacting_residues)\n",
    "                binders_list.append(binders)\n",
    "\n",
    "\n",
    "            #remove pdb file\n",
    "            os.remove(pdb_file_path)\n",
    "            \n",
    "    return structures, interacting_residues_list, binders_list, pdbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dssp(uniprotID, type =\"natural\"):\n",
    "\n",
    "    dssp_folder = \"../examples/3Dstructures/dssp/\"\n",
    "    dssp_filename = dssp_folder+uniprotID+\".dssp\"\n",
    "    remove_tmp = False\n",
    "    mapping_uniprot_pdb_numbering = None\n",
    "\n",
    "    if not os.path.exists(dssp_filename):\n",
    "        try: \n",
    "            pdb_representative = representative_structures_json[uniprotID][\"pdb_id\"]\n",
    "            gpcr_chain = representative_structures_json[uniprotID][\"gpcr_chain\"]\n",
    "            print(\"Problem, GPCR has exp structure and has no DSSP file\", pdb_representative, gpcr_chain)\n",
    "            #add chimera exp structure\n",
    "        except:\n",
    "            #AF\n",
    "            #check if gpcrdb has updated model\n",
    "            if type == \"natural\":\n",
    "                if os.path.exists(f\"../examples/3Dstructures/AF_gpcrdb_2024/{uniprotID}.pdb\"):\n",
    "                    input_file = f\"../examples/3Dstructures/AF_gpcrdb_2024/{uniprotID}.pdb\"\n",
    "                    gpcr_chain = \"A\"\n",
    "                else:\n",
    "                    #AlphaFold2 DB\n",
    "                    input_file = f'../examples/3Dstructures/tmp/{uniprotID}.pdb'\n",
    "                    urllib.request.urlretrieve(f\"https://alphafold.ebi.ac.uk/files/AF-{uniprotID}-F1-model_v4.pdb\", input_file)\n",
    "                    gpcr_chain = \"A\"\n",
    "                    remove_tmp = True\n",
    "            else:\n",
    "                if os.path.exists(f\"../examples/3Dstructures/AF_chimera_2024/{uniprotID}.pdb\"):\n",
    "                    input_file = f\"../examples/3Dstructures/AF_chimera_2024/{uniprotID}.pdb\"\n",
    "                    gpcr_chain = \"A\"\n",
    "            \n",
    "            subprocess.run(\n",
    "                    [\"mkdssp\", input_file, dssp_filename])\n",
    "                \n",
    "            if remove_tmp:\n",
    "                os.remove(input_file)\n",
    "        return dssp_filename, gpcr_chain, mapping_uniprot_pdb_numbering\n",
    "\n",
    "    else:\n",
    "        try: \n",
    "            pdb_representative = representative_structures_json[uniprotID][\"pdb_id\"]\n",
    "            gpcr_chain = representative_structures_json[uniprotID][\"gpcr_chain\"]\n",
    "            mapping_uniprot_pdb_numbering = json.load(open(\"../examples/3Dstructures/uniprot_pdb_mapping/\"+pdb_representative+\".json\"))\n",
    "            mapping_uniprot_pdb_numbering = {int(k): v for k, v in mapping_uniprot_pdb_numbering.items()} #keys are strings\n",
    "        except: #AF model\n",
    "            gpcr_chain = \"A\"\n",
    "        return dssp_filename, gpcr_chain, mapping_uniprot_pdb_numbering\n",
    "\n",
    "def find_closest(target,dict_interest):\n",
    "    if target in dict_interest:\n",
    "        min_start = dict_interest[target]\n",
    "    else:\n",
    "        closest_key = min(dict_interest.keys(), key=lambda x: abs(x - target))\n",
    "        min_start = dict_interest[closest_key]\n",
    "    return min_start\n",
    "    \n",
    "def remap_min_max_limits_prot_interest(min_max_limits,aligned_seq_interest):\n",
    "    translate_seq_MSA = map_seq_MSA(aligned_seq_interest) #gives position of a unaligned res in msa\n",
    "    translate_MSA_seq = {v: k for k, v in translate_seq_MSA.items()} #gives position of a aligned res in unaligned seq\n",
    "    min_max_limits_translated = {}\n",
    "    for TMname, limits in min_max_limits.items():\n",
    "        min_start = find_closest(limits[0][0],translate_MSA_seq)\n",
    "        max_start = find_closest(limits[0][1],translate_MSA_seq)\n",
    "        min_end = find_closest(limits[1][0],translate_MSA_seq)\n",
    "        max_end = find_closest(limits[1][1],translate_MSA_seq)\n",
    "        min_max_limits_translated[TMname]=[[min_start,max_start],[min_end,max_end]]\n",
    "    return min_max_limits_translated\n",
    "\n",
    "\n",
    "def refine_TM_regions(min_max_limits_translated,TM_regions_prot):\n",
    "\n",
    "    refined_TMs = {}\n",
    "    \n",
    "    # Step 1: Ensure each TM aligns with MSA-defined limits\n",
    "    for tm_label, (dssp_start, dssp_end) in TM_regions_prot:\n",
    "        if tm_label in min_max_limits_translated:\n",
    "            (min_start, max_start), (min_end, max_end) = min_max_limits_translated[tm_label]\n",
    "\n",
    "            # Step 2: Adjust start and end points based on MSA limits\n",
    "            adjusted_start = max(min_start, min(dssp_start, max_start))  # Keep within min-max range\n",
    "            adjusted_end = min(max_end, max(dssp_end, min_end))  # Keep within min-max range\n",
    "\n",
    "            # Ensure the TM region has at least 10 residues\n",
    "            if adjusted_end - adjusted_start + 1 < 10:\n",
    "                # If too short, expand towards the closest allowed limit\n",
    "                if adjusted_start > min_start:\n",
    "                    adjusted_start = max(min_start, adjusted_start - (10 - (adjusted_end - adjusted_start + 1)))\n",
    "                if adjusted_end < max_end:\n",
    "                    adjusted_end = min(max_end, adjusted_end + (10 - (adjusted_end - adjusted_start + 1)))\n",
    "\n",
    "            refined_TMs[tm_label] = (adjusted_start, adjusted_end)\n",
    "\n",
    "    # Step 3: Handle cases where DSSP predicts fewer than 7 TMs\n",
    "    if len(refined_TMs) < 7:\n",
    "        missing_TMs = [tm for tm in min_max_limits_translated.keys() if tm not in refined_TMs]\n",
    "        for tm in missing_TMs:\n",
    "            (min_start, max_start), (min_end, max_end) = min_max_limits_translated[tm]\n",
    "            refined_TMs[tm] = (min_start, max_end)  # Assign entire range if missing\n",
    "\n",
    "    # Step 4: Handle cases where DSSP predicts too many TMs\n",
    "    if len(refined_TMs) > 7:\n",
    "        # Merge small or overlapping helices\n",
    "        tm_keys = sorted(refined_TMs.keys(), key=lambda x: int(x[2:]))  # Sort TM labels (TM1, TM2, ...)\n",
    "        merged_TMs = {}\n",
    "        prev_tm = None\n",
    "\n",
    "        for tm in tm_keys:\n",
    "            if prev_tm is None:\n",
    "                merged_TMs[tm] = refined_TMs[tm]\n",
    "            else:\n",
    "                prev_start, prev_end = merged_TMs[prev_tm]\n",
    "                curr_start, curr_end = refined_TMs[tm]\n",
    "\n",
    "                # Merge if overlap or short segment\n",
    "                if curr_start - prev_end < 5 or (curr_end - curr_start + 1 < 10):\n",
    "                    merged_TMs[prev_tm] = (prev_start, curr_end)  # Extend previous TM\n",
    "                else:\n",
    "                    merged_TMs[tm] = refined_TMs[tm]\n",
    "\n",
    "            prev_tm = tm\n",
    "        \n",
    "        refined_TMs = merged_TMs\n",
    "\n",
    "    # Step 5: Ensure exactly 7 TM regions\n",
    "    if len(refined_TMs) < 7:\n",
    "        missing_TMs = [tm for tm in min_max_limits_translated.keys() if tm not in refined_TMs]\n",
    "        for tm in missing_TMs[: 7 - len(refined_TMs)]:  # Fill in missing TMs up to 7\n",
    "            (min_start, max_start), (min_end, max_end) = min_max_limits_translated[tm]\n",
    "            refined_TMs[tm] = (min_start, max_end)\n",
    "\n",
    "    return refined_TMs\n",
    "\n",
    "def map_min_max_limitsMSA_chimera(min_max_limits_translated_parent,aligned_seq_interest_ref,seq_chimera):\n",
    "\n",
    "    unaligned_ref = aligned_seq_interest_ref.replace(\"-\",\"\")\n",
    "\n",
    "    alignments_global = pairwise2.align.globalms(\n",
    "        unaligned_ref, seq_chimera, match=2, mismatch=-1,\n",
    "        open=-10, extend=-2,\n",
    "        one_alignment_only=True\n",
    "    )\n",
    "\n",
    "    aligned_ref_MSAchimera, aligned_seq_interest_chimera = alignments_global[0].seqA, alignments_global[0].seqB\n",
    "\n",
    "    #map min max limits TMs parent to MSA with chimera\n",
    "    min_max_limits_MSAchimera = {}\n",
    "    translate_seq_MSA = map_seq_MSA(aligned_ref_MSAchimera) #gives position of a unaligned res in msa\n",
    "    for TMname, limits in min_max_limits_translated_parent.items():\n",
    "        min_start = translate_seq_MSA[limits[0][0]]\n",
    "        max_start = translate_seq_MSA[limits[0][1]]\n",
    "        min_end = translate_seq_MSA[limits[1][0]]\n",
    "        max_end = translate_seq_MSA[limits[1][1]]\n",
    "        min_max_limits_MSAchimera[TMname]=[[min_start,max_start],[min_end,max_end]]\n",
    "    return min_max_limits_MSAchimera,aligned_seq_interest_chimera\n",
    "\n",
    "def compute_dssp_TM_regions(uniprotID,MSA,type_gpcr = \"natural\",ref_id=None, seq_chimera = None):\n",
    "\n",
    "    record_dict = SeqIO.index(MSA, \"fasta\")\n",
    "    if type_gpcr == \"natural\":\n",
    "        aligned_seq_interest = str(record_dict[uniprotID].seq)\n",
    "        length_prot = len(aligned_seq_interest.replace(\"-\",\"\"))\n",
    "    else:\n",
    "        length_prot = len(seq_chimera)\n",
    "        aligned_seq_interest_ref = str(record_dict[ref_id].seq)\n",
    "    \n",
    "    dssp_filename, gpcr_chain, mapping_uniprot_pdb_numbering  = run_dssp(uniprotID,type_gpcr) #run dssp if needed on AF model else retrieve dssp file\n",
    "    dssp_tup = make_dssp_dict(dssp_filename)\n",
    "    dssp_dic = dssp_tup[0]\n",
    "\n",
    "    # Extract DSSP codes along with their actual residue positions\n",
    "    dssp_positions = []\n",
    "    for key, value in dssp_dic.items():\n",
    "        chain, res_info = key  # Extract chain and residue details\n",
    "        if chain == gpcr_chain:  # Filter only the chain of interest\n",
    "            res_id = res_info[1]  # Actual residue position in the protein\n",
    "            dssp_code = value[1]  # DSSP secondary structure code\n",
    "            dssp_positions.append((res_id, dssp_code))\n",
    "\n",
    "    # Replace '-' with 'X' in DSSP codes\n",
    "    dssp_positions = [(res_id, code.replace('-', 'X')) for res_id, code in dssp_positions]\n",
    "\n",
    "    # Define conserved secondary structure elements (Helix structures)\n",
    "    conserved_2structure_dssp = {\"H\", \"I\", \"G\"}\n",
    "\n",
    "    # Filter for helix positions\n",
    "    helix_positions = [res_id for res_id, code in dssp_positions if code in conserved_2structure_dssp]\n",
    "\n",
    "    if mapping_uniprot_pdb_numbering != None:\n",
    "        helix_positions_renumbered = []\n",
    "        mapping_pdb_uniprot_numbering = {v: k for k, v in mapping_uniprot_pdb_numbering.items()}\n",
    "        for pos in helix_positions:\n",
    "            try:\n",
    "                helix_positions_renumbered.append(mapping_pdb_uniprot_numbering[int(pos)])\n",
    "            except: #not part of GPCR\n",
    "                continue\n",
    "        helix_positions = helix_positions_renumbered\n",
    "\n",
    "    TM_regions = []\n",
    "    #find start and end anchor points\n",
    "    start = helix_positions[0]\n",
    "    stop = None\n",
    "    counter = 1\n",
    "    min_tm_length = 10 #a TM are typically at least 10 residues long otherwise they are considered as small helices connecting the TMs\n",
    "    for i in range(1, len(helix_positions)):\n",
    "        if helix_positions[i] - helix_positions[i - 1] > 1:\n",
    "            if helix_positions[i-1] - start >= min_tm_length :\n",
    "                stop = helix_positions[i-1]\n",
    "            else:\n",
    "                start = helix_positions[i]\n",
    "        if stop != None:\n",
    "            # if stop-start+1 > 3: #we need at least 3 consecutive columns with enough conserved secondary structure elements before considering it as an anchor region\n",
    "            TM_regions.append((\"TM\"+str(counter),(start,stop)))\n",
    "            counter +=1\n",
    "            stop = None\n",
    "            start = helix_positions[i]\n",
    "    TM_regions.append((\"TM\"+str(counter),(start,helix_positions[-1])))\n",
    "    \n",
    "    #adapt TM regions based on 50%-80% limits\n",
    "    min_max_limits = {'TM1':[[559,563],[588,588]],\n",
    "                        'TM2':[[606,606],[636,637]],\n",
    "                        'TM3':[[654,657],[690,690]],\n",
    "                        'TM4':[[717,717],[741,742]],\n",
    "                        'TM5':[[913,916],[946,948]],\n",
    "                        'TM6':[[1169,1174],[1203,1204]],\n",
    "                        'TM7':[[1246,1248],[1270,1271]],} #based on partial dssp MSA with mapping dssp exp structures representative\n",
    "    \n",
    "    if type_gpcr == \"natural\":\n",
    "        min_max_limits_translated_interest = remap_min_max_limits_prot_interest(min_max_limits,aligned_seq_interest)\n",
    "        refined_TM_regions = refine_TM_regions(min_max_limits_translated_interest,TM_regions)\n",
    "    else:\n",
    "\n",
    "        min_max_limits_translated_parent = remap_min_max_limits_prot_interest(min_max_limits,aligned_seq_interest_ref)\n",
    "        min_max_limits_MSAchimera_parent,aligned_seq_interest_chimera = map_min_max_limitsMSA_chimera(min_max_limits_translated_parent,aligned_seq_interest_ref,seq_chimera)\n",
    "        min_max_limits_translated_interest = remap_min_max_limits_prot_interest(min_max_limits_MSAchimera_parent,aligned_seq_interest_chimera)\n",
    "        refined_TM_regions = refine_TM_regions(min_max_limits_translated_interest, TM_regions)\n",
    "    \n",
    "    structured_regions = [\n",
    "        (\"Nterm\", 1, refined_TM_regions[\"TM1\"][0] - 1),\n",
    "        (\"TM1\", refined_TM_regions[\"TM1\"][0], refined_TM_regions[\"TM1\"][1]),\n",
    "        (\"ICL1\", refined_TM_regions[\"TM1\"][1] + 1, refined_TM_regions[\"TM2\"][0] - 1),\n",
    "        (\"TM2\", refined_TM_regions[\"TM2\"][0],refined_TM_regions[\"TM2\"][1]),\n",
    "        (\"ECL1\", refined_TM_regions[\"TM2\"][1] + 1, refined_TM_regions[\"TM3\"][0] - 1),\n",
    "        (\"TM3\",  refined_TM_regions[\"TM3\"][0], refined_TM_regions[\"TM3\"][1]),\n",
    "        (\"ICL2\",  refined_TM_regions[\"TM3\"][1] + 1,  refined_TM_regions[\"TM4\"][0] - 1),\n",
    "        (\"TM4\",  refined_TM_regions[\"TM4\"][0],refined_TM_regions[\"TM4\"][1]),\n",
    "        (\"ECL2\", refined_TM_regions[\"TM4\"][1] + 1, refined_TM_regions[\"TM5\"][0] - 1),\n",
    "        (\"TM5\", refined_TM_regions[\"TM5\"][0],refined_TM_regions[\"TM5\"][1]),\n",
    "        (\"ICL3\", refined_TM_regions[\"TM5\"][1] + 1, refined_TM_regions[\"TM6\"][0] - 1),\n",
    "        (\"TM6\", refined_TM_regions[\"TM6\"][0],refined_TM_regions[\"TM6\"][1]),\n",
    "        (\"ECL3\", refined_TM_regions[\"TM6\"][1] + 1, refined_TM_regions[\"TM7\"][0] - 1),\n",
    "        (\"TM7\", refined_TM_regions[\"TM7\"][0],refined_TM_regions[\"TM7\"][1]),\n",
    "        (\"H8&Cterm\", refined_TM_regions[\"TM7\"][1] + 1, length_prot)\n",
    "    ]\n",
    "\n",
    "    regions = []\n",
    "    for name, start, end in structured_regions:\n",
    "        regions.append({\n",
    "            \"name\": name,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"reference\": \"DSSP\"\n",
    "        })\n",
    "\n",
    "    return regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutting_pts_2_ss_region(dict_regions,all_regions=None):\n",
    "\n",
    "    translated_regions = []\n",
    "    if isinstance(dict_regions,dict):\n",
    "        for region in dict_regions.keys():\n",
    "            lower_lim = dict_regions[region][0]\n",
    "            upper_lim = dict_regions[region][1]\n",
    "            for ss_region in all_regions:\n",
    "                if lower_lim >= ss_region[\"start\"] and lower_lim <= ss_region[\"end\"]:\n",
    "                    ss_lower_lim = ss_region[\"name\"]\n",
    "                if upper_lim >= ss_region[\"start\"] and upper_lim <= ss_region[\"end\"]:\n",
    "                    ss_upper_lim = ss_region[\"name\"]\n",
    "            translated_regions.append(str(lower_lim)+\"-\"+str(upper_lim)+f\" ({ss_lower_lim}-{ss_upper_lim})\")\n",
    "\n",
    "    elif isinstance(dict_regions,list):\n",
    "        if all_regions:\n",
    "            for section in dict_regions:\n",
    "                info = []\n",
    "                for pos in section:\n",
    "                    for ss_region in all_regions:\n",
    "                        if int(pos) >= ss_region[\"start\"] and int(pos) <= ss_region[\"end\"]:\n",
    "                            ss_pos = ss_region[\"name\"]\n",
    "                            break\n",
    "                    info.append(pos)\n",
    "                    info.append(ss_pos)\n",
    "                translated_regions.append(str(info[0])+\"-\"+str(info[2])+f\" ({info[1]}-{info[3]})\")\n",
    "        else:\n",
    "            for region in dict_regions:\n",
    "                lower_lim = region[0]\n",
    "                upper_lim = region[1]\n",
    "                translated_regions.append(str(lower_lim)+\"-\"+str(upper_lim))\n",
    "    return translated_regions\n",
    "\n",
    "\n",
    "def sections_coloring_chimera(info_specific_chimera,allregions):\n",
    "    coloring = {\"Nterm\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"TM1\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"TM1-Loop-TM2\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"TM2\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"TM2-Loop-TM3\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"TM3\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"TM3-Loop-TM4\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"TM4\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"TM4-Loop-TM5\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"TM5\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"TM5-Loop-TM6\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"TM6\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"TM6-Loop-TM7\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"TM7\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"},\n",
    "                \"H8&Cterm\":{0:\"\",1:\"\",2:\"\",3:\"\",4:\"\"}}\n",
    "\n",
    "    cutting_pts_with_ss = info_specific_chimera[\"cutting_point_chimera\"]\n",
    "\n",
    "    for parent,region in enumerate(cutting_pts_with_ss):\n",
    "\n",
    "        positions=region.split(\" \")[0]\n",
    "        name=region.split(\" \")[1][1:-1]\n",
    "        info = []\n",
    "        for i in range(1):\n",
    "            position = int(positions.split(\"-\")[i])\n",
    "            name_region= name.split(\"-\")[i]\n",
    "            lower_lim = [d for d in allregions if d.get(\"name\") == name_region][0][\"start\"]\n",
    "            upper_lim = [d for d in allregions if d.get(\"name\") == name_region][0][\"end\"]\n",
    "            a_fifth = round((upper_lim-lower_lim)/5)\n",
    "\n",
    "            if position < (lower_lim + a_fifth):\n",
    "                idx = 0\n",
    "            elif position < (lower_lim + 2*a_fifth):\n",
    "                idx = 1\n",
    "            elif position < (lower_lim + 3*a_fifth):\n",
    "                idx = 2\n",
    "            elif position < (lower_lim + 4*a_fifth):\n",
    "                idx = 3\n",
    "            else:\n",
    "                idx = 4\n",
    "\n",
    "            #make names match\n",
    "            name_loops = {\"ICL1\":\"TM1-Loop-TM2\",\"ECL1\":\"TM2-Loop-TM3\",\n",
    "                            \"ICL2\":\"TM3-Loop-TM4\",\"ECL2\":\"TM4-Loop-TM5\",\n",
    "                            \"ICL3\":\"TM5-Loop-TM6\",\"ECL3\":\"TM6-Loop-TM7\"}\n",
    "            if \"IC\" in name_region or \"EC\" in name_region:\n",
    "                name_region = name_loops[name_region]\n",
    "\n",
    "            if (parent % 2)==0:\n",
    "                parent_name = \"EC\"\n",
    "            else:\n",
    "                parent_name = \"IC\"\n",
    "\n",
    "            if len(coloring[name_region][idx]) != 0:\n",
    "                coloring[name_region][idx+1]=parent_name\n",
    "            else:\n",
    "                coloring[name_region][idx]=parent_name\n",
    "            \n",
    "    content = \"EC\"\n",
    "    for region_coloring, idx_coloring in coloring.items():\n",
    "            for idx in idx_coloring.keys():\n",
    "                if len(idx_coloring[idx]) == 0:\n",
    "                    idx_coloring[idx] = content\n",
    "                else:\n",
    "                    content = idx_coloring[idx]\n",
    "               \n",
    "    return coloring\n",
    "\n",
    "def retrieve_involvement_natural_chimeric_design(uniprot_id,all_regions):\n",
    "\n",
    "    involvement = []\n",
    "\n",
    "    for parent_column_id in ['Reference_id','Target_id']:\n",
    "\n",
    "        #find rows that have uniprot as ref id or target id\n",
    "        designs_parent = chimeric_design_df[chimeric_design_df[parent_column_id] == uniprot_id]\n",
    "\n",
    "        #Info from rows\n",
    "        names_chimeras = designs_parent['Chimera_name'].tolist()\n",
    "        ids_chimeras = designs_parent['Chimera_name_ids'].tolist()\n",
    "        regions_chimera = designs_parent['Chimera_parts'].tolist()\n",
    "        name_target_chimeras = designs_parent['Target_name'].tolist()\n",
    "        id_target_chimeras = designs_parent['Target_id'].tolist()\n",
    "\n",
    "        name_ref_chimeras = designs_parent['Reference_name'].tolist()\n",
    "        id_ref_chimeras = designs_parent['Reference_id'].tolist()\n",
    "\n",
    "        regions_ref_chimeras = designs_parent['Reference_cutting_points'].tolist()\n",
    "        regions_target_chimeras = designs_parent['Target_cutting_points'].tolist()\n",
    "\n",
    "        expression = designs_parent['Expression binary'].tolist()\n",
    "        fct = designs_parent['Function binary'].tolist()\n",
    "\n",
    "        application = designs_parent['Application'].tolist()\n",
    "        type_chimera = designs_parent['Chimera Type (1/2/3)'].tolist()\n",
    "        Gprot = designs_parent['G-protein'].tolist()\n",
    "        Ligand =designs_parent['Ligand'].tolist()\n",
    "        structures = designs_parent['3D structure PDB'].tolist()\n",
    "        biblio = designs_parent['DOI'].tolist()\n",
    "\n",
    "        for i,(name, id) in enumerate(zip(names_chimeras,ids_chimeras)):\n",
    "            sequence_chimera = str(chimeras_record_dict[name].seq)\n",
    "            all_regions = compute_dssp_TM_regions(id,MSA,type_gpcr = \"chimera\",ref_id=id_ref_chimeras[i], seq_chimera = sequence_chimera)\n",
    "            cutting_pt_chimera = cutting_pts_2_ss_region(eval(regions_chimera[i]),all_regions) \n",
    "\n",
    "            all_regions_ref = compute_dssp_TM_regions(id_ref_chimeras[i],MSA,type_gpcr = \"natural\")\n",
    "            cutting_pt_ref = cutting_pts_2_ss_region(eval(regions_ref_chimeras[i]),all_regions_ref)\n",
    "\n",
    "            all_regions_target = compute_dssp_TM_regions(id_target_chimeras[i],MSA,type_gpcr = \"natural\")\n",
    "            cutting_pt_target = cutting_pts_2_ss_region(eval(regions_target_chimeras[i]),all_regions_target)\n",
    "\n",
    "            pharma_name_ref = html.unescape(get_pharma_name(id_ref_chimeras[i],name_ref_chimeras[i]))\n",
    "            pharma_name_target = html.unescape(get_pharma_name(id_target_chimeras[i], name_target_chimeras[i]))\n",
    "\n",
    "            pharma_name_ref_ = pharma_name_ref\n",
    "            pharma_name_target_ = pharma_name_target\n",
    "            if \"receptor\" in pharma_name_ref.lower():\n",
    "                pharma_name_ref_ = pharma_name_ref.replace(\" receptor\",\"\")\n",
    "            if \"receptor\" in pharma_name_target.lower():\n",
    "                pharma_name_target_ = pharma_name_target.replace(\" receptor\",\"\")\n",
    "            pharma_name = pharma_name_ref_ + \" \" + pharma_name_target_ + \" receptor\"\n",
    "            if \"adrenoceptor\" in pharma_name:\n",
    "                pharma_name = pharma_name.replace(\" receptor\",\"\")\n",
    "\n",
    "            if isinstance(structures[i],str):\n",
    "                pdb = structures[i]\n",
    "            else:\n",
    "                pdb = \"\"\n",
    "\n",
    "            if isinstance(Gprot[i],str):\n",
    "                gprot=Gprot[i]\n",
    "            else:\n",
    "                gprot=\"\"\n",
    "\n",
    "            if isinstance(Ligand[i],str):\n",
    "                ligand=Ligand[i]\n",
    "            else:\n",
    "                ligand=\"\"\n",
    "\n",
    "            chimera={\n",
    "            \"name\":name,\n",
    "            \"name_pharma\":pharma_name,\n",
    "            \"id\":ids_chimeras[i],\n",
    "            \"ref\": name_ref_chimeras[i],\n",
    "            \"ref_pharma_name\": pharma_name_ref,\n",
    "            \"target\": name_target_chimeras[i],\n",
    "            \"target_pharma_name\":pharma_name_target,\n",
    "            \"cutting_point_chimera\": cutting_pt_chimera,\n",
    "            \"cutting_point_ref\": cutting_pt_ref,\n",
    "            \"cutting_point_target\": cutting_pt_target,\n",
    "            \"expression_function\": fct[i],\n",
    "            \"type\": type_chimera[i],\n",
    "            \"GprotLigand\": gprot+\" \"+ligand,\n",
    "            \"application\": application[i]+\" \"+pdb,\n",
    "            \"reference\": biblio[i]\n",
    "            }\n",
    "            involvement.append(chimera)\n",
    "\n",
    "    return involvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_3D_structure_info(info_3Dstructure):\n",
    "    structures_pdb = []\n",
    "    for pdb_info in info_3Dstructure.split(\",\"):\n",
    "        pdb_id = pdb_info.split('(')[0]\n",
    "        chain = pdb_info.split('(')[1].split(' ')[0]\n",
    "        state = pdb_info.split('(')[1].split(' ')[1][:-1]\n",
    "        pdb_dict={\"PDB id\":pdb_id,\"chain\":chain,\"state\":state}\n",
    "        structures_pdb.append(pdb_dict)\n",
    "    return structures_pdb\n",
    "\n",
    "def split_string(input_string):\n",
    "    match = re.match(r\"([a-zA-Z]?)(-?)(\\d*)(-?)([a-zA-Z]?)\", input_string)\n",
    "    if match:\n",
    "        return list(filter(None, match.groups()))\n",
    "    return []\n",
    "\n",
    "def retrieve_mutations_vs_parents_info(mutations,biblio):\n",
    "    mutations_list = []\n",
    "    if len(mutations)>=4:\n",
    "        for mut in mutations.split(\",\"):\n",
    "            if len(mut)>3:\n",
    "                split_mut = split_string(mut)\n",
    "                position = split_mut[1]\n",
    "                parent_res = split_mut[0]\n",
    "                chimera_res = split_mut[2]\n",
    "                mut_dict = {\"start\":int(position),\"end\":int(position),\"original residue\":parent_res, \"alternative residue\":chimera_res,\"reference\":biblio}\n",
    "                mutations_list.append(mut_dict)\n",
    "        return mutations_list\n",
    "    return TypeError\n",
    "\n",
    "def retrieve_chimera_design_info(abb_name):\n",
    "\n",
    "    all_info = chimeric_design_df[chimeric_design_df[\"Chimera_name\"] == abb_name]\n",
    "\n",
    "    uniprot_id = all_info[\"Chimera_name_ids\"].values[0]\n",
    "    given_name = all_info[\"Given name\"].values[0]\n",
    "    cutting_points = eval(all_info[\"Chimera_parts\"].values[0])\n",
    "    ref_parent_name = all_info[\"Reference_name\"].values[0]\n",
    "    ref_parent_id = all_info[\"Reference_id\"].values[0]\n",
    "    target_parent_name = all_info[\"Target_name\"].values[0]\n",
    "    target_parent_id = all_info[\"Target_id\"].values[0]\n",
    "    Gprot = all_info['G-protein'].values[0]\n",
    "\n",
    "    if isinstance(Gprot,str):\n",
    "        gprot=Gprot\n",
    "    else:\n",
    "        gprot=\"\"  \n",
    "    \n",
    "    biblio = all_info['DOI'].values[0]\n",
    "    try:\n",
    "        structures_pdb =  retrieve_3D_structure_info(all_info[\"3D structure PDB\"].values[0])\n",
    "    except:\n",
    "        structures_pdb = []\n",
    "    try:\n",
    "        mutations_vs_parents = retrieve_mutations_vs_parents_info(all_info[\"Mutations\"].values[0],biblio)\n",
    "    except:\n",
    "        mutations_vs_parents = []\n",
    "\n",
    "    return uniprot_id,given_name,cutting_points,ref_parent_name,ref_parent_id,target_parent_name,target_parent_id,structures_pdb,mutations_vs_parents,gprot,biblio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def familly_subclass_parents(ref_parent_name,ref_parent_id,target_parent_name,target_parent_id):\n",
    "    #find classfication based on human classification in GPCRdb\n",
    "    #find human ortholog\n",
    "    family_chimera = []\n",
    "    subclass_ligand_chimera = []\n",
    "    subclass_phylo_chimera = []\n",
    "\n",
    "    for abbreviated_name,uniprotID in zip([ref_parent_name,target_parent_name],[ref_parent_id,target_parent_id]):\n",
    "        try:\n",
    "            if listGPCRdb_df[listGPCRdb_df['Uniprot ID'] == uniprotID][\"Phylogenetically-based\"].values[0] == \"A-other\":\n",
    "                if not abbreviated_name.endswith(\"HUMAN\"):\n",
    "                    abbreviated_name_human = (abbreviated_name.split('_')[0]+\"_\"+\"HUMAN\").lower()\n",
    "                    uniprot_id_human = listGPCRdb_df[listGPCRdb_df['Name'] == abbreviated_name_human]['Uniprot ID'].values[0] \n",
    "            else:\n",
    "                uniprot_id_human = uniprotID\n",
    "\n",
    "\n",
    "            family = listGPCRdb_df[listGPCRdb_df['Uniprot ID'] == uniprot_id_human][\"Subclass\"].values[0].rstrip() #need to change this to interpro API for those not on GPCRdb\n",
    "\n",
    "\n",
    "            if \"Class A\" in family:\n",
    "                family = family.replace(\"Class A \",\"\")\n",
    "                family = family[0].upper()+family[1:]\n",
    "            if \"receptors\" in family:\n",
    "                family = family.replace(\"receptors\",\"\").rstrip()\n",
    "            elif \"receptor\" in family:\n",
    "                family = family.replace(\"receptor\",\"\").rstrip()\n",
    "            subclass_ligand = listGPCRdb_df[listGPCRdb_df['Uniprot ID'] == uniprot_id_human][\"Ligand-based\"].values[0].rstrip() #need to change so it works for all mammals (put same as what we have for humans? What with those not on GPCRdb?)\n",
    "\n",
    "            \n",
    "            if \"receptors\" in subclass_ligand:\n",
    "                subclass_ligand = subclass_ligand.replace(\"receptors\",\"\").rstrip()\n",
    "            elif \"receptor\" in subclass_ligand:\n",
    "                subclass_ligand = subclass_ligand.replace(\"receptor\",\"\").rstrip()\n",
    "            subclass_phylo = listGPCRdb_df[listGPCRdb_df['Uniprot ID'] == uniprot_id_human][\"Phylogenetically-based\"].values[0]\n",
    "            if \"A-\" in subclass_phylo:\n",
    "                subclass_phylo = subclass_phylo.split('-')[1].rstrip()\n",
    "                subclass_phylo = subclass_phylo[0].upper()+subclass_phylo[1:]\n",
    "\n",
    "        except:\n",
    "                family = \"Olfactory\"\n",
    "                subclass_ligand = \"Olfactory\"\n",
    "                subclass_phylo = \"Olfactory\"\n",
    "\n",
    "        family_chimera.append(family)\n",
    "        subclass_ligand_chimera.append(subclass_ligand)\n",
    "        subclass_phylo_chimera.append(subclass_phylo)\n",
    "    \n",
    "    family_chimera_str = \" & \".join(family_chimera)\n",
    "    subclass_ligand_chimera_str = \" & \".join(subclass_ligand_chimera)\n",
    "    subclass_phylo_chimera_str = \" & \".join(subclass_phylo_chimera)\n",
    "    \n",
    "    return family_chimera_str,subclass_ligand_chimera_str,subclass_phylo_chimera_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cutting_point_region(cutting_points,ref_parent_name,target_parent_name,biblio):\n",
    "    cutting_points_regions_parent = []\n",
    "    for i,region in enumerate(cutting_points):\n",
    "        if (i%2)==0:\n",
    "            parent = ref_parent_name\n",
    "            parent_type = \"EC side parent\"\n",
    "        else:\n",
    "            parent = target_parent_name\n",
    "            parent_type = \"IC side parent\"\n",
    "\n",
    "        region_parent =  {\n",
    "        \"name\": f\"{parent} ({parent_type})\",\n",
    "        \"start\": region[0],\n",
    "        \"end\": region[1],\n",
    "        \"reference\": biblio\n",
    "        }\n",
    "        cutting_points_regions_parent.append(region_parent)\n",
    "    return cutting_points_regions_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert scientific name UniProt to common name\n",
    "def parse_species_file(file_path):\n",
    "    species_dict = {}\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    scientific_name = None\n",
    "    \n",
    "    for line in lines:\n",
    "        if \"N=\" in line:\n",
    "            scientific_name = line.split(\"N=\")[1].strip()\n",
    "        elif \"C=\" in line and scientific_name:\n",
    "            common_name = line.split(\"C=\")[1].strip()\n",
    "            species_dict[scientific_name] = common_name\n",
    "            scientific_name = None  # Reset for the next entry\n",
    "    \n",
    "    return species_dict\n",
    "\n",
    "def species_uniprot(ref_parent_id,target_parent_id):\n",
    "    species_parents = []\n",
    "    for prot_id in [ref_parent_id,target_parent_id]:\n",
    "        requestURL = f\"https://rest.uniprot.org/uniprotkb/{prot_id}.json\"\n",
    "        r = requests.get(requestURL, headers={ \"Accept\" : \"application/json\"})\n",
    "        if not r.ok:\n",
    "            r.raise_for_status()\n",
    "            sys.exit()\n",
    "        uniprot_json = json.loads(r.text)\n",
    "        species = uniprot_json['organism']['scientificName']\n",
    "        species_parents.append(species)\n",
    "\n",
    "    #convert scientific names to common names\n",
    "    uniprot_names = \"../data/UniProt_names_scientific_common.txt\"\n",
    "    species_dict = parse_species_file(uniprot_names)\n",
    "    species_parents_common_name = []\n",
    "    for species in species_parents:\n",
    "        species_parents_common_name.append(species_dict[species])\n",
    "    return \" & \".join(species_parents_common_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_seq_parents(ref_parent_id,target_parent_id):\n",
    "    record_dict = SeqIO.index(MSA, \"fasta\")\n",
    "    ref_seq = str(record_dict[ref_parent_id].seq).replace(\"-\",\"\")\n",
    "    target_seq = str(record_dict[target_parent_id].seq).replace(\"-\",\"\")\n",
    "    return ref_seq,target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPCRdb finds sodium pockets\n",
    "#As microswitches are well defined in literature we can check ourselves if these well knwon microswitches are present in our gpcrs\n",
    "#All known microswitches in literature for class A\n",
    "E_DRY_W = {\"positions\":[\"3.49\", \"3.50\", \"3.51\"],\"residues\":[\"ED\", \"R\", \"WY\"], \"name\": \"E/DRY/W motif (ionic lock switch)\"}\n",
    "CWxP = {\"positions\":[\"6.47\", \"6.48\", \"6.50\"], \"residues\":[\"C\", \"W\", \"P\"], \"name\": \"CWxP motif (transmission toggle switch)\"}\n",
    "NPxxY = {\"positions\":[\"7.49\", \"7.50\", \"7.53\"], \"residues\":[\"N\",\"P\",\"Y\"], \"name\": \"NPxxY motif (tyr toggle switch)\"}\n",
    "PIF = {\"positions\": [\"5.50\", \"3.40\", \"6.44\"], \"residues\":[\"P\",\"I\",\"F\"], \"name\": \"PIF motif\"}\n",
    "hydrophobic_lock = {\"positions\":[\"3.43\",\"6.40\"], \"residues\":[\"LVIM\", \"LVIM\"], \"name\": \"hydrophobic lock\"}\n",
    "# ionic_lock = {\"positions\":[\"6.30\"], \"residues\":[\"DE\"], \"name\": \"ionic lock\"}\n",
    "#disulfide bond between TM3 and ECL2 is already identified by Uniprot in the \"Disulfide bonds\" section\n",
    "#Sodium binding pocket (allosteric action): middle of the 7TMs. Identified by GPCRdb but are the identified ones all of them???\n",
    "\n",
    "#the positions are in human readable format (not pyton - starts at 0)\n",
    "MSA_E_DRY_W = {\"positions\":[684,685,686],\"residues\":[\"ED\", \"R\", \"WY\"], \"name\": \"E/DRY/W motif (ionic lock switch)\"}\n",
    "MSA_CWxP = {\"positions\":[1191,1192,1194], \"residues\":[\"C\", \"W\", \"P\"], \"name\": \"CWxP motif (transmission toggle switch)\"}\n",
    "MSA_NPxxY = {\"positions\":[1265,1266,1269], \"residues\":[\"N\",\"P\",\"Y\"], \"name\": \"NPxxY motif (tyr toggle switch)\"}\n",
    "MSA_PIF = {\"positions\": [930,675,1187], \"residues\":[\"P\",\"I\",\"F\"], \"name\": \"PIF motif\"}\n",
    "MSA_hydrophobic_lock = {\"positions\":[678,1183], \"residues\":[\"LVIM\", \"LVIM\"], \"name\": \"Hydrophobic lock\"}\n",
    "# MSA_ionic_lock = {\"positions\":[1190], \"residues\":[\"DE\"], \"name\": \"Ionic lock\"}\n",
    "# MSA_sodium_pocket = {\"positions\":[620,684], \"residues\":[\"D\",\"S\"], \"name\": \"Sodium binding pocket\"}\n",
    "\n",
    "TM1x50={\"positions\":[579],\"residues\":[\"N\"], \"name\": \"1.50 (BW numbering)\"}\n",
    "TM2x50={\"positions\":[620],\"residues\":[\"D\"], \"name\": \"2.50 (BW numbering)\"}\n",
    "TM3x50={\"positions\":[685],\"residues\":[\"R\"], \"name\": \"3.50 (BW numbering)\"}\n",
    "TM4x50={\"positions\":[729],\"residues\":[\"W\"], \"name\": \"4.50 (BW numbering)\"}\n",
    "TM5x50={\"positions\":[930],\"residues\":[\"P\"], \"name\": \"5.50 (BW numbering)\"}\n",
    "TM6x50={\"positions\":[1194],\"residues\":[\"P\"], \"name\": \"6.50 (BW numbering)\"}\n",
    "TM7x50={\"positions\":[1266],\"residues\":[\"P\"], \"name\": \"7.50 (BW numbering)\"}\n",
    "\n",
    "#equivalence between positions in sequence and in MSA\n",
    "#dictionary with list of list. In every sublist, 2 elements, 1st is the position in sequence, the 2nd the position in MSA\n",
    "def map_seq_MSA(sequence_aligned):\n",
    "    previous = 0\n",
    "    translate = {}\n",
    "    sequence_nogaps = sequence_aligned.replace(\"-\",\"\")\n",
    "    for res in range(len(sequence_nogaps)):\n",
    "        idx_msa = previous + sequence_aligned[previous:].index(sequence_nogaps[res])\n",
    "        translate[res+1]=idx_msa+1\n",
    "        previous = idx_msa + 1\n",
    "    return translate\n",
    "\n",
    "#Microswitches/motifs - identify them based on their defined columns in mammalian MSA\n",
    "def motifs_microswitches_literature(MSA,uniprot_id):\n",
    "\n",
    "    alignment = AlignIO.read(open(MSA), \"fasta\")\n",
    "    len_MSA=alignment.get_alignment_length()\n",
    "    record_dict = SeqIO.index(MSA, \"fasta\")\n",
    "    aligned_seq_interest = str(record_dict[uniprot_id].seq)\n",
    "    translate_seq_MSA = map_seq_MSA(aligned_seq_interest) #gives position of a unaligned res in msa\n",
    "    translate_MSA_seq = {v: k for k, v in translate_seq_MSA.items()} #gives position of a aligned res in unaligned seq\n",
    "    # microswitch_types = [MSA_E_DRY_W, MSA_CWxP, MSA_NPxxY, MSA_PIF, MSA_hydrophobic_lock,\n",
    "    #                      TM1x50,TM2x50,TM3x50,TM4x50,TM5x50,TM6x50,TM7x50]\n",
    "    microswitch_types = [MSA_E_DRY_W, MSA_CWxP, MSA_NPxxY, MSA_PIF, MSA_hydrophobic_lock]\n",
    "    \n",
    "    microswitches = []\n",
    "    microswitches_residues = []\n",
    "\n",
    "    for microswitch_type in microswitch_types:\n",
    "        are_there = []\n",
    "        for position, residue in zip(microswitch_type[\"positions\"], microswitch_type[\"residues\"]):\n",
    "            if aligned_seq_interest[position-1] in residue:\n",
    "                are_there.append(True)\n",
    "            else:\n",
    "                are_there.append(False)\n",
    "        for i, (position, residue) in enumerate(zip(microswitch_type[\"positions\"], microswitch_type[\"residues\"])):\n",
    "            microswitch_residue = {}\n",
    "            \n",
    "            #take into account the possibility that there is a gap at that position in the MSA\n",
    "            if position in translate_MSA_seq:\n",
    "                microswitch_residue[\"start\"] = translate_MSA_seq[position]\n",
    "                microswitch_residue[\"end\"] = translate_MSA_seq[position]\n",
    "                residue_motif = aligned_seq_interest[position-1]\n",
    "            else:\n",
    "                for next in range(position+1,len_MSA):\n",
    "                    if next in translate_MSA_seq:\n",
    "                        microswitch_residue[\"start\"] = translate_MSA_seq[next]\n",
    "                        microswitch_residue[\"end\"] = translate_MSA_seq[next]\n",
    "                        residue_motif = aligned_seq_interest[next-1]\n",
    "                        break\n",
    "\n",
    "            if not all(are_there) and not are_there[i]:\n",
    "                if residue_motif == \"F\" and microswitch_type[\"name\"]==\"PIF motif\":\n",
    "                    microswitch_residue[\"description\"] = residue_motif + \" instead of \"+ residue+ \" from \" + \" (part of \" + microswitch_type[\"name\"]+ \")\"\n",
    "                elif residue_motif == \"R\" and microswitch_type[\"name\"]==\"E/DRY/W motif (ionic lock switch)\":\n",
    "                    microswitch_residue[\"description\"] = residue_motif + \" instead of \"+ residue + \" (part of \" + microswitch_type[\"name\"]+ \")\"\n",
    "                elif \"(BW numbering)\" in microswitch_type[\"name\"]:\n",
    "                    microswitch_residue[\"description\"] = residue_motif+ \" instead of \"+ residue + \" \" + microswitch_type[\"name\"]\n",
    "                else:\n",
    "                    microswitch_residue[\"description\"] = residue_motif + \" instead of \"+ residue + \" (part of \" + microswitch_type[\"name\"]+ \")\"\n",
    "            else:\n",
    "                if residue_motif == \"F\" and microswitch_type[\"name\"]==\"PIF motif\":\n",
    "                    microswitch_residue[\"description\"] = residue_motif + \" part of \" + microswitch_type[\"name\"] +\" and hydrophobic lock\"\n",
    "                elif residue_motif == \"R\" and microswitch_type[\"name\"]==\"E/DRY/W motif (ionic lock switch)\":\n",
    "                    microswitch_residue[\"description\"] = residue_motif+ \" part of \" + microswitch_type[\"name\"] +\" and ionic lock\"\n",
    "                elif \"(BW numbering)\" in microswitch_type[\"name\"]:\n",
    "                    microswitch_residue[\"description\"] = residue_motif+ \" \" + microswitch_type[\"name\"]\n",
    "                else:\n",
    "                    microswitch_residue[\"description\"] = residue_motif+ \" part of \" + microswitch_type[\"name\"]\n",
    "            if are_there[i]:\n",
    "                microswitch_residue[\"conserved\"] = \"yes\"\n",
    "            else:\n",
    "                microswitch_residue[\"conserved\"] = \"no\"\n",
    "            microswitch_residue[\"reference\"] = \"Based on alignment\"\n",
    "            microswitches_residues.append(microswitch_residue)  \n",
    "\n",
    "    return microswitches_residues\n",
    "\n",
    "def find_motifs_parent_chimera(cutting_points_parent,microswitches_to_look_at,cutting_points_chimera,counter):\n",
    "    microswitches_found = []\n",
    "    positions_BW_50_chimera = []\n",
    "    for region_parent in cutting_points_parent:\n",
    "        region_chimera = cutting_points_chimera[counter]\n",
    "        start_region_chimera = int(region_chimera.split(' ')[0].split('-')[0])\n",
    "        stop_region_chimera = int(region_chimera.split(' ')[0].split('-')[1])\n",
    "\n",
    "        start_region_parent = int(region_parent.split(' ')[0].split('-')[0])\n",
    "        stop_region_parent = int(region_parent.split(' ')[0].split('-')[1])\n",
    "\n",
    "        if start_region_parent > start_region_chimera:\n",
    "            difference = start_region_parent - start_region_chimera\n",
    "        else:\n",
    "            difference = start_region_chimera - start_region_parent\n",
    "\n",
    "        for position in range(start_region_parent,stop_region_parent+1):\n",
    "            motifs_associated_pos = []\n",
    "                    \n",
    "            motifs_associated_pos = [item for item in microswitches_to_look_at if item[\"start\"] == position]\n",
    "            if len(motifs_associated_pos)>0:\n",
    "                if start_region_parent > start_region_chimera:\n",
    "                    position_chimera = position - difference\n",
    "                else:\n",
    "                    position_chimera = position + difference\n",
    "                for motif in motifs_associated_pos:\n",
    "                    motif_chimera = motif.copy()\n",
    "                    motif_chimera[\"start\"] = position_chimera\n",
    "                    motif_chimera[\"end\"] = position_chimera\n",
    "                    microswitches_found.append(motif_chimera)\n",
    "\n",
    "                    if \"BW numbering\" in motif_chimera[\"description\"]: #keep position of .50 residues for BW numbering\n",
    "                        positions_BW_50_chimera.append(position_chimera)\n",
    "\n",
    "        counter +=2\n",
    "    return microswitches_found, positions_BW_50_chimera\n",
    "\n",
    "def computeBW_numbering_chimera(positions_BW_50_chimera,all_regions):\n",
    "    length_prot = all_regions[-1][\"end\"]\n",
    "    uniprot_BW_mapping =  {i + 1: \"\" for i in range(length_prot)}\n",
    "    counter = 1\n",
    "\n",
    "    for TMname in all_regions:\n",
    "        if \"TM\" in TMname[\"name\"]:\n",
    "            position_50_seq = positions_BW_50_chimera[counter-1]\n",
    "            numberTM = str(counter)+\".\"\n",
    "\n",
    "            uniprot_BW_mapping[position_50_seq]+=(numberTM+str(50))\n",
    "\n",
    "            start = TMname[\"start\"]\n",
    "            end = TMname[\"end\"]\n",
    "            distance = 1\n",
    "            for residue in range(position_50_seq-1,start-1,-1):\n",
    "                pos = 50-distance\n",
    "                uniprot_BW_mapping[residue]+=(numberTM+str(pos))\n",
    "                distance +=1\n",
    "\n",
    "            distance = 1\n",
    "            for residue in range(position_50_seq+1,end+1):\n",
    "                pos = 50+distance\n",
    "                uniprot_BW_mapping[residue]+=(numberTM+str(pos))\n",
    "                distance +=1\n",
    "            \n",
    "            counter +=1\n",
    "            \n",
    "    for pos, value in uniprot_BW_mapping.items():\n",
    "        if len(value)==0:\n",
    "            uniprot_BW_mapping[pos]=\"N/A\"\n",
    "    return uniprot_BW_mapping\n",
    "\n",
    "def motifs_microswitches_BWnumbering_chimera(all_regions,ref_parent_id,target_parent_id,info_specific_chimera, MSA):\n",
    "    #find motifs chimera based on motifs parents. For every region coming from a particular parent, look if the parent has motifs in that region. If yes, chimera also has it.\n",
    "\n",
    "    cutting_points_chimera = info_specific_chimera[\"cutting_point_chimera\"]\n",
    "    cutting_points_ref = info_specific_chimera[\"cutting_point_ref\"]\n",
    "    cutting_points_target = info_specific_chimera[\"cutting_point_target\"]\n",
    "\n",
    "    microswitches_ref = motifs_microswitches_literature(MSA,ref_parent_id)\n",
    "    microswitches_target = motifs_microswitches_literature(MSA,target_parent_id)\n",
    "    microswitches_chimera_ref, positions_BW_50_chimera_ref  = find_motifs_parent_chimera(cutting_points_ref,microswitches_ref,cutting_points_chimera,0)\n",
    "    microswitches_chimera_target, positions_BW_50_chimera_target = find_motifs_parent_chimera(cutting_points_target,microswitches_target,cutting_points_chimera,1)\n",
    "\n",
    "    microswitches_residues = microswitches_chimera_ref + microswitches_chimera_target\n",
    "    positions_BW_50_chimera = positions_BW_50_chimera_ref + positions_BW_50_chimera_target\n",
    "\n",
    "    uniprot_BW_mapping = computeBW_numbering_chimera(positions_BW_50_chimera,all_regions)\n",
    "    \n",
    "    return microswitches_residues, uniprot_BW_mapping\n",
    "\n",
    "\n",
    "def disulfide_bonds_chimera(ref_parent_id,seq_ref,seq_chimera,allregions):\n",
    "\n",
    "    disulfide_bonds=[]\n",
    "    #methodology can be optimized \n",
    "    #cannot use MSA as Cys in ECL2 => position not conserved\n",
    "    requestURL = f\"https://rest.uniprot.org/uniprotkb/{ref_parent_id}.json\"\n",
    "    r = requests.get(requestURL, headers={ \"Accept\" : \"application/json\"})\n",
    "\n",
    "    if not r.ok:\n",
    "        r.raise_for_status()\n",
    "        sys.exit()\n",
    "\n",
    "    uniprot_json = json.loads(r.text)\n",
    "    ref_ssbonds =[]\n",
    "    for i in range(len(uniprot_json['features'])):\n",
    "        if uniprot_json['features'][i]['type'] == 'Disulfide bond':\n",
    "            ref_ssbonds.append( [uniprot_json['features'][i]['location']['start']['value'],\n",
    "        uniprot_json['features'][i]['location']['end']['value'] ])\n",
    "   \n",
    "\n",
    "    alignments_global = pairwise2.align.globalms(\n",
    "        seq_ref, seq_chimera, match=2, mismatch=-1,\n",
    "        open=-10, extend=-2,\n",
    "        one_alignment_only=True\n",
    "    )\n",
    "\n",
    "    aligned_ref, aligned_chimera = alignments_global[0].seqA, alignments_global[0].seqB\n",
    "\n",
    "    #map min max limits TMs parent to MSA with chimera\n",
    "    translate_seq_MSA_ref = map_seq_MSA(aligned_ref) #gives position of a unaligned res in msa\n",
    "    translate_seq_MSA_chimera = map_seq_MSA(aligned_chimera) #gives position of a unaligned res in msa\n",
    "    translate_MSA_seq_chimera = {v: k for k, v in translate_seq_MSA_chimera.items()}\n",
    "\n",
    "    #if ss in TM3 ECL2 of chimera than very likely the known conserved ss bond\n",
    "    for CysCys in ref_ssbonds:\n",
    "        start_ref = CysCys[0]\n",
    "        aligned_start_ref = translate_seq_MSA_ref[start_ref]\n",
    "        start_chimera = translate_MSA_seq_chimera[aligned_start_ref]\n",
    "        end_ref = CysCys[1]\n",
    "        aligned_end_ref = translate_seq_MSA_ref[end_ref]\n",
    "        end_chimera = translate_MSA_seq_chimera[aligned_end_ref]\n",
    "\n",
    "        #this is quite hacky, to be improved!\n",
    "        region1 = [d for d in allregions if d[\"start\"] <= start_chimera <= d[\"end\"]][0][\"name\"]\n",
    "        if region1 == \"TM3\":\n",
    "            region2 = [d for d in allregions if d[\"start\"] <= end_chimera <= d[\"end\"]][0][\"name\"]\n",
    "            if region2 == \"ECL2\":\n",
    "                disulfide_bonds.append({\n",
    "                    'start':CysCys[0],\n",
    "                    'end':CysCys[1],\n",
    "                    \"description\":\"Disulfide bond\",\n",
    "                    \"reference\": \"Inferred from EC parent.\",\n",
    "                })\n",
    "    \n",
    "    return disulfide_bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pharma_name(uniprotID,abb_name):\n",
    "    #GtoP or gpcrdb_name or pharmacological name\n",
    "    try:\n",
    "        requestURL = f\"https://gpcrdb.org/services/protein/accession/{uniprotID}\"\n",
    "\n",
    "        r = requests.get(requestURL, headers={ \"Accept\" : \"application/json\"})\n",
    "        if not r.ok:\n",
    "            info_entry = None\n",
    "        else:\n",
    "            info_entry = json.loads(r.text)\n",
    "    except:\n",
    "        info_entry = None\n",
    "\n",
    "    if not info_entry is None:\n",
    "        clean_html_tags = re.compile('<.*?>')\n",
    "        pharma_name = re.sub(clean_html_tags, '', info_entry[\"name\"])\n",
    "    else:\n",
    "        pharma_name = abb_name\n",
    "\n",
    "    return pharma_name\n",
    "\n",
    "def retrieve_pharma_name_parents(ref_parent_name,ref_parent_id,target_parent_name,target_parent_id):\n",
    "    pharma_name_ref = get_pharma_name(ref_parent_id,ref_parent_name)\n",
    "    pharma_name_target = get_pharma_name(target_parent_id,target_parent_name)\n",
    "\n",
    "    if \"receptor\" in pharma_name_ref.lower():\n",
    "        pharma_name_ref = pharma_name_ref.replace(\" receptor\",\"\")\n",
    "    if \"receptor\" in pharma_name_target.lower():\n",
    "        pharma_name_target = pharma_name_target.replace(\" receptor\",\"\")\n",
    "    \n",
    "    pharma_name = pharma_name_ref + \" \" + pharma_name_target + \" receptor\"\n",
    "\n",
    "    if \"adrenoceptor\" in pharma_name:\n",
    "        pharma_name = pharma_name.replace(\" receptor\",\"\")\n",
    "\n",
    "    return pharma_name_ref,pharma_name_target,pharma_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_predicted_models(structures,uniprotID):\n",
    "\n",
    "    #AlphaFold2\n",
    "    af_model_path = f\"file:///examples/3Dstructures/AF_chimera_2024/{uniprotID}.pdb\"\n",
    "    structures.append({\"value\":f\"AlphaFold2\",\"chain\": \"A\", \"state\":\"Undetermined\", \"offset\":  0, \"gaps\": [], \"resolution\": \"\", \"method\": \"Predicted\", \"url\": af_model_path, \"reference\":\"AlphaFold2\",\"date\":\"2024\"})\n",
    "    \n",
    "    #AlphaFold multistate. Don't have a AF ms for every GPCR (only humans). Need to check if file exist:\n",
    "    af_ms_active = f\"../examples/3Dstructures/AFms_2023/Active/{uniprotID}.pdb\"\n",
    "    af_ms_inactive = f\"../examples/3Dstructures/AFms_2023/Inactive/{uniprotID}.pdb\"\n",
    "    esmf = f\"../examples/3Dstructures/ESMF_chimera_2023/{uniprotID}.pdb\"\n",
    "    if os.path.exists(af_ms_active):\n",
    "        af_ms_active = f\"file:///examples/3Dstructures/AFms_2023/Active/{uniprotID}.pdb\"\n",
    "        structures.append({\"value\":f\"AlphaFold2-Multistate Active\",\"chain\": \"A\",  \"state\":\"Active\", \"offset\":  0, \"gaps\": [], \"resolution\": \"\", \"method\": \"Predicted\", \"url\": af_ms_active, \"reference\":\"AlphaFold multistate\", \"date\":\"2023\"})\n",
    "    if os.path.exists(af_ms_inactive):\n",
    "        af_ms_inactive = f\"file:///examples/3Dstructures/AFms_2023/Inactive/{uniprotID}.pdb\"\n",
    "        structures.append({\"value\":f\"AlphaFold2-Multistate Inactive\",\"chain\": \"A\",  \"state\":\"Inactive\", \"offset\":  0, \"gaps\": [], \"resolution\": \"\", \"method\": \"Predicted\", \"url\": af_ms_inactive, \"reference\":\"AlphaFold multistate\", \"date\":\"2023\"})\n",
    "    if os.path.exists(esmf):\n",
    "        esmf = f\"file:///examples/3Dstructures/ESMF_chimera_2023/{uniprotID}.pdb\"\n",
    "        structures.append({\"value\":f\"ESMFold\",\"chain\": \"A\", \"state\":\"Undetermined\", \"offset\":  0, \"gaps\": [], \"resolution\": \"\", \"method\": \"Predicted\", \"url\": esmf, \"reference\":\"ESMFold\", \"date\":\"2023\"})\n",
    "\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_interacting_residues(interacting_residues_list,binders_list,pdbs):\n",
    "    # Reprocessing the data with the updated binders list\n",
    "    interactions = {}\n",
    "\n",
    "    for i, residue_groups in enumerate(interacting_residues_list):\n",
    "        binders = binders_list[i]\n",
    "        pdb_id = pdbs[i]\n",
    "\n",
    "        for j,residue_group in enumerate(residue_groups):\n",
    "            for residue in residue_group:\n",
    "                if residue not in interactions:\n",
    "                    interactions[residue] = [[binders[j],pdb_id]]\n",
    "                else:\n",
    "                    interactions[residue].append([binders[j],pdb_id])\n",
    "\n",
    "    contacts_list =[]\n",
    "    for residue,info in interactions.items():\n",
    "        pdbs = []\n",
    "        binders = []\n",
    "        for data in info:\n",
    "            if not data[0] is None:\n",
    "                binders.append(data[0])\n",
    "                pdbs.append(data[1])\n",
    "        binders=list(set(binders))\n",
    "        if len(binders)>0:\n",
    "            contacts_list.append({\n",
    "                \"start\": residue,\n",
    "                \"end\": residue,\n",
    "                \"type\": \", \".join(binders),  # Ensuring unique binders\n",
    "                \"description\": f\"Inferred from {', '.join(pdbs)}.\",\n",
    "                \"reference\": \"https://www.ebi.ac.uk/pdbe/pisa/\"\n",
    "            })\n",
    "\n",
    "    return contacts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPSD_BOVIN_ACM3_HUMAN_1\n",
      "OPRM_MOUSE_OPRK_HUMAN_1\n",
      "NTR1_HUMAN_OPRK_HUMAN_1\n",
      "SSR2_HUMAN_OPRK_HUMAN_1\n",
      "HRH2_HUMAN_OPRK_HUMAN_1\n",
      "ADA1A_HUMAN_OPRK_HUMAN_1\n",
      "CXCR3_HUMAN_OPRK_HUMAN_1\n",
      "ADA1A_HUMAN_OPRK_HUMAN_2\n"
     ]
    }
   ],
   "source": [
    "for abbreviated_name, prot_seq in entry_uniprotID_seq.items():\n",
    "    print(abbreviated_name)\n",
    "    #if not os.path.exists(f'../examples/json_entries/new_chimeras_2/{abbreviated_name.upper()}.json'):\n",
    "    if \"a\" == \"a\":\n",
    "    # if abbreviated_name == \"ACM3_HUMAN_ACM4_HUMAN_1\":\n",
    "\n",
    "        class_ = 'A & A' #always class A for now\n",
    "        uniprotID,given_name,cutting_points,ref_parent_name,ref_parent_id,target_parent_name,target_parent_id,structures_pdb,mutations_vs_parents,gprot,biblio = retrieve_chimera_design_info(abbreviated_name)\n",
    "\n",
    "        ref_parent_seq,target_parent_seq = retrieve_seq_parents(ref_parent_id,target_parent_id)\n",
    "\n",
    "        #pharma name based on pharma name parents\n",
    "        pharma_name_ref,pharma_name_target,pharma_name = retrieve_pharma_name_parents(ref_parent_name,ref_parent_id,target_parent_name,target_parent_id)\n",
    "        \n",
    "        species = species_uniprot(ref_parent_id,target_parent_id)\n",
    "        \n",
    "        family,subclass_ligand,subclass_phylo=familly_subclass_parents(ref_parent_name,ref_parent_id,target_parent_name,target_parent_id)\n",
    "\n",
    "        #Secondary structure info\n",
    "        allregions = compute_dssp_TM_regions(uniprotID,MSA,type_gpcr = \"chimera\",ref_id=ref_parent_id, seq_chimera = prot_seq)\n",
    "        #structures\n",
    "        structures, interacting_residues_list, binders_list, pdbs = retrieve_pdb_dsbonds_chimera_info(structures_pdb,uniprotID,prot_seq)\n",
    "        structures = retrieve_predicted_models(structures,uniprotID)\n",
    "        \n",
    "        #retrieve the residues interacting with ligand/Gprot/Nb/Ab in PDB and link it to region\n",
    "        #Add manually extra IC and EC contacts\n",
    "        #should follow the following structure: list regrouping all dictionaries with 1 dict per contact\n",
    "        #in dictionary: {\"start\":,\"end\",\"type\",\"description\",\"reference\"}\n",
    "        #for the EC contacts the types can be \"orthosteric\",\"allosteric\",\"VHH EC\"\n",
    "        #for the IC contacts the types can be \"G-protein\",\"VHH IC\"\n",
    "        # manual_ICs,manual_ECs = translate_interacting_residues_IC_EC(interacting_residues_list,binders,pdbs,allregions)\n",
    "        # contacts = translate_interacting_residues_IC_EC(interacting_residues_list,binders,pdbs,allregions)\n",
    "        contacts =gather_interacting_residues(interacting_residues_list,binders_list,pdbs)\n",
    "        disulfide_bonds = disulfide_bonds_chimera(ref_parent_id,ref_parent_seq,prot_seq,allregions)\n",
    "\n",
    "        #chimeric info\n",
    "        chimeras_ref_involved = retrieve_involvement_natural_chimeric_design(ref_parent_id,allregions)\n",
    "        chimeras_target_involved = retrieve_involvement_natural_chimeric_design(target_parent_id,allregions)  \n",
    "        chimeras = chimeras_ref_involved + chimeras_target_involved\n",
    "        chimeras_no_duplicates = {}\n",
    "        for d in chimeras:\n",
    "            chimeras_no_duplicates[d['name']] = d\n",
    "        chimeras_no_duplicates = list(chimeras_no_duplicates.values())\n",
    "\n",
    "        confo_biosensor = []\n",
    "\n",
    "        #chimeric info specfic design being studied\n",
    "        for d in chimeras_no_duplicates:\n",
    "            if d.get(\"name\") == abbreviated_name:\n",
    "                info_specific_chimera = d.copy()\n",
    "        info_specific_chimera[\"mutations vs parents\"] = mutations_vs_parents\n",
    "        info_specific_chimera[\"ref parent seq\"] = ref_parent_seq\n",
    "        info_specific_chimera[\"target parent seq\"] = target_parent_seq\n",
    "        info_specific_chimera[\"coloring\"] =sections_coloring_chimera(info_specific_chimera,allregions)\n",
    "\n",
    "        #microswitches based on microswitches/motifs in parents\n",
    "        microswitches, BW_numbering = motifs_microswitches_BWnumbering_chimera(allregions,ref_parent_id,target_parent_id,info_specific_chimera, MSA)\n",
    "        \n",
    "        info = {}\n",
    "        #standard needed in json chimera for DB\n",
    "        info[\"chimera\"] = True\n",
    "        info[\"ancestors\"] = [ref_parent_name,target_parent_name]\n",
    "\n",
    "        info[\"info specific chimera\"]=info_specific_chimera\n",
    "\n",
    "        #Abbreviated name\n",
    "        info[\"Abbreviated name\"] = [{\"value\": abbreviated_name.upper(), \"reference\": \"GPCRchimeraDB\"}]\n",
    "\n",
    "        #pharma name\n",
    "        info[\"Pharmacological name\"] = [{\"value\": pharma_name, \"reference\": \"GPCRdb\"}]\n",
    "\n",
    "        #Name\n",
    "        info[\"Name(s)\"] = [{\"value\": given_name, \"reference\": \"GPCRchimeraDB\"}]\n",
    "\n",
    "        #Uniprot ID\n",
    "        info[\"Uniprot ID\"] = [{\"value\": uniprotID, \"reference\": \"GPCRchimeraDB\"}]\n",
    "\n",
    "        #Species\n",
    "        info[\"Organism\"] =  [{\"value\":species, \"reference\": \"UniProt\"}]\n",
    "\n",
    "        #Class\n",
    "        info[\"Class\"] = [{\"value\":class_, \"reference\": \"GPCRdb\"}]\n",
    "\n",
    "        #Family\n",
    "        info[\"Family\"] = [{\"value\": family, \"reference\": \"GPCRdb\"}]\n",
    "\n",
    "        #Subclass\n",
    "        #Phylogenetically based & Ligand based\n",
    "        info[\"Subclass\"] = {\"Phylogenetically based\": [{\"value\": subclass_phylo, \"reference\": \"10.1124/mol.63.6.1256\"}],\n",
    "                            \"Ligand based\": [{\"value\":subclass_ligand, \"reference\": \"GPCRdb\"}]}\n",
    "\n",
    "        #endogenous ligand\n",
    "        info[\"Endogenous ligand\"]= []\n",
    "        \n",
    "        #Gport and Barr coupling data\n",
    "        if len(gprot)>0:\n",
    "            info[\"G-protein coupling\"]= [{\"value\":gprot, \"reference\": biblio}]\n",
    "        else:\n",
    "            info[\"G-protein coupling\"]= []\n",
    "        info[\"Beta-arrestin coupling\"] = []\n",
    "\n",
    "        #Structures\n",
    "        info[\"Structures\"] = structures\n",
    "\n",
    "        #Info related to chimeric design\n",
    "        info[\"Conformational biosensor\"] = confo_biosensor\n",
    "        info[\"Involvement in chimeric design\"] = chimeras_no_duplicates\n",
    "        info[\"Cutting point values\"] = write_cutting_point_region(cutting_points,ref_parent_name,target_parent_name,biblio)\n",
    "\n",
    "        features = {}\n",
    "        features['Microswitches'] = microswitches\n",
    "        features['PTMs'] = []\n",
    "        features['Disulfide bonds'] = disulfide_bonds\n",
    "        features['Mutagenesis'] = []\n",
    "        features['Pharmacological mutagenesis'] = []\n",
    "        features[\"Contacts\"] = contacts\n",
    "        info[\"Features\"] = features\n",
    "\n",
    "        #Sequence\n",
    "        info[\"Sequence\"] = [{\"value\":prot_seq, \"reference\": biblio}]\n",
    "\n",
    "        #Secondary structure info\n",
    "        info[\"Limits regions\"] = allregions\n",
    "\n",
    "        #BW numbering\n",
    "        info[\"BWnumbering\"] = [{\"value\":BW_numbering, \"reference\": \"MSA\"}] \n",
    "\n",
    "        #Gather info that could be useful for chimeric design: for chimera put just yes chimera so that filter can be used on DB?\n",
    "        known_info = [{\"value\": \"Yes chimera\"}]\n",
    "        info[\"Known info chimeric design\"] = known_info\n",
    "        info[\"entryType\"] = \"chimera\"\n",
    "\n",
    "        json.dump(info, open(f'../examples/json_entries/new_chimeras_2/{abbreviated_name.upper()}.json', 'w'), indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
