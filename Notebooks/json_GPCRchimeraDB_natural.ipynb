{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AQCGa7gnNVZ3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlottecrauwels/anaconda3/lib/python3.10/site-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from Bio import AlignIO, SeqIO, pairwise2\n",
    "import re\n",
    "import html\n",
    "from Bio.PDB.DSSP import make_dssp_dict\n",
    "from Bio import PDB\n",
    "import pandas as pd \n",
    "import subprocess\n",
    "import mapping_uniprot_pdb\n",
    "import requests, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VZnsxbIFNDzL"
   },
   "outputs": [],
   "source": [
    "# Download the needed files\n",
    "\n",
    "#Download uniprot ID of all proteins on GPCRdb and their classification\n",
    "\n",
    "#Download file with classification info\n",
    "filename_listGPCRdb = \"../data/250220_Classification_GPCRdb.xlsx\"\n",
    "listGPCRdb_df = pd.read_excel(filename_listGPCRdb)\n",
    "\n",
    "#Load the excel file of your protein entry from https://gpcrdb.org/mutational_landscape/ #check for updates?!\n",
    "\n",
    "filename_mutagenesis = \"../data/GPCRdb_variants.xlsx\"\n",
    "mutagenesis_GPCRdb_raw_data = pd.read_excel(filename_mutagenesis)\n",
    "\n",
    "#Load the excel file with Gprot and Barr coupling data from https://gproteindb.org/signprot/statistics_venn (filtered with own code)\n",
    "#this info is also on GtoP but not as complete\n",
    "filename_Gprot = \"../data/GproteinDB_table.xlsx\"\n",
    "Gprot_GPCRdb = pd.read_excel(filename_Gprot)\n",
    "filename_Barr = \"../data/BarrDB_table.xlsx\"\n",
    "Barr_GPCRdb = pd.read_excel(filename_Barr)\n",
    "\n",
    "#chimeric design info\n",
    "filename_chimeric_designs = \"../data/previous_designs.xlsx\"\n",
    "chimeric_design_df = pd.read_excel(filename_chimeric_designs)\n",
    "\n",
    "#Download all structures on GPCRdb to access the GPCR state\n",
    "requestURL = \"https://gpcrdb.org/services/structure/\"\n",
    "r = requests.get(requestURL)\n",
    "\n",
    "if not r.ok:\n",
    "  r.raise_for_status()\n",
    "  sys.exit()\n",
    "\n",
    "structures_chain = json.loads(r.text)\n",
    "json.dump(structures_chain, open(\"../data/structures_data.json\", \"w\"), indent=2)\n",
    "# structures_chain= json.load(open(\"../data/structures_data.json\"))\n",
    "\n",
    "#Download all endogenous on GPCRdb to access the GPCR state\n",
    "requestURL = \"https://gpcrdb.org/services/ligands/endogenousligands/\"\n",
    "r = requests.get(requestURL)\n",
    "\n",
    "if not r.ok:\n",
    "  r.raise_for_status()\n",
    "  sys.exit()\n",
    "\n",
    "endogenous_ligands = json.loads(r.text)\n",
    "json.dump(endogenous_ligands, open(\"../data/endogenous_ligands.json\", \"w\"), indent=2)\n",
    "# endogenous_ligands = json.load(open(\"../data/endogenous_ligands.json\"))\n",
    "\n",
    "#alignment with all the natural GPCRs\n",
    "MSA = \"../data/MSA_all_mammalian.fasta\"\n",
    "\n",
    "# file with the representative experimental 3D structures (lowest resolution independent of activation state)\n",
    "# file generate with the notebook \"reference_structure.ipynb\" updated Feb 2025\n",
    "representative_structures_json = json.load(open(\"../data/250225_representative_structures_exp_pdbID_uniprotID.json\"))\n",
    "\n",
    "#chimeras\n",
    "chimeric_entry_data = \"../data/all_designs.fasta\"\n",
    "chimeras_record_dict = SeqIO.index(chimeric_entry_data, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'Q9BXA5': '----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------MLGIMAWNATCKNWLAAEAALEKYYLSIFYGIEFVVGVLGNTIVVYGYIFSLK-----------NWNSSNI-YLFNLSV-SDLAFLCT-LPMLIRSYANGN-----------WIYGDVL-CISNRYVLHANL-YTSILFLTFISIDRYLIIKYPFRE----------------HLLQKKEFA-ILISLAIWVLVTLELLPILPLINPVITDNGTT-----------------------------------------------------------------------------------------------------------------------------------------------------CNDFASSGDPNYNLIYSMCLTLLGFLIPLFVMCFFYYKIALFLKQRNRQV-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ATALPLEKPLNLVIMAVVIFSV-LFTPYHVMRNVRIASRLGSWKQ---------------------------YQCTQVVINSFYIVTRPLAFLNSVINPVFYFLL-GDHFRDM---LMNQLRHNFKSLTSFSRWAHELLLSFREK---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------'}\n"
     ]
    }
   ],
   "source": [
    "#for now select only 6 (demonstration purposes)\n",
    "of_interest=[\"Q9BXA5\",]\n",
    "\n",
    "naturals_entry_data = \"../data/MSA_all_mammalian.fasta\"\n",
    "entry_uniprotID_seq = {}\n",
    "for record in SeqIO.parse(naturals_entry_data,\"fasta\"):\n",
    "    if record.id in of_interest:\n",
    "        entry_uniprotID_seq[record.id]=str(record.seq)\n",
    "print(len(entry_uniprotID_seq))\n",
    "print(entry_uniprotID_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coupling_Gprot_Barr(uniprot_id):\n",
    "    Gprot_coupling_data = []\n",
    "    Gprot_coupling_data_prot = {}\n",
    "    gprot = \"\"\n",
    "    for i in Gprot_GPCRdb[Gprot_GPCRdb['Uniprot ID'] == uniprot_id].iloc[0][1:]:\n",
    "        if not i is np.NaN:\n",
    "            gprot +=i\n",
    "            gprot += \", \"\n",
    "    Gprot_coupling_data_prot[\"value\"]=gprot[:-2]\n",
    "    Gprot_coupling_data_prot[\"reference\"]=\"https://gproteindb.org/signprot/statistics_venn\"\n",
    "    Gprot_coupling_data.append(Gprot_coupling_data_prot)\n",
    "\n",
    "    Barr_coupling_data = []\n",
    "    Barr_coupling_data_prot = {}\n",
    "    barr = \"\"\n",
    "    for i in Barr_GPCRdb[Barr_GPCRdb['Uniprot ID'] == uniprot_id].iloc[0][1:]:\n",
    "        if not i is np.NaN:\n",
    "            barr +=i\n",
    "            barr += \", \"\n",
    "    Barr_coupling_data_prot[\"value\"]=barr[:-2]\n",
    "    Barr_coupling_data_prot[\"reference\"]=\"https://arrestindb.org/signprot/arrestin_venn\"\n",
    "    Barr_coupling_data.append(Barr_coupling_data_prot)\n",
    "\n",
    "    return Gprot_coupling_data,Barr_coupling_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_mutagenesis_info_Uniprot(variations_uniprot_json,uniprot_id):\n",
    "    mutations_Uniprot = []\n",
    "    for variant in range(len(variations_uniprot_json[\"features\"])):\n",
    "        mutation = {}\n",
    "        mutation[\"start\"] = int(variations_uniprot_json[\"features\"][variant][\"begin\"])\n",
    "        mutation[\"end\"] = int(variations_uniprot_json[\"features\"][variant][\"end\"])\n",
    "        mutation_type = variations_uniprot_json[\"features\"][variant][\"consequenceType\"]\n",
    "        mutation[\"type\"] = mutation_type\n",
    "\n",
    "        typical_AA = \"AVLIPMCFYWSTQNHKRDEG\"\n",
    "        mutation[\"original residue\"] = variations_uniprot_json[\"features\"][variant][\"wildType\"]\n",
    "        if not mutation[\"original residue\"] in typical_AA:\n",
    "            continue\n",
    "        predictions = []\n",
    "        prediction = {}\n",
    "        consensus = []\n",
    "        if mutation_type.lower() == \"missense\":\n",
    "            mutation[\"alternative residue\"] = variations_uniprot_json[\"features\"][variant][\"mutatedType\"]\n",
    "            try:\n",
    "                nb_predictors = len(variations_uniprot_json[\"features\"][variant][\"predictions\"])\n",
    "                for predictor in range(nb_predictors):\n",
    "                    prediction = {}\n",
    "                    algorithm = variations_uniprot_json[\"features\"][variant][\"predictions\"][predictor][\"predAlgorithmNameType\"].lower()\n",
    "                    score = variations_uniprot_json[\"features\"][variant][\"predictions\"][predictor][\"score\"]\n",
    "                    prediction_value = variations_uniprot_json[\"features\"][variant][\"predictions\"][predictor][\"predictionValType\"]\n",
    "                    if algorithm.lower() == \"polyphen\":\n",
    "                        prediction[\"predictor\"] = \"polyphen\"\n",
    "                        prediction[\"value\"] = str(score)\n",
    "                        if score >= 0.2:\n",
    "                            prediction[\"prediction\"] = \"probably damaging\"\n",
    "                            consensus.append(\"-\")\n",
    "                        elif score >= 0.1:\n",
    "                            prediction[\"prediction\"] = \"possibly damaging\"\n",
    "                            consensus.append(\"-\")\n",
    "                        else:\n",
    "                            prediction[\"prediction\"] = \"benign\"\n",
    "                            consensus.append(\"+\")\n",
    "                        predictions.append(prediction)\n",
    "                    elif algorithm.lower() == \"sift\":\n",
    "                        prediction[\"predictor\"] = \"SIFT\"\n",
    "                        prediction[\"value\"] = str(score)\n",
    "                        if score <= 0.05:\n",
    "                            prediction[\"prediction\"] = \"deleterious\"\n",
    "                            consensus.append(\"-\")\n",
    "                        else:\n",
    "                            prediction[\"prediction\"] = \"tolerated\"\n",
    "                            consensus.append(\"+\")\n",
    "                        predictions.append(prediction)\n",
    "            except:\n",
    "                prediction[\"predictor\"] = \"\"\n",
    "                prediction[\"value\"] = \"\"\n",
    "                try:\n",
    "                    effect = variations_uniprot_json[\"features\"][variant][\"clinicalSignificances\"][0][\"type\"]\n",
    "                    if \"benign\" in effect.lower() or \"tolerated\" in effect.lower():\n",
    "                        prediction[\"prediction\"] = \"tolerated\"\n",
    "                        consensus.append(\"+\")\n",
    "                    elif \"deleterious\" in effect.lower() or \"damaging\" in effect.lower():\n",
    "                        prediction[\"prediction\"] = \"deleterious\"\n",
    "                        consensus.append(\"-\")\n",
    "                    else:\n",
    "                        prediction[\"prediction\"] = \"not indicated\"\n",
    "                except:\n",
    "                    prediction[\"prediction\"] = \"not indicated\"\n",
    "                predictions.append(prediction)\n",
    "\n",
    "        elif mutation_type.lower() == \"stop gained\":\n",
    "            mutation[\"alternative residue\"] = \"termination\"\n",
    "            prediction[\"predictor\"] = \"\"\n",
    "            prediction[\"value\"] = \"\"\n",
    "            try:\n",
    "                effect = variations_uniprot_json[\"features\"][variant][\"clinicalSignificances\"][0][\"type\"]\n",
    "                if \"benign\" in effect.lower() or \"tolerated\" in effect.lower():\n",
    "                    prediction[\"prediction\"] = \"tolerated\"\n",
    "                    consensus.append(\"+\")\n",
    "                elif \"deleterious\" in effect.lower() or \"damaging\" in effect.lower():\n",
    "                    prediction[\"prediction\"] = \"deleterious\"\n",
    "                    consensus.append(\"-\")\n",
    "                else:\n",
    "                    prediction[\"prediction\"] = \"not indicated\"\n",
    "            except:\n",
    "                prediction[\"prediction\"] = \"not indicated\"\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        elif mutation_type.lower() == \"inframe deletion\":\n",
    "            mutation[\"alternative residue\"] = \"missing\"\n",
    "            prediction[\"predictor\"] = \"\"\n",
    "            prediction[\"value\"] = \"\"\n",
    "            try:\n",
    "                effect = variations_uniprot_json[\"features\"][variant][\"clinicalSignificances\"][0][\"type\"]\n",
    "                if \"benign\" in effect.lower() or \"tolerated\" in effect.lower():\n",
    "                    prediction[\"prediction\"] = \"tolerated\"\n",
    "                    consensus.append(\"+\")\n",
    "                elif \"deleterious\" in effect.lower() or \"damaging\" in effect.lower():\n",
    "                    prediction[\"prediction\"] = \"deleterious\"\n",
    "                    consensus.append(\"-\")\n",
    "                else:\n",
    "                    prediction[\"prediction\"] = \"not indicated\"\n",
    "            except:\n",
    "                prediction[\"prediction\"] = \"not indicated\"\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        elif mutation_type.lower() == \"frameshift\":\n",
    "            mutation[\"alternative residue\"] = \"\"\n",
    "            prediction[\"predictor\"] = \"\"\n",
    "            prediction[\"value\"] = \"\"\n",
    "            try:\n",
    "                effect = variations_uniprot_json[\"features\"][variant][\"clinicalSignificances\"][0][\"type\"]\n",
    "                if \"benign\" in effect.lower() or \"tolerated\" in effect.lower():\n",
    "                    prediction[\"prediction\"] = \"tolerated\"\n",
    "                    consensus.append(\"+\")\n",
    "                elif \"deleterious\" in effect.lower() or \"damaging\" in effect.lower():\n",
    "                    prediction[\"prediction\"] = \"deleterious\"\n",
    "                    consensus.append(\"-\")\n",
    "                else:\n",
    "                        prediction[\"prediction\"] = \"not indicated\"\n",
    "            except:\n",
    "                prediction[\"prediction\"] = \"not indicated\"\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        elif mutation_type.lower() == \"stop lost\":\n",
    "            mutation[\"alternative residue\"] = variations_uniprot_json[\"features\"][variant][\"mutatedType\"]\n",
    "            prediction[\"predictor\"] = \"\"\n",
    "            prediction[\"value\"] = \"\"\n",
    "            try:\n",
    "                effect = variations_uniprot_json[\"features\"][variant][\"clinicalSignificances\"][0][\"type\"]\n",
    "                if \"benign\" in effect.lower() or \"tolerated\" in effect.lower():\n",
    "                    prediction[\"prediction\"] = \"tolerated\"\n",
    "                    consensus.append(\"+\")\n",
    "                elif \"deleterious\" in effect.lower() or \"damaging\" in effect.lower():\n",
    "                    prediction[\"prediction\"] = \"deleterious\"\n",
    "                    consensus.append(\"-\")\n",
    "                else:\n",
    "                    prediction[\"prediction\"] = \"not indicated\"\n",
    "            except:\n",
    "                prediction[\"prediction\"] = \"not indicated\"\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        else: #skip if it not a missense, frameshift, stop gained, stop lost, inframe deletion\n",
    "            continue\n",
    "\n",
    "        mutation[\"effect(s)\"] = predictions\n",
    "\n",
    "        if len(consensus) == 0:\n",
    "                consensus_ = \"no consensus\"\n",
    "        else:\n",
    "            for element in consensus:\n",
    "                if len(list(set(consensus))) == 1:\n",
    "                    if list(set(consensus))[0] == \"+\":\n",
    "                        consensus_ = \"tolerated\"\n",
    "                    elif list(set(consensus))[0] == \"-\":\n",
    "                        consensus_ = \"deleterious\"\n",
    "                else:\n",
    "                    consensus_ = \"no consensus\"\n",
    "\n",
    "        mutation[\"consensus effect\"] = consensus_\n",
    "\n",
    "        mutation[\"reference\"] = f\"https://www.uniprot.org/uniprotkb/{uniprot_id}/variant-viewer\"\n",
    "        mutations_Uniprot.append(mutation)\n",
    "\n",
    "    return mutations_Uniprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pharmaco_info_GPCRdb(pharmaco_data_json):\n",
    "    pharmaco = []\n",
    "    for dic in pharmaco_data_json:\n",
    "        pharmaco_dic = {}\n",
    "        pharmaco_dic[\"start\"]= dic[\"mutation_pos\"]\n",
    "        pharmaco_dic[\"end\"]= dic[\"mutation_pos\"]\n",
    "        pharmaco_dic[\"original residue\"]= dic[\"mutation_from\"]\n",
    "        pharmaco_dic[\"alternative residue\"]= dic[\"mutation_to\"]\n",
    "        pharmaco_dic[\"studied parameter\"]= dic[\"exp_type\"]\n",
    "        DB = dic[\"ligand_id\"]\n",
    "        pharmaco_dic[\"ligand\"]= dic[\"ligand_name\"]\n",
    "        if \"CHEMBL\" in DB: #chembl\n",
    "            pharmaco_dic[\"link ligand\"] = f\"https://www.ebi.ac.uk/chembl/compound_report_card/{DB}/\"\n",
    "        elif DB.isnumeric(): #pubchem\n",
    "            pharmaco_dic[\"link ligand\"] = f\"https://pubchem.ncbi.nlm.nih.gov/compound/{DB}\"\n",
    "        else:\n",
    "            pharmaco_dic[\"link ligand\"] = \"\"\n",
    "        pharmaco_dic[\"ligand type\"]= \"\"\n",
    "        effect_value = round(dic[\"exp_fold_change\"],1)\n",
    "        if effect_value < 0:\n",
    "            impact = \"increase\"\n",
    "            effect_value = effect_value*-1\n",
    "        else:\n",
    "            impact = \"decrease\"\n",
    "\n",
    "        if effect_value == 0.0:\n",
    "            pharmaco_dic[\"effect\"]=\"None\"\n",
    "        else:\n",
    "            pharmaco_dic[\"effect\"]= str(effect_value) + \" fold \" + impact\n",
    "\n",
    "        pharmaco_dic[\"reference\"]= dic[\"reference\"]\n",
    "\n",
    "        pharmaco.append(pharmaco_dic)\n",
    "    return pharmaco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sq_atom_distance(i, j):\n",
    "    \"\"\"Squared euclidean distance between two 3d points\"\"\"\n",
    "    return (i[0] - j[0]) * (i[0] - j[0]) + \\\n",
    "            (i[1] - j[1]) * (i[1] - j[1]) + \\\n",
    "            (i[2] - j[2]) * (i[2] - j[2])\n",
    "\n",
    "def identify_gaps(pdb_file, chain_pdb, offset, end): #code modified from pdb_gap.py file from pdbtools Copyright 2018 João Pedro Rodrigues\n",
    "    fhandle = open(pdb_file, 'r')\n",
    "    centroid = ' CA '  # respect spacing. 'CA  ' != ' CA '\n",
    "    distance_threshold = 4.0 * 4.0\n",
    "    prev_at = (None, None, None, None, (None, None, None))\n",
    "    model = 0\n",
    "    n_gaps = 0\n",
    "    gap = []\n",
    "    for line in fhandle:\n",
    "\n",
    "        if line.startswith('MODEL'):\n",
    "            model = int(line[10:14])\n",
    "\n",
    "        elif line.startswith('ATOM'):\n",
    "            atom_name = line[12:16]\n",
    "            if atom_name != centroid:\n",
    "                continue\n",
    "\n",
    "            resn = line[17:20]\n",
    "            resi = int(line[22:26])\n",
    "            chain = line[21]\n",
    "            x = float(line[30:38])\n",
    "            y = float(line[38:46])\n",
    "            z = float(line[46:54])\n",
    "\n",
    "            at_uid = (model, chain, resi, resn, atom_name, (x, y, z))\n",
    "            #Detects gaps both by a distance criterion or discontinuous residue numbering. Only applies to protein residues.\n",
    "            if prev_at[0] == at_uid[0] and prev_at[1] == at_uid[1]:\n",
    "                d = calculate_sq_atom_distance(at_uid[5], prev_at[5])\n",
    "                if d > distance_threshold:\n",
    "                    gap.append([prev_at[1],prev_at[2],at_uid[1],at_uid[2]])\n",
    "                    n_gaps += 1\n",
    "                elif prev_at[2] + 1 != at_uid[2]:\n",
    "                    gap.append([prev_at[1],prev_at[2],at_uid[1],at_uid[2]])\n",
    "                    n_gaps += 1\n",
    "\n",
    "            prev_at = at_uid\n",
    "\n",
    "    gaps_cleaned = []\n",
    "    start = offset\n",
    "    for section in gap:\n",
    "        if section[0] == chain_pdb and section[2] == chain_pdb:\n",
    "            stop = section[1]\n",
    "            if start < 1000 and stop < 1000:\n",
    "                gaps_cleaned.append([start,stop])\n",
    "            start = section[3]\n",
    "    gaps_cleaned.append([start,end])\n",
    "    return gaps_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_duplicates(dicts):\n",
    "    # Step 1: Group dictionaries by the 'start' key\n",
    "    grouped = defaultdict(list)\n",
    "    for d in dicts:\n",
    "        grouped[d['start']].append(d)\n",
    "    \n",
    "    result = []\n",
    "    conflicts = []\n",
    "\n",
    "    # Step 2: Process each group\n",
    "    for start, items in grouped.items():\n",
    "        if len(items) > 1:\n",
    "            # Check if all 'type' values are the same\n",
    "            types = set(d['type'] for d in items)\n",
    "            if len(types) == 1:\n",
    "                # Merge 'reference' values\n",
    "                merged_references = \"\"\n",
    "                for d in items:\n",
    "                    merged_references += d['description']\n",
    "                # Create a new dictionary with merged references\n",
    "                new_dict = items[0].copy()\n",
    "                new_dict['description'] = merged_references[:-1]\n",
    "                new_dict['reference'] = \"https://www.ebi.ac.uk/pdbe/pisa/\"\n",
    "                result.append(new_dict)\n",
    "            else:\n",
    "                # Print dictionaries with different 'type' values\n",
    "                for d in items:\n",
    "                    conflicts.append(d)\n",
    "        else:\n",
    "            result.append(items[0])\n",
    "    \n",
    "    return result, conflicts\n",
    "\n",
    "def retrieve_interacting_residues_PDB(pdb_id, chain_pdb,mapping_uniprot_PDB_dict,pdb_file_path):\n",
    "    mapping_PDB_uniprot = {v: k for k, v in mapping_uniprot_PDB_dict.items()} #gives position of a aligned res in unaligned seq\n",
    "\n",
    "    # retrieve the interacting residues in the PDBs from PISA, need to make sure it doesn't take into accound the interactions between 2 sym GPCRs\n",
    "    # interacting residues is defined by a bsa > 0\n",
    "    #https://github.com/PDBe-KB/pdbe-pisa-json/blob/main/PISA-APIs.ipynb\n",
    "\n",
    "    interacting_residues_list = []\n",
    "    binders_chain= []\n",
    "    # for pdb_id, chain_pdb,uniprot_pdb_start,pdb_start in zip(pdb_ids,chain_pdbs,uniprot_pdb_starts,pdb_starts):\n",
    "    # try: #when its just 1 chain or 1 chain and a ligand PISA doesn't work\n",
    "        \n",
    "    if \"a\"==\"a\":\n",
    "        \n",
    "        response = requests.get(f\"https://www.ebi.ac.uk/pdbe/api/pisa/assembly/{pdb_id.lower()}/1\")\n",
    "        interface_count = response.json()[pdb_id.lower()][\"assembly\"][\"interface_count\"]\n",
    "        for i in range(1,interface_count+1):\n",
    "            interacting_residues = []\n",
    "            response_single_interface = requests.get(f\"https://www.ebi.ac.uk/pdbe/api/pisa/interface/{pdb_id.lower()}/1/{i}/\")\n",
    "            data = response_single_interface.json()\n",
    "            if \"/\" in chain_pdb:\n",
    "                chain_pdb = chain_pdb.split(\"/\")\n",
    "            for j in range(len(data[\"molecules\"])):\n",
    "                if isinstance(chain_pdb,str):\n",
    "                    if data[\"molecules\"][j][\"chain_id\"]==chain_pdb:\n",
    "                        for bsa,position in zip(data[\"molecules\"][j][\"buried_surface_areas\"],data[\"molecules\"][j]['residue_seq_ids']):\n",
    "                            if bsa > 0.0:\n",
    "                                try:\n",
    "                                    interacting_residues.append(mapping_PDB_uniprot[int(position)])\n",
    "                                except:\n",
    "                                    continue\n",
    "                        if j == 0: #there is supposed to be only 2 molecules, the GPCR and the interacting molecule\n",
    "                            chain_interacting_molecule = data[\"molecules\"][1][\"chain_id\"]\n",
    "                        else: \n",
    "                            chain_interacting_molecule = data[\"molecules\"][0][\"chain_id\"]\n",
    "                        binders_chain.append(extract_name_binders(chain_interacting_molecule,pdb_file_path))\n",
    "                        interacting_residues_list.append(list(set(interacting_residues)))\n",
    "\n",
    "                elif isinstance(chain_pdb,list):\n",
    "                    if chain_pdb[0] in data[\"molecules\"][j][\"chain_id\"] and chain_pdb[1] in data[\"molecules\"][j+1][\"chain_id\"]:\n",
    "                        break\n",
    "                    else:\n",
    "                        if chain_pdb[0] in data[\"molecules\"][j][\"chain_id\"] or chain_pdb[1] in data[\"molecules\"][j][\"chain_id\"]:\n",
    "                            for bsa,position in zip(data[\"molecules\"][j][\"buried_surface_areas\"],data[\"molecules\"][j]['residue_seq_ids']):\n",
    "                                if bsa >0.0:\n",
    "                                    try:\n",
    "                                        interacting_residues.append(mapping_PDB_uniprot[int(position)])\n",
    "                                    except:\n",
    "                                        continue\n",
    "                        if j == 0: #there is supposed to be only 2 molecules, the GPCR and the interacting molecule\n",
    "                            chain_interacting_molecule = data[\"molecules\"][1][\"chain_id\"]\n",
    "                        else: \n",
    "                            chain_interacting_molecule = data[\"molecules\"][0][\"chain_id\"]\n",
    "                        binders_chain.append(extract_name_binders(chain_interacting_molecule,pdb_file_path))\n",
    "                        interacting_residues_list.append(list(set(interacting_residues)))\n",
    "\n",
    "        return interacting_residues_list,binders_chain\n",
    "    # except:\n",
    "    #     return [],\"\"\n",
    "\n",
    "\n",
    "def extract_name_binders(chain_of_interest,pdb_file_path):\n",
    "    molecule_name = None\n",
    "    current_molecule = \"\"\n",
    "    reading_molecule = False\n",
    "    found_chain = False\n",
    "    \n",
    "    # Open and read the PDB file\n",
    "    with open(pdb_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith(\"COMPND\"):\n",
    "            # Start reading the molecule name if \"MOLECULE\" is in the line\n",
    "            if \"MOLECULE\" in line:\n",
    "                reading_molecule = True\n",
    "                current_molecule = line.split(\":\")[1].strip().rstrip(\";\")  # Extract initial part of the molecule name\n",
    "            # If the molecule name is being read and it continues on the next line\n",
    "            elif reading_molecule and \"CHAIN\" not in line:\n",
    "                pattern = r\"\\d+\\s+(.+)\"\n",
    "                match = re.search(pattern, line)\n",
    "                current_molecule += \" \"+match.group(1).strip().rstrip(\";\")\n",
    "            # Once we reach the chain of interest\n",
    "            if bool(re.search(rf\"CHAIN:\\s*(?:[^,]*,\\s*)*{chain_of_interest}\\b\", line)):\n",
    "                found_chain = True\n",
    "            # If molecule and chain have been found, stop reading\n",
    "            if found_chain and current_molecule and \";\" in line:\n",
    "                molecule_name = current_molecule\n",
    "                break\n",
    "    return molecule_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import PDB\n",
    "\n",
    "def extract_resolution_and_method(file_path):\n",
    "    \"\"\"\n",
    "    Extracts resolution and experimental method from a PDB or mmCIF file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the structure file (.pdb or .cif).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the resolution and experimental method.\n",
    "    \"\"\"\n",
    "    method_abb = {\"ELECTRON MICROSCOPY\":\"EM\",\"X-RAY DIFFRACTION\":\"X-RAY\",\"SOLUTION NMR\":\"NMR\", \"SOLID-STATE NMR\":\"NMR\",\n",
    "                  \"ELECTRON CRYSTALLOGRAPHY\":\"EM\"}\n",
    "    \n",
    "    if file_path.endswith(\".cif\"):\n",
    "        parser = PDB.MMCIFParser(QUIET=True)\n",
    "    elif file_path.endswith(\".pdb\"):\n",
    "        parser = PDB.PDBParser(QUIET=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a .pdb or .cif file.\")\n",
    "\n",
    "    structure = parser.get_structure(\"structure\", file_path)\n",
    "\n",
    "    # Extracting metadata from the structure header\n",
    "    header = structure.header\n",
    "\n",
    "    resolution = str(header.get(\"resolution\", \"Not available\"))+\"Å\"\n",
    "    experimental_method = method_abb[header.get(\"structure_method\", \"Not available\").upper()]\n",
    "    if experimental_method == \"NMR\":\n",
    "        resolution = \"\"\n",
    "\n",
    "    return resolution,experimental_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pdb_dsbonds_interactions(uniprot_json,uniprot_id,sequence):\n",
    "    structures = []\n",
    "    ds_bonds = []\n",
    "    pdbs = []\n",
    "    chains = []\n",
    "    uniprot_pdb_starts = []\n",
    "    pdb_starts = []\n",
    "    interacting_residues_list = []\n",
    "    binders_list= []\n",
    "    for i in range(len(uniprot_json['uniProtKBCrossReferences'])):\n",
    "        if uniprot_json['uniProtKBCrossReferences'][i]['database'] == 'PDB':\n",
    "            try:\n",
    "                pdb_id = uniprot_json['uniProtKBCrossReferences'][i]['id']\n",
    "                print(pdb_id)\n",
    "                length_chain = 0\n",
    "                if ',' in uniprot_json['uniProtKBCrossReferences'][i][\"properties\"][2][\"value\"]: #when there is a \",\" it means that there are multiple fragments, let's assume it's longer than 200 residues then\n",
    "                    sections = uniprot_json['uniProtKBCrossReferences'][i][\"properties\"][2][\"value\"].split(\",\")\n",
    "                else:\n",
    "                    sections = [uniprot_json['uniProtKBCrossReferences'][i][\"properties\"][2][\"value\"]]\n",
    "                for j in range(len(sections)):\n",
    "                    range_chain=sections[j].split(\"=\")[1].split(\"-\")\n",
    "                    length_chain += int(range_chain[1])-int(range_chain[0])\n",
    "                if length_chain > 200:\n",
    "                    print(pdb_id)\n",
    "                    #download pdb file\n",
    "                    if not os.path.exists('../data/tmp/'):\n",
    "                        os.mkdir('../data/tmp/')\n",
    "                    try:\n",
    "                        mmcif=False\n",
    "                        pdb_file_path = f'../data/tmp/{pdb_id}.pdb'\n",
    "                        urllib.request.urlretrieve(f'https://files.rcsb.org/download/{pdb_id}.pdb', pdb_file_path)\n",
    "                    except:\n",
    "                        mmcif=True\n",
    "                        # pdb_file_path = f'../data/tmp/{pdb_id}.cif'\n",
    "                        # urllib.request.urlretrieve(f'https://files.rcsb.org/download/{pdb_id}.cif', pdb_file_path)\n",
    "                    \n",
    "                    if not mmcif:\n",
    "                        #get conformational state structure => provided by GPCRdb\n",
    "                        state = \"Undetermined\" #default\n",
    "                        for structure in structures_chain:\n",
    "                            if structure[\"pdb_code\"] == pdb_id:\n",
    "                                state = structure[\"state\"]\n",
    "                                break\n",
    "                        chain_pdb =  uniprot_json['uniProtKBCrossReferences'][i][\"properties\"][2][\"value\"][0]\n",
    "                        # full_chain_pdb = uniprot_json['uniProtKBCrossReferences'][i][\"properties\"][2][\"value\"].split(\"=\")[0]\n",
    "\n",
    "                        #get uniprot to pdb mapping\n",
    "                        folder_mapping_json = \"../examples/3Dstructures/uniprot_pdb_mapping/\"\n",
    "                        mapping_file = folder_mapping_json+pdb_id+\".json\"\n",
    "                        if not os.path.exists(folder_mapping_json+pdb_id+\".json\"):\n",
    "                            mapping_uniprot_pdb_dict= mapping_uniprot_pdb.map_PDB_uniprot(pdb_id,pdb_file_path,chain_pdb,uniprot_id,sequence,folder_mapping_json, type_gpcr = \"natural\")\n",
    "                        else:\n",
    "                            mapping_uniprot_pdb_dict = json.load(open(mapping_file))\n",
    "                            mapping_uniprot_pdb_dict = {int(k): v for k, v in mapping_uniprot_pdb_dict.items()} #keys are strings\n",
    "\n",
    "                            \n",
    "                        #this is needed to find the interactions within the pdb file\n",
    "                        pdbs.append(pdb_id)\n",
    "\n",
    "                        resolution,method =  extract_resolution_and_method(pdb_file_path)\n",
    "                        if mmcif:\n",
    "                            url = f\"https://files.rcsb.org/download/{pdb_id}.cif\"\n",
    "                        else:\n",
    "                            url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\n",
    "                            \n",
    "                        structures.append({\"offset\":  0, \"gaps\": [],\"value\":pdb_id,\"chain\": chain_pdb, \"state\":state, \"mapping\": f\"file:///examples/3Dstructures/uniprot_pdb_mapping/{pdb_id}.json\", \"resolution\": resolution, \"method\": method, \"url\":url, \"reference\":f\"https://www.rcsb.org/structure/{pdb_id.upper()}\", \"date\":\"\"})\n",
    "\n",
    "                        #find interacting residues at ligand binding site and G protein binding site\n",
    "                        interacting_residues,binders = retrieve_interacting_residues_PDB(pdb_id,chain_pdb,mapping_uniprot_pdb_dict,pdb_file_path)\n",
    "                        if len(interacting_residues)>0:\n",
    "                            interacting_residues_list.append(interacting_residues)\n",
    "                            binders_list.append(binders)\n",
    "\n",
    "                        #remove pdb file\n",
    "                        os.remove(pdb_file_path)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                with open(\"to_look_at_pdbs.txt\",\"a\") as f:\n",
    "                    f.write(pdb_id+\"\\n\")\n",
    "                    f.close()\n",
    "  \n",
    "    return structures, interacting_residues_list, binders_list, pdbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPCRdb finds sodium pockets\n",
    "#As microswitches are well defined in literature we can check ourselves if these well knwon microswitches are present in our gpcrs\n",
    "#All known microswitches in literature for class A\n",
    "E_DRY_W = {\"positions\":[\"3.49\", \"3.50\", \"3.51\"],\"residues\":[\"ED\", \"R\", \"WY\"], \"name\": \"E/DRY/W motif (ionic lock switch)\"}\n",
    "CWxP = {\"positions\":[\"6.47\", \"6.48\", \"6.50\"], \"residues\":[\"C\", \"W\", \"P\"], \"name\": \"CWxP motif (transmission toggle switch)\"}\n",
    "NPxxY = {\"positions\":[\"7.49\", \"7.50\", \"7.53\"], \"residues\":[\"N\",\"P\",\"Y\"], \"name\": \"NPxxY motif (tyr toggle switch)\"}\n",
    "PIF = {\"positions\": [\"5.50\", \"3.40\", \"6.44\"], \"residues\":[\"P\",\"I\",\"F\"], \"name\": \"PIF motif\"}\n",
    "hydrophobic_lock = {\"positions\":[\"3.43\",\"6.40\"], \"residues\":[\"LVIM\", \"LVIM\"], \"name\": \"hydrophobic lock\"}\n",
    "# ionic_lock = {\"positions\":[\"6.30\"], \"residues\":[\"DE\"], \"name\": \"ionic lock\"}\n",
    "#disulfide bond between TM3 and ECL2 is already identified by Uniprot in the \"Disulfide bonds\" section\n",
    "#Sodium binding pocket (allosteric action): middle of the 7TMs. Identified by GPCRdb but are the identified ones all of them???\n",
    "\n",
    "#the positions are in human readable format (not pyton - starts at 0)\n",
    "MSA_E_DRY_W = {\"positions\":[684,685,686],\"residues\":[\"ED\", \"R\", \"WY\"], \"name\": \"E/DRY/W motif (ionic lock switch)\"}\n",
    "MSA_CWxP = {\"positions\":[1191,1192,1194], \"residues\":[\"C\", \"W\", \"P\"], \"name\": \"CWxP motif (transmission toggle switch)\"}\n",
    "MSA_NPxxY = {\"positions\":[1265,1266,1269], \"residues\":[\"N\",\"P\",\"Y\"], \"name\": \"NPxxY motif (tyr toggle switch)\"}\n",
    "MSA_PIF = {\"positions\": [930,675,1187], \"residues\":[\"P\",\"I\",\"F\"], \"name\": \"PIF motif\"}\n",
    "MSA_hydrophobic_lock = {\"positions\":[678,1183], \"residues\":[\"LVIM\", \"LVIM\"], \"name\": \"Hydrophobic lock\"}\n",
    "# MSA_ionic_lock = {\"positions\":[1190], \"residues\":[\"DE\"], \"name\": \"Ionic lock\"}\n",
    "# MSA_sodium_pocket = {\"positions\":[620,684], \"residues\":[\"D\",\"S\"], \"name\": \"Sodium binding pocket\"}\n",
    "\n",
    "TM1x50={\"positions\":[579],\"residues\":[\"N\"], \"name\": \"1.50 (BW numbering)\"}\n",
    "TM2x50={\"positions\":[620],\"residues\":[\"D\"], \"name\": \"2.50 (BW numbering)\"}\n",
    "TM3x50={\"positions\":[685],\"residues\":[\"R\"], \"name\": \"3.50 (BW numbering)\"}\n",
    "TM4x50={\"positions\":[729],\"residues\":[\"W\"], \"name\": \"4.50 (BW numbering)\"}\n",
    "TM5x50={\"positions\":[930],\"residues\":[\"P\"], \"name\": \"5.50 (BW numbering)\"}\n",
    "TM6x50={\"positions\":[1194],\"residues\":[\"P\"], \"name\": \"6.50 (BW numbering)\"}\n",
    "TM7x50={\"positions\":[1266],\"residues\":[\"P\"], \"name\": \"7.50 (BW numbering)\"}\n",
    "\n",
    "#equivalence between positions in sequence and in MSA\n",
    "#dictionary with list of list. In every sublist, 2 elements, 1st is the position in sequence, the 2nd the position in MSA\n",
    "def map_seq_MSA(sequence_aligned):\n",
    "    previous = 0\n",
    "    translate = {}\n",
    "    sequence_nogaps = sequence_aligned.replace(\"-\",\"\")\n",
    "    for res in range(len(sequence_nogaps)):\n",
    "        idx_msa = previous + sequence_aligned[previous:].index(sequence_nogaps[res])\n",
    "        translate[res+1]=idx_msa+1\n",
    "        previous = idx_msa + 1\n",
    "    return translate\n",
    "\n",
    "#Microswitches/motifs - identify them based on their defined columns in mammalian MSA\n",
    "def motifs_microswitches_literature(MSA,uniprot_id):\n",
    "\n",
    "    alignment = AlignIO.read(open(MSA), \"fasta\")\n",
    "    len_MSA=alignment.get_alignment_length()\n",
    "    record_dict = SeqIO.index(MSA, \"fasta\")\n",
    "    aligned_seq_interest = str(record_dict[uniprot_id].seq)\n",
    "    translate_seq_MSA = map_seq_MSA(aligned_seq_interest) #gives position of a unaligned res in msa\n",
    "    translate_MSA_seq = {v: k for k, v in translate_seq_MSA.items()} #gives position of a aligned res in unaligned seq\n",
    "    # microswitch_types = [MSA_E_DRY_W, MSA_CWxP, MSA_NPxxY, MSA_PIF, MSA_hydrophobic_lock,\n",
    "    #                      TM1x50,TM2x50,TM3x50,TM4x50,TM5x50,TM6x50,TM7x50]\n",
    "    microswitch_types = [MSA_E_DRY_W, MSA_CWxP, MSA_NPxxY, MSA_PIF, MSA_hydrophobic_lock,]\n",
    "    \n",
    "    microswitches = []\n",
    "    microswitches_residues = []\n",
    "\n",
    "    for microswitch_type in microswitch_types:\n",
    "        are_there = []\n",
    "        for position, residue in zip(microswitch_type[\"positions\"], microswitch_type[\"residues\"]):\n",
    "            if aligned_seq_interest[position-1] in residue:\n",
    "                are_there.append(True)\n",
    "            else:\n",
    "                are_there.append(False)\n",
    "        for i, (position, residue) in enumerate(zip(microswitch_type[\"positions\"], microswitch_type[\"residues\"])):\n",
    "            microswitch_residue = {}\n",
    "            \n",
    "            #take into account the possibility that there is a gap at that position in the MSA\n",
    "            if position in translate_MSA_seq:\n",
    "                microswitch_residue[\"start\"] = translate_MSA_seq[position]\n",
    "                microswitch_residue[\"end\"] = translate_MSA_seq[position]\n",
    "                residue_motif = aligned_seq_interest[position-1]\n",
    "            else:\n",
    "                for next in range(position+1,len_MSA):\n",
    "                    if next in translate_MSA_seq:\n",
    "                        microswitch_residue[\"start\"] = translate_MSA_seq[next]\n",
    "                        microswitch_residue[\"end\"] = translate_MSA_seq[next]\n",
    "                        residue_motif = aligned_seq_interest[next-1]\n",
    "                        break\n",
    "\n",
    "            if not all(are_there) and not are_there[i]:\n",
    "                if residue_motif == \"F\" and microswitch_type[\"name\"]==\"PIF motif\":\n",
    "                    microswitch_residue[\"description\"] = residue_motif + \" instead of \"+ residue+ \" from \" + \" (part of \" + microswitch_type[\"name\"]+ \")\"\n",
    "                elif residue_motif == \"R\" and microswitch_type[\"name\"]==\"E/DRY/W motif (ionic lock switch)\":\n",
    "                    microswitch_residue[\"description\"] = residue_motif + \" instead of \"+ residue + \" (part of \" + microswitch_type[\"name\"]+ \")\"\n",
    "                elif \"(BW numbering)\" in microswitch_type[\"name\"]:\n",
    "                    microswitch_residue[\"description\"] = residue_motif+ \" instead of \"+ residue + \" \" + microswitch_type[\"name\"]\n",
    "                else:\n",
    "                    microswitch_residue[\"description\"] = residue_motif + \" instead of \"+ residue + \" (part of \" + microswitch_type[\"name\"]+ \")\"\n",
    "            else:\n",
    "                if residue_motif == \"F\" and microswitch_type[\"name\"]==\"PIF motif\":\n",
    "                    microswitch_residue[\"description\"] = residue_motif + \" part of \" + microswitch_type[\"name\"] +\" and hydrophobic lock\"\n",
    "                elif residue_motif == \"R\" and microswitch_type[\"name\"]==\"E/DRY/W motif (ionic lock switch)\":\n",
    "                    microswitch_residue[\"description\"] = residue_motif+ \" part of \" + microswitch_type[\"name\"] +\" and ionic lock\"\n",
    "                elif \"(BW numbering)\" in microswitch_type[\"name\"]:\n",
    "                    microswitch_residue[\"description\"] = residue_motif+ \" \" + microswitch_type[\"name\"]\n",
    "                else:\n",
    "                    microswitch_residue[\"description\"] = residue_motif+ \" part of \" + microswitch_type[\"name\"]\n",
    "            if are_there[i]:\n",
    "                microswitch_residue[\"conserved\"] = \"yes\"\n",
    "            else:\n",
    "                microswitch_residue[\"conserved\"] = \"no\"\n",
    "            microswitch_residue[\"reference\"] = \"Based on alignment\"\n",
    "            microswitches_residues.append(microswitch_residue)  \n",
    "\n",
    "    return microswitches_residues\n",
    "\n",
    "\n",
    "#this is our numbering based on the BW numbering definition\n",
    "def computeBW_numbering(uniprotID,abb_name,all_regions,MSA):\n",
    "\n",
    "    BWnumbering_gpcrdb = retrieve_numberingGPCRdb(abb_name)\n",
    "\n",
    "    aligned_seq_interest = str(SeqIO.to_dict(SeqIO.parse(MSA,\"fasta\"))[uniprotID].seq)\n",
    "    translate_seq_MSA = map_seq_MSA(aligned_seq_interest) #gives position of a unaligned res in msa\n",
    "    translate_MSA_seq = {v: k for k, v in translate_seq_MSA.items()} #gives position of a aligned res in unaligned seq\n",
    "\n",
    "    BW_conserved_positions = [TM1x50,TM2x50,TM3x50,TM4x50,TM5x50,TM6x50,TM7x50]\n",
    "\n",
    "    uniprot_BW_mapping =  {i + 1: \"\" for i in range(len(aligned_seq_interest.replace(\"-\",\"\")))}\n",
    "    counter = 1\n",
    "\n",
    "    for TMname in all_regions:\n",
    "        if \"TM\" in TMname[\"name\"]:\n",
    "            position_50 = BW_conserved_positions[counter-1][\"positions\"][0]\n",
    "            try:\n",
    "                position_50_seq = translate_MSA_seq[position_50]\n",
    "            except:\n",
    "                try:\n",
    "                    position_50_seq = translate_MSA_seq[position_50+1]\n",
    "                except:\n",
    "                    try:\n",
    "                        position_50_seq = translate_MSA_seq[position_50-1]\n",
    "                    except:\n",
    "                        print('This entry has no residue that align with most conserved residue of TM, neither one left or right of it, weird')\n",
    "            numberTM = str(counter)+\".\"\n",
    "\n",
    "            uniprot_BW_mapping[position_50_seq]+=(numberTM+str(50)) \n",
    "            uniprot_BW_mapping[position_50_seq]+= f\" | {BWnumbering_gpcrdb[int(position_50_seq)]}\"\n",
    "\n",
    "            start = TMname[\"start\"]\n",
    "            end = TMname[\"end\"]\n",
    "            distance = 1\n",
    "            for residue in range(position_50_seq-1,start-1,-1):\n",
    "                pos = 50-distance\n",
    "                uniprot_BW_mapping[residue]+=(numberTM+str(pos))\n",
    "                uniprot_BW_mapping[residue]+= f\" | {BWnumbering_gpcrdb[int(residue)]}\"\n",
    "                distance +=1\n",
    "\n",
    "            distance = 1\n",
    "            for residue in range(position_50_seq+1,end+1):\n",
    "                pos = 50+distance\n",
    "                uniprot_BW_mapping[residue]+=(numberTM+str(pos))\n",
    "                uniprot_BW_mapping[residue]+= f\" | {BWnumbering_gpcrdb[int(residue)]}\"\n",
    "                distance +=1\n",
    "            \n",
    "            counter +=1\n",
    "            \n",
    "    for pos, value in uniprot_BW_mapping.items():\n",
    "        if len(value)==0:\n",
    "            uniprot_BW_mapping[pos]=f\"N/A | {BWnumbering_gpcrdb[int(pos)]}\"\n",
    "    \n",
    "    return uniprot_BW_mapping\n",
    "\n",
    "def retrieve_numberingGPCRdb(abb_name):\n",
    "    url = f\"https://gpcrdb.org/services/residues/extended/{abb_name.lower()}/\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data_gpcr = response.json()\n",
    "\n",
    "        BW_numbering = {}\n",
    "        for info_position in data_gpcr:\n",
    "            position = info_position[\"sequence_number\"]\n",
    "            BW_info = info_position[\"display_generic_number\"]\n",
    "            if BW_info is None:\n",
    "                BW_info = \"N/A\"\n",
    "            elif BW_info.startswith(\"8.\"): #gpcrdb also numbers H8\n",
    "                BW_info = \"N/A\"\n",
    "            else:\n",
    "                BW_info = BW_info\n",
    "            \n",
    "            #special cases: \n",
    "            if abb_name == \"OXER1_HUMAN\":\n",
    "                position= position - 39\n",
    "\n",
    "            if abb_name == \"MCHR1_HUMAN\" or abb_name == \"MCHR1_PANTR\":\n",
    "                position= position - 69\n",
    "\n",
    "            if abb_name == \"MCHR1_MACMU\":\n",
    "                position= position - 35\n",
    "\n",
    "            if position >0:\n",
    "                BW_numbering[position]=BW_info\n",
    "        return BW_numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features from Uniprot binding site, PTM, natural variants\n",
    "def features_uniprot(uniprot_json,uniprot_id):\n",
    "    binding_sites = []\n",
    "    PTMs = []\n",
    "    disulfide_bonds = []\n",
    "    mutagenesiss = []\n",
    "\n",
    "    for i in range(len(uniprot_json['features'])):\n",
    "\n",
    "        #Motifs/Microswitches Uniprot are already identified by the self written code above where we check if all well defined microswitches knwon in literature are present/absent in the GPCR of interest\n",
    "\n",
    "        #Binding site (orthosteric & allosteric)\n",
    "        if uniprot_json['features'][i]['type'] == 'Binding site':\n",
    "            binding_site = {}\n",
    "            binding_site['start'] = uniprot_json['features'][i]['location']['start']['value']\n",
    "            binding_site['end'] = uniprot_json['features'][i]['location']['end']['value']\n",
    "            binding_site['type'] = uniprot_json['features'][i]['ligand']['name']\n",
    "            binding_site['description'] = \"Orthosteric or allosteric extracellular binding site.\"\n",
    "            binding_site[\"reference\"] = f\"https://www.uniprot.org/uniprotkb/{uniprot_id}/entry\"\n",
    "            binding_sites.append(binding_site)\n",
    "        ##PTMs\n",
    "        #Glycosylation\n",
    "        elif uniprot_json['features'][i]['type'] == 'Glycosylation' or uniprot_json['features'][i]['type'] == 'Lipidation' or uniprot_json['features'][i]['type'] == 'Modified residue':\n",
    "            PTM = {}\n",
    "            if uniprot_json['features'][i]['type'] == 'Glycosylation':\n",
    "                PTM['start'] = uniprot_json['features'][i]['location']['start']['value']\n",
    "                PTM['end'] = uniprot_json['features'][i]['location']['end']['value']\n",
    "                PTM['description'] = 'Glycosylation'\n",
    "            elif uniprot_json['features'][i]['type'] == 'Lipidation':\n",
    "                PTM['start'] = uniprot_json['features'][i]['location']['start']['value']\n",
    "                PTM['end'] = uniprot_json['features'][i]['location']['end']['value']\n",
    "                PTM['description'] = 'Lipidation'\n",
    "            else:\n",
    "                ptm_types = [\"phospho\",\"methyl\",\"acetyl\", \"amid\", \"pyrro\", \"hydroxy\", \"l-\", \"d-\", \"sulf\",\"nitro\"]\n",
    "                full_ptm_types = [\"Phosphorylation\", \"Methylation\", \"Acetylation\", \"Amidation\", \"Pyrrolidone carboxylic acid\", \"Hydroxylation\", \"Isomerization\", \"Isomerization\", \"Sulfation\", \"Nitrosylation\"]\n",
    "                description = uniprot_json['features'][i]['description']\n",
    "                for idx, ptm_type in enumerate(ptm_types):\n",
    "                    if ptm_type in uniprot_json['features'][i]['description'].lower():\n",
    "                        description = full_ptm_types[idx]\n",
    "                        break\n",
    "                PTM['start'] = uniprot_json['features'][i]['location']['start']['value']\n",
    "                PTM['end'] = uniprot_json['features'][i]['location']['end']['value']\n",
    "                PTM['description'] = description\n",
    "            PTM['reference'] = f\"https://www.uniprot.org/uniprotkb/{uniprot_id}/entry\"\n",
    "            PTMs.append(PTM)\n",
    "\n",
    "        # #Disulfide bond\",\n",
    "        elif uniprot_json['features'][i]['type'] == 'Disulfide bond':\n",
    "            disulfide_bond = {}\n",
    "            disulfide_bond['start'] = uniprot_json['features'][i]['location']['start']['value'] #start and end actually mean between residue x and y - start & end are the residues involved in disulfide bridge\"\n",
    "            disulfide_bond['end'] = uniprot_json['features'][i]['location']['end']['value'] #start and end actually mean between residue x and y - start & end are the residues involved in disulfide bridge\"\n",
    "            disulfide_bond[\"description\"] = \"Disulfide bond\"\n",
    "            disulfide_bond[\"reference\"] = \"UniProt\"\n",
    "            disulfide_bonds.append(disulfide_bond)\n",
    "\n",
    "    return binding_sites,PTMs,disulfide_bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve PTMs on Scop3P and compare with PTMs we have already found\n",
    "def retrieve_PTM_Scop3P(uniprot_id):\n",
    "    requestURL = f\"https://iomics.ugent.be/scop3p/api/modifications?accession={uniprot_id}\"\n",
    "    try: \n",
    "        r = requests.get(requestURL)\n",
    "        if not r.ok:\n",
    "            r.raise_for_status()\n",
    "            sys.exit()\n",
    "        scop3P_PTM = json.loads(r.text)\n",
    "    except:\n",
    "        scop3P_PTM = []\n",
    "\n",
    "    if len(scop3P_PTM)>0:\n",
    "\n",
    "        #add if not alrady there\n",
    "        ptms = []\n",
    "        for ptm in scop3P_PTM[\"modifications\"]:\n",
    "            position = ptm['position']\n",
    "            # if not position in positions_previous:\n",
    "            ptm_dict = {'start': position , 'end': position, 'description': 'Phosphorylation', 'reference': \"https://iomics.ugent.be/scop3p/index?protein={uniprot_id}\"}\n",
    "            ptms.append(ptm_dict)\n",
    "    else:\n",
    "        ptms=[]\n",
    "    return ptms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('TM1', (34, 60)), ('TM2', (68, 93)), ('TM3', (102, 135)), ('TM4', (147, 168)), ('TM5', (190, 218)), ('TM6', (229, 258)), ('TM7', (267, 287))]\n",
      "{'TM1': [[31, 35], [60, 60]], 'TM2': [[67, 67], [94, 95]], 'TM3': [[101, 104], [135, 135]], 'TM4': [[145, 145], [168, 169]], 'TM5': [[185, 188], [217, 219]], 'TM6': [[224, 229], [257, 258]], 'TM7': [[265, 267], [289, 290]]}\n",
      "{'TM1': (34, 60), 'TM2': (67, 94), 'TM3': (102, 135), 'TM4': (145, 168), 'TM5': (188, 218), 'TM6': (229, 258), 'TM7': (267, 289)}\n",
      "[{'name': 'Nterm', 'start': 1, 'end': 33, 'reference': 'DSSP'}, {'name': 'TM1', 'start': 34, 'end': 60, 'reference': 'DSSP'}, {'name': 'ICL1', 'start': 61, 'end': 66, 'reference': 'DSSP'}, {'name': 'TM2', 'start': 67, 'end': 94, 'reference': 'DSSP'}, {'name': 'ECL1', 'start': 95, 'end': 101, 'reference': 'DSSP'}, {'name': 'TM3', 'start': 102, 'end': 135, 'reference': 'DSSP'}, {'name': 'ICL2', 'start': 136, 'end': 144, 'reference': 'DSSP'}, {'name': 'TM4', 'start': 145, 'end': 168, 'reference': 'DSSP'}, {'name': 'ECL2', 'start': 169, 'end': 187, 'reference': 'DSSP'}, {'name': 'TM5', 'start': 188, 'end': 218, 'reference': 'DSSP'}, {'name': 'ICL3', 'start': 219, 'end': 228, 'reference': 'DSSP'}, {'name': 'TM6', 'start': 229, 'end': 258, 'reference': 'DSSP'}, {'name': 'ECL3', 'start': 259, 'end': 266, 'reference': 'DSSP'}, {'name': 'TM7', 'start': 267, 'end': 289, 'reference': 'DSSP'}, {'name': 'H8&Cterm', 'start': 290, 'end': 353, 'reference': 'DSSP'}]\n"
     ]
    }
   ],
   "source": [
    "def run_dssp(uniprotID, type =\"natural\"):\n",
    "\n",
    "    dssp_folder = \"../examples/3Dstructures/dssp/\"\n",
    "    dssp_filename = dssp_folder+uniprotID+\".dssp\"\n",
    "    remove_tmp = False\n",
    "    mapping_uniprot_pdb_numbering = None\n",
    "\n",
    "    if not os.path.exists(dssp_filename):\n",
    "        try: \n",
    "            pdb_representative = representative_structures_json[uniprotID][\"pdb_id\"]\n",
    "            gpcr_chain = representative_structures_json[uniprotID][\"gpcr_chain\"]\n",
    "            print(\"Problem, GPCR has exp structure and has no DSSP file\", pdb_representative, gpcr_chain)\n",
    "            #add chimera exp structure\n",
    "        except:\n",
    "            #AF\n",
    "            #check if gpcrdb has updated model\n",
    "            if type == \"natural\":\n",
    "                if os.path.exists(f\"../examples/3Dstructures/AF_gpcrdb_2024/{uniprotID}.pdb\"):\n",
    "                    input_file = f\"../examples/3Dstructures/AF_gpcrdb_2024/{uniprotID}.pdb\"\n",
    "                    gpcr_chain = \"A\"\n",
    "                else:\n",
    "                    #AlphaFold2 DB\n",
    "                    input_file = f'../examples/3Dstructures/tmp/{uniprotID}.pdb'\n",
    "                    urllib.request.urlretrieve(f\"https://alphafold.ebi.ac.uk/files/AF-{uniprotID}-F1-model_v4.pdb\", input_file)\n",
    "                    gpcr_chain = \"A\"\n",
    "                    remove_tmp = True\n",
    "            else:\n",
    "                if os.path.exists(f\"../examples/3Dstructures/AF_chimera_2024/{uniprotID}.pdb\"):\n",
    "                    input_file = f\"../examples/3Dstructures/AF_chimera_2024/{uniprotID}.pdb\"\n",
    "                    gpcr_chain = \"A\"\n",
    "            \n",
    "            subprocess.run(\n",
    "                    [\"mkdssp\", input_file, dssp_filename])\n",
    "                \n",
    "            if remove_tmp:\n",
    "                os.remove(input_file)\n",
    "        return dssp_filename, gpcr_chain, mapping_uniprot_pdb_numbering\n",
    "\n",
    "    else:\n",
    "        try: \n",
    "            pdb_representative = representative_structures_json[uniprotID][\"pdb_id\"]\n",
    "            gpcr_chain = representative_structures_json[uniprotID][\"gpcr_chain\"]\n",
    "            mapping_uniprot_pdb_numbering = json.load(open(\"../examples/3Dstructures/uniprot_pdb_mapping/\"+pdb_representative+\".json\"))\n",
    "            mapping_uniprot_pdb_numbering = {int(k): v for k, v in mapping_uniprot_pdb_numbering.items()} #keys are strings\n",
    "        except: #AF model\n",
    "            gpcr_chain = \"A\"\n",
    "        return dssp_filename, gpcr_chain, mapping_uniprot_pdb_numbering\n",
    "\n",
    "def find_closest(target,dict_interest):\n",
    "    if target in dict_interest:\n",
    "        min_start = dict_interest[target]\n",
    "    else:\n",
    "        closest_key = min(dict_interest.keys(), key=lambda x: abs(x - target))\n",
    "        min_start = dict_interest[closest_key]\n",
    "    return min_start\n",
    "    \n",
    "def remap_min_max_limits_prot_interest(min_max_limits,aligned_seq_interest):\n",
    "    translate_seq_MSA = map_seq_MSA(aligned_seq_interest) #gives position of a unaligned res in msa\n",
    "    translate_MSA_seq = {v: k for k, v in translate_seq_MSA.items()} #gives position of a aligned res in unaligned seq\n",
    "    min_max_limits_translated = {}\n",
    "    for TMname, limits in min_max_limits.items():\n",
    "        min_start = find_closest(limits[0][0],translate_MSA_seq)\n",
    "        max_start = find_closest(limits[0][1],translate_MSA_seq)\n",
    "        min_end = find_closest(limits[1][0],translate_MSA_seq)\n",
    "        max_end = find_closest(limits[1][1],translate_MSA_seq)\n",
    "        min_max_limits_translated[TMname]=[[min_start,max_start],[min_end,max_end]]\n",
    "    return min_max_limits_translated\n",
    "\n",
    "\n",
    "def refine_TM_regions(min_max_limits_translated,TM_regions_prot):\n",
    "    refined_TMs = {}\n",
    "    \n",
    "    # Step 1: Ensure each TM aligns with MSA-defined limits\n",
    "    for tm_label, (dssp_start, dssp_end) in TM_regions_prot:\n",
    "        if tm_label in min_max_limits_translated:\n",
    "            (min_start, max_start), (min_end, max_end) = min_max_limits_translated[tm_label]\n",
    "\n",
    "            # Step 2: Adjust start and end points based on MSA limits\n",
    "            adjusted_start = max(min_start, min(dssp_start, max_start))  # Keep within min-max range\n",
    "            adjusted_end = min(max_end, max(dssp_end, min_end))  # Keep within min-max range\n",
    "\n",
    "            # Ensure the TM region has at least 10 residues\n",
    "            if adjusted_end - adjusted_start + 1 < 10:\n",
    "                # If too short, expand towards the closest allowed limit\n",
    "                if adjusted_start > min_start:\n",
    "                    adjusted_start = max(min_start, adjusted_start - (10 - (adjusted_end - adjusted_start + 1)))\n",
    "                if adjusted_end < max_end:\n",
    "                    adjusted_end = min(max_end, adjusted_end + (10 - (adjusted_end - adjusted_start + 1)))\n",
    "\n",
    "            refined_TMs[tm_label] = (adjusted_start, adjusted_end)\n",
    "    print(refined_TMs)\n",
    "    # Step 3: Handle cases where DSSP predicts fewer than 7 TMs\n",
    "    if len(refined_TMs) < 7:\n",
    "        missing_TMs = [tm for tm in min_max_limits_translated.keys() if tm not in refined_TMs]\n",
    "        for tm in missing_TMs:\n",
    "            (min_start, max_start), (min_end, max_end) = min_max_limits_translated[tm]\n",
    "            refined_TMs[tm] = (min_start, max_end)  # Assign entire range if missing\n",
    "\n",
    "    # Step 4: Handle cases where DSSP predicts too many TMs\n",
    "    if len(refined_TMs) > 7:\n",
    "        # Merge small or overlapping helices\n",
    "        tm_keys = sorted(refined_TMs.keys(), key=lambda x: int(x[2:]))  # Sort TM labels (TM1, TM2, ...)\n",
    "        merged_TMs = {}\n",
    "        prev_tm = None\n",
    "\n",
    "        for tm in tm_keys:\n",
    "            if prev_tm is None:\n",
    "                merged_TMs[tm] = refined_TMs[tm]\n",
    "            else:\n",
    "                prev_start, prev_end = merged_TMs[prev_tm]\n",
    "                curr_start, curr_end = refined_TMs[tm]\n",
    "\n",
    "                # Merge if overlap or short segment\n",
    "                if curr_start - prev_end < 5 or (curr_end - curr_start + 1 < 10):\n",
    "                    merged_TMs[prev_tm] = (prev_start, curr_end)  # Extend previous TM\n",
    "                else:\n",
    "                    merged_TMs[tm] = refined_TMs[tm]\n",
    "\n",
    "            prev_tm = tm\n",
    "        \n",
    "        refined_TMs = merged_TMs\n",
    "\n",
    "    # Step 5: Ensure exactly 7 TM regions\n",
    "    if len(refined_TMs) < 7:\n",
    "        missing_TMs = [tm for tm in min_max_limits_translated.keys() if tm not in refined_TMs]\n",
    "        for tm in missing_TMs[: 7 - len(refined_TMs)]:  # Fill in missing TMs up to 7\n",
    "            (min_start, max_start), (min_end, max_end) = min_max_limits_translated[tm]\n",
    "            refined_TMs[tm] = (min_start, max_end)\n",
    "\n",
    "    return refined_TMs\n",
    "\n",
    "def map_min_max_limitsMSA_chimera(min_max_limits_translated_parent,aligned_seq_interest_ref,seq_chimera):\n",
    "\n",
    "    unaligned_ref = aligned_seq_interest_ref.replace(\"-\",\"\")\n",
    "\n",
    "    alignments_global = pairwise2.align.globalms(\n",
    "        unaligned_ref, seq_chimera, match=2, mismatch=-1,\n",
    "        open=-2, extend=-1,\n",
    "        one_alignment_only=True\n",
    "    )\n",
    "    aligned_ref_MSAchimera, aligned_seq_interest_chimera = alignments_global[0].seqA, alignments_global[0].seqB\n",
    "\n",
    "    #map min max limits TMs parent to MSA with chimera\n",
    "    min_max_limits_MSAchimera = {}\n",
    "    translate_seq_MSA = map_seq_MSA(aligned_ref_MSAchimera) #gives position of a unaligned res in msa\n",
    "    for TMname, limits in min_max_limits_translated_parent.items():\n",
    "        min_start = translate_seq_MSA[limits[0][0]]\n",
    "        max_start = translate_seq_MSA[limits[0][1]]\n",
    "        min_end = translate_seq_MSA[limits[1][0]]\n",
    "        max_end = translate_seq_MSA[limits[1][1]]\n",
    "        min_max_limits_MSAchimera[TMname]=[[min_start,max_start],[min_end,max_end]]\n",
    "    return min_max_limits_MSAchimera,aligned_seq_interest_chimera\n",
    "\n",
    "def compute_dssp_TM_regions(uniprotID,MSA,type_gpcr = \"natural\",ref_id=None, seq_chimera = None):\n",
    "\n",
    "    record_dict = SeqIO.index(MSA, \"fasta\")\n",
    "    if type_gpcr == \"natural\":\n",
    "        aligned_seq_interest = str(record_dict[uniprotID].seq)\n",
    "        length_prot = len(aligned_seq_interest.replace(\"-\",\"\"))\n",
    "    else:\n",
    "        length_prot = len(seq_chimera)\n",
    "        aligned_seq_interest_ref = str(record_dict[ref_id].seq)\n",
    "\n",
    "    dssp_filename, gpcr_chain, mapping_uniprot_pdb_numbering  = run_dssp(uniprotID,type_gpcr) #run dssp if needed on AF model else retrieve dssp file\n",
    "    dssp_tup = make_dssp_dict(dssp_filename)\n",
    "    dssp_dic = dssp_tup[0]\n",
    "\n",
    "    # Extract DSSP codes along with their actual residue positions\n",
    "    dssp_positions = []\n",
    "    for key, value in dssp_dic.items():\n",
    "        chain, res_info = key  # Extract chain and residue details\n",
    "        if chain == gpcr_chain:  # Filter only the chain of interest\n",
    "            res_id = res_info[1]  # Actual residue position in the protein\n",
    "            dssp_code = value[1]  # DSSP secondary structure code\n",
    "            dssp_positions.append((res_id, dssp_code))\n",
    "\n",
    "    # Replace '-' with 'X' in DSSP codes\n",
    "    dssp_positions = [(res_id, code.replace('-', 'X')) for res_id, code in dssp_positions]\n",
    "\n",
    "    # Define conserved secondary structure elements (Helix structures)\n",
    "    conserved_2structure_dssp = {\"H\", \"I\", \"G\"}\n",
    "\n",
    "    # Filter for helix positions\n",
    "    helix_positions = [res_id for res_id, code in dssp_positions if code in conserved_2structure_dssp]\n",
    "    if mapping_uniprot_pdb_numbering != None:\n",
    "        helix_positions_renumbered = []\n",
    "        mapping_pdb_uniprot_numbering = {v: k for k, v in mapping_uniprot_pdb_numbering.items()}\n",
    "        for pos in helix_positions:\n",
    "            try:\n",
    "                helix_positions_renumbered.append(mapping_pdb_uniprot_numbering[int(pos)])\n",
    "            except: #not part of GPCR\n",
    "                continue\n",
    "        helix_positions = helix_positions_renumbered\n",
    "    TM_regions = []\n",
    "    #find start and end anchor points\n",
    "    start = helix_positions[0]\n",
    "    stop = None\n",
    "    counter = 1\n",
    "    min_tm_length = 10 #a TM are typically at least 10 residues long otherwise they are considered as small helices connecting the TMs\n",
    "    for i in range(1, len(helix_positions)):\n",
    "        if helix_positions[i] - helix_positions[i - 1] > 1:\n",
    "            if helix_positions[i-1] - start >= min_tm_length :\n",
    "                stop = helix_positions[i-1]\n",
    "            else:\n",
    "                start = helix_positions[i]\n",
    "        if stop != None:\n",
    "            # if stop-start+1 > 3: #we need at least 3 consecutive columns with enough conserved secondary structure elements before considering it as an anchor region\n",
    "            TM_regions.append((\"TM\"+str(counter),(start,stop)))\n",
    "            counter +=1\n",
    "            stop = None\n",
    "            start = helix_positions[i]\n",
    "    TM_regions.append((\"TM\"+str(counter),(start,helix_positions[-1]))) #!this might include H8 but will be trimmed out later by code\n",
    "    print(TM_regions)\n",
    "    #adapt TM regions based on 50%-80% limits\n",
    "    min_max_limits = {'TM1':[[559,563],[588,588]],\n",
    "                        'TM2':[[606,606],[636,637]],\n",
    "                        'TM3':[[654,657],[690,690]],\n",
    "                        'TM4':[[717,717],[741,742]],\n",
    "                        'TM5':[[913,916],[946,948]],\n",
    "                        'TM6':[[1169,1174],[1203,1204]],\n",
    "                        'TM7':[[1246,1248],[1270,1271]],} #based on partial dssp MSA with mapping dssp exp structures representative\n",
    "    if type_gpcr == \"natural\":\n",
    "        min_max_limits_translated_interest = remap_min_max_limits_prot_interest(min_max_limits,aligned_seq_interest)\n",
    "        print(min_max_limits_translated_interest)\n",
    "        refined_TM_regions = refine_TM_regions(min_max_limits_translated_interest,TM_regions)\n",
    "    else:\n",
    "        min_max_limits_translated_parent = remap_min_max_limits_prot_interest(min_max_limits,aligned_seq_interest_ref)\n",
    "        min_max_limits_MSAchimera_parent,aligned_seq_interest_chimera = map_min_max_limitsMSA_chimera(min_max_limits_translated_parent,aligned_seq_interest_ref,seq_chimera)\n",
    "        min_max_limits_translated_interest = remap_min_max_limits_prot_interest(min_max_limits_MSAchimera_parent,aligned_seq_interest_chimera)\n",
    "        refined_TM_regions = refine_TM_regions(min_max_limits_translated_interest, TM_regions)\n",
    "    \n",
    "    structured_regions = [\n",
    "        (\"Nterm\", 1, refined_TM_regions[\"TM1\"][0] - 1),\n",
    "        (\"TM1\", refined_TM_regions[\"TM1\"][0], refined_TM_regions[\"TM1\"][1]),\n",
    "        (\"ICL1\", refined_TM_regions[\"TM1\"][1] + 1, refined_TM_regions[\"TM2\"][0] - 1),\n",
    "        (\"TM2\", refined_TM_regions[\"TM2\"][0],refined_TM_regions[\"TM2\"][1]),\n",
    "        (\"ECL1\", refined_TM_regions[\"TM2\"][1] + 1, refined_TM_regions[\"TM3\"][0] - 1),\n",
    "        (\"TM3\",  refined_TM_regions[\"TM3\"][0], refined_TM_regions[\"TM3\"][1]),\n",
    "        (\"ICL2\",  refined_TM_regions[\"TM3\"][1] + 1,  refined_TM_regions[\"TM4\"][0] - 1),\n",
    "        (\"TM4\",  refined_TM_regions[\"TM4\"][0],refined_TM_regions[\"TM4\"][1]),\n",
    "        (\"ECL2\", refined_TM_regions[\"TM4\"][1] + 1, refined_TM_regions[\"TM5\"][0] - 1),\n",
    "        (\"TM5\", refined_TM_regions[\"TM5\"][0],refined_TM_regions[\"TM5\"][1]),\n",
    "        (\"ICL3\", refined_TM_regions[\"TM5\"][1] + 1, refined_TM_regions[\"TM6\"][0] - 1),\n",
    "        (\"TM6\", refined_TM_regions[\"TM6\"][0],refined_TM_regions[\"TM6\"][1]),\n",
    "        (\"ECL3\", refined_TM_regions[\"TM6\"][1] + 1, refined_TM_regions[\"TM7\"][0] - 1),\n",
    "        (\"TM7\", refined_TM_regions[\"TM7\"][0],refined_TM_regions[\"TM7\"][1]),\n",
    "        (\"H8&Cterm\", refined_TM_regions[\"TM7\"][1] + 1, length_prot)\n",
    "    ]\n",
    "\n",
    "    regions = []\n",
    "    for name, start, end in structured_regions:\n",
    "        regions.append({\n",
    "            \"name\": name,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"reference\": \"DSSP\"\n",
    "        })\n",
    "\n",
    "    return regions\n",
    "\n",
    "\n",
    "print(compute_dssp_TM_regions(\"O95136\",MSA,type_gpcr = \"natural\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutting_pts_2_ss_region(dict_regions,all_regions=None):\n",
    "    translated_regions = []\n",
    "    if isinstance(dict_regions,dict):\n",
    "        for region in dict_regions.keys():\n",
    "            lower_lim = dict_regions[region][0]\n",
    "            upper_lim = dict_regions[region][1]\n",
    "            for ss_region in all_regions:\n",
    "                if lower_lim >= ss_region[\"start\"] and lower_lim <= ss_region[\"end\"]:\n",
    "                    ss_lower_lim = ss_region[\"name\"]\n",
    "                if upper_lim >= ss_region[\"start\"] and upper_lim <= ss_region[\"end\"]:\n",
    "                    ss_upper_lim = ss_region[\"name\"]\n",
    "            translated_regions.append(str(lower_lim)+\"-\"+str(upper_lim)+f\" ({ss_lower_lim}-{ss_upper_lim})\")\n",
    "    elif isinstance(dict_regions,list):\n",
    "        if all_regions:\n",
    "            for section in dict_regions:\n",
    "                info = []\n",
    "                for pos in section:\n",
    "                    for ss_region in all_regions:\n",
    "                        if int(pos) >= ss_region[\"start\"] and int(pos) <= ss_region[\"end\"]:\n",
    "                            ss_pos = ss_region[\"name\"]\n",
    "                            break\n",
    "                    info.append(pos)\n",
    "                    info.append(ss_pos)\n",
    "                translated_regions.append(str(info[0])+\"-\"+str(info[2])+f\" ({info[1]}-{info[3]})\")\n",
    "        else:\n",
    "            for region in dict_regions:\n",
    "                lower_lim = region[0]\n",
    "                upper_lim = region[1]\n",
    "                translated_regions.append(str(lower_lim)+\"-\"+str(upper_lim))\n",
    "    return translated_regions\n",
    "\n",
    "def often_used_cutting_pts(cutting_points_parent,chimera,sequence,all_regions_cutting_pts,related_chimeras,dict_regions):\n",
    "    cutting_pts = chimera[cutting_points_parent]\n",
    "    for region in cutting_pts:\n",
    "        positions=region.split(\" \")[0]\n",
    "        name=region.split(\" \")[1][1:-1]\n",
    "        if positions.split(\"-\")[0] != \"1\":\n",
    "            position = int(positions.split(\"-\")[0])\n",
    "            name_region= name.split(\"-\")[0]\n",
    "            lower_lim = [d for d in dict_regions if d.get(\"name\") == name_region][0][\"start\"]\n",
    "            upper_lim = [d for d in dict_regions if d.get(\"name\") == name_region][0][\"end\"]\n",
    "            a_third = round((upper_lim-lower_lim)/3)\n",
    "            if position < (lower_lim + a_third):\n",
    "                idx = 0\n",
    "            elif position < (lower_lim + 2*a_third):\n",
    "                idx = 1\n",
    "            else:\n",
    "                idx = 2\n",
    "            try:\n",
    "                all_regions_cutting_pts[name_region].append([sequence[position-1]+str(position),idx])\n",
    "            except:\n",
    "                all_regions_cutting_pts[name_region]=[[sequence[position-1]+str(position),idx]]\n",
    "            try:\n",
    "                related_chimeras[sequence[position-1]+str(position)].append(chimera[\"name\"])\n",
    "            except:\n",
    "                related_chimeras[sequence[position-1]+str(position)]=[chimera[\"name\"]]\n",
    "        if positions.split(\"-\")[1] != str(len(sequence)):\n",
    "            position = int(positions.split(\"-\")[1])\n",
    "            name_region= name.split(\"-\")[1]\n",
    "            lower_lim = [d for d in dict_regions if d.get(\"name\") == name_region][0][\"start\"]\n",
    "            upper_lim = [d for d in dict_regions if d.get(\"name\") == name_region][0][\"end\"]\n",
    "            a_third = lower_lim+round((upper_lim-lower_lim)/3)\n",
    "            if position < (lower_lim + a_third):\n",
    "                idx = 0\n",
    "            elif position < (lower_lim + 2*a_third):\n",
    "                idx = 1\n",
    "            else:\n",
    "                idx = 2\n",
    "            try:\n",
    "                all_regions_cutting_pts[name_region].append([sequence[position-1]+str(position),idx])\n",
    "            except:\n",
    "                all_regions_cutting_pts[name_region]=[[sequence[position-1]+str(position),idx]]\n",
    "            try:\n",
    "                related_chimeras[sequence[position-1]+str(position)].append(chimera[\"name\"])\n",
    "            except:\n",
    "                related_chimeras[sequence[position-1]+str(position)]=[chimera[\"name\"]]\n",
    "    return all_regions_cutting_pts,related_chimeras\n",
    "\n",
    "\n",
    "def retrieve_involvement_natural_chimeric_design(uniprot_id,abb_name,sequence,chimeric_design_df):\n",
    "\n",
    "    involvement = []\n",
    "\n",
    "    all_regions_cutting_pts_all = {}\n",
    "    related_chimeras_all = {}\n",
    "    for parent_column_id in ['Reference_id','Target_id']:\n",
    "\n",
    "        #find rows that have uniprot as ref id or target id\n",
    "        designs_parent = chimeric_design_df[chimeric_design_df[parent_column_id] == uniprot_id]\n",
    "\n",
    "        #Info from rows\n",
    "        names_chimeras = designs_parent['Chimera_name'].tolist()\n",
    "        ids_chimeras = designs_parent['Chimera_name_ids'].tolist()\n",
    "        regions_chimera = designs_parent['Chimera_parts'].tolist()\n",
    "        name_target_chimeras = designs_parent['Target_name'].tolist()\n",
    "        id_target_chimeras = designs_parent['Target_id'].tolist()\n",
    "\n",
    "        name_ref_chimeras = designs_parent['Reference_name'].tolist()\n",
    "        id_ref_chimeras = designs_parent['Reference_id'].tolist()\n",
    "\n",
    "        regions_ref_chimeras = designs_parent['Reference_cutting_points'].tolist()\n",
    "        regions_target_chimeras = designs_parent['Target_cutting_points'].tolist()\n",
    "\n",
    "        expression = designs_parent['Expression binary'].tolist()\n",
    "        fct = designs_parent['Function binary'].tolist()\n",
    "\n",
    "        application = designs_parent['Application'].tolist()\n",
    "        type_chimera = designs_parent['Chimera Type (1/2/3)'].tolist()\n",
    "        Gprot = designs_parent['G-protein'].tolist()\n",
    "        Ligand = designs_parent['Ligand'].tolist()\n",
    "        structures = designs_parent['3D structure PDB'].tolist()\n",
    "        biblio = designs_parent['DOI'].tolist()\n",
    "\n",
    "        for i,(name, id) in enumerate(zip(names_chimeras,ids_chimeras)):\n",
    "\n",
    "            sequence_chimera = str(chimeras_record_dict[name].seq)\n",
    "            all_regions = compute_dssp_TM_regions(id,MSA,type_gpcr = \"chimera\",ref_id=id_ref_chimeras[i], seq_chimera = sequence_chimera)\n",
    "            cutting_pt_chimera = cutting_pts_2_ss_region(eval(regions_chimera[i]),all_regions) \n",
    "\n",
    "            all_regions_ref = compute_dssp_TM_regions(id_ref_chimeras[i],MSA,type_gpcr = \"natural\")\n",
    "            cutting_pt_ref = cutting_pts_2_ss_region(eval(regions_ref_chimeras[i]),all_regions_ref)\n",
    "\n",
    "            all_regions_target = compute_dssp_TM_regions(id_target_chimeras[i],MSA,type_gpcr = \"natural\")\n",
    "            cutting_pt_target = cutting_pts_2_ss_region(eval(regions_target_chimeras[i]),all_regions_target)\n",
    "\n",
    "            pharma_name_ref = html.unescape(get_pharma_name(id_ref_chimeras[i],name_ref_chimeras[i]))\n",
    "            pharma_name_target = html.unescape(get_pharma_name(id_target_chimeras[i], name_target_chimeras[i]))\n",
    "\n",
    "            pharma_name_ref_ = pharma_name_ref\n",
    "            pharma_name_target_ = pharma_name_target\n",
    "            if \"receptor\" in pharma_name_ref.lower():\n",
    "                pharma_name_ref_ = pharma_name_ref.replace(\" receptor\",\"\")\n",
    "            if \"receptor\" in pharma_name_target.lower():\n",
    "                pharma_name_target_ = pharma_name_target.replace(\" receptor\",\"\")\n",
    "            pharma_name = pharma_name_ref_ + \" \" + pharma_name_target_ + \" receptor\"\n",
    "            if \"adrenoceptor\" in pharma_name:\n",
    "                pharma_name = pharma_name.replace(\" receptor\",\"\")\n",
    "\n",
    "            if isinstance(structures[i],str):\n",
    "                pdb = structures[i]\n",
    "            else:\n",
    "                pdb = \"\"\n",
    "\n",
    "            if isinstance(Gprot[i],str):\n",
    "                gprot=Gprot[i]\n",
    "            else:\n",
    "                gprot=\"\"\n",
    "\n",
    "            if isinstance(Ligand[i],str):\n",
    "                ligand=Ligand[i]\n",
    "            else:\n",
    "                ligand=\"\"\n",
    "\n",
    "            chimera={\n",
    "            \"name\":name,\n",
    "            \"name_pharma\":pharma_name,\n",
    "            \"id\":ids_chimeras[i],\n",
    "            \"ref\": name_ref_chimeras[i],\n",
    "            \"ref_pharma_name\": pharma_name_ref,\n",
    "            \"target\": name_target_chimeras[i],\n",
    "            \"target_pharma_name\":pharma_name_target,\n",
    "            \"cutting_point_chimera\": cutting_pt_chimera,\n",
    "            \"cutting_point_ref\": cutting_pt_ref,\n",
    "            \"cutting_point_target\": cutting_pt_target,\n",
    "            \"expression_function\": fct[i],\n",
    "            \"type\":type_chimera[i],\n",
    "            \"GprotLigand\": gprot+\" \"+ligand,\n",
    "            \"application\": application[i]+\" \"+pdb,\n",
    "            \"reference\": biblio[i]\n",
    "            }\n",
    "\n",
    "            involvement.append(chimera)\n",
    "\n",
    "            if \"_\".join(name.split(\"_\")[:2]) == abb_name:\n",
    "                all_regions_cutting_pts_all,related_chimeras_all=often_used_cutting_pts(\"cutting_point_ref\",chimera,sequence,all_regions_cutting_pts_all,related_chimeras_all,all_regions_ref)\n",
    "            else: \n",
    "                all_regions_cutting_pts_all,related_chimeras_all=often_used_cutting_pts(\"cutting_point_target\",chimera,sequence,all_regions_cutting_pts_all,related_chimeras_all,all_regions_target)                \n",
    "    \n",
    "    for key,value in all_regions_cutting_pts_all.items():\n",
    "        unique_items = list(map(list, set(map(tuple, value))))\n",
    "        all_regions_cutting_pts_all[key] = unique_items\n",
    "\n",
    "    for key,value in related_chimeras_all.items():\n",
    "        unique_items = list(set(value))\n",
    "        related_chimeras_all[key] = unique_items\n",
    "\n",
    "    return involvement,all_regions_cutting_pts_all,related_chimeras_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pharma_name(uniprotID,abb_name):\n",
    "    #GtoP or gpcrdb_name or pharmacological name\n",
    "    try:\n",
    "        requestURL = f\"https://gpcrdb.org/services/protein/accession/{uniprotID}\"\n",
    "\n",
    "        r = requests.get(requestURL, headers={ \"Accept\" : \"application/json\"})\n",
    "        if not r.ok:\n",
    "            info_entry = None\n",
    "        else:\n",
    "            info_entry = json.loads(r.text)\n",
    "    except:\n",
    "        info_entry = None\n",
    "\n",
    "    if not info_entry is None:\n",
    "        clean_html_tags = re.compile('<.*?>')\n",
    "        pharma_name = re.sub(clean_html_tags, '', info_entry[\"name\"])\n",
    "    else:\n",
    "        pharma_name = abb_name\n",
    "\n",
    "    return pharma_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(list_dictionaries,uniprot_id, descriminator1, descriminator2=None):\n",
    "    # Initialize a dictionary to count occurrences of (start, description) pairs\n",
    "    count = {}\n",
    "\n",
    "    # First pass: Count occurrences of each (start, description) pair\n",
    "    for d in list_dictionaries:\n",
    "        if descriminator2 != None:\n",
    "            identifier = (d[descriminator1], d[descriminator2])\n",
    "        else:\n",
    "            identifier = (d[descriminator1])\n",
    "        if identifier in count:\n",
    "            count[identifier] += 1\n",
    "        else:\n",
    "            count[identifier] = 1\n",
    "\n",
    "    # Initialize a set to track seen (start, description) pairs\n",
    "    seen = set()\n",
    "    # Initialize a list to store the filtered dictionaries\n",
    "    unique_dict_list = []\n",
    "\n",
    "    # Second pass: Filter dictionaries and update \"other_key\" for duplicates\n",
    "    for d in list_dictionaries:\n",
    "        if descriminator2 != None:\n",
    "            identifier = (d[descriminator1], d[descriminator2])\n",
    "        else:\n",
    "            identifier = (d[descriminator1])\n",
    "        if identifier not in seen:\n",
    "            # If this (start, description) pair is a duplicate (appears more than once)\n",
    "            if count[identifier] > 1:\n",
    "                # don't choose randomly the reference, set it to the preferred reference type\n",
    "                if descriminator2 != None:\n",
    "                    d[\"reference\"] = f\"https://iomics.ugent.be/scop3p/index?protein={uniprot_id}\"\n",
    "                else:\n",
    "                    d[\"reference\"] = \"https://www.ebi.ac.uk/pdbe/pisa/\"\n",
    "            # Add it to the seen set\n",
    "            seen.add(identifier)\n",
    "            # Add the dictionary to the unique list\n",
    "            unique_dict_list.append(d)\n",
    "    return unique_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert scientific name UniProt to common name\n",
    "def parse_species_file(file_path):\n",
    "    species_dict = {}\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    scientific_name = None\n",
    "    \n",
    "    for line in lines:\n",
    "        if \"N=\" in line:\n",
    "            scientific_name = line.split(\"N=\")[1].strip()\n",
    "        elif \"C=\" in line and scientific_name:\n",
    "            common_name = line.split(\"C=\")[1].strip()\n",
    "            species_dict[scientific_name] = common_name\n",
    "            scientific_name = None  # Reset for the next entry\n",
    "    \n",
    "    return species_dict\n",
    "\n",
    "file_path = '../data/UniProt_names_scientific_common.txt' \n",
    "species_dict = parse_species_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_predicted_models(structures,uniprotID):\n",
    "\n",
    "    #AF\n",
    "    #check if gpcrdb has updated model\n",
    "    if os.path.exists(f\"../examples/3Dstructures/AF_gpcrdb_2024/{uniprotID}.pdb\"):\n",
    "        af_gpcrdb = f\"file:///examples/3Dstructures/AF_gpcrdb_2024/{uniprotID}.pdb\"\n",
    "        states_AF_json = json.load(open(\"../examples/3Dstructures/AF_gpcrdb_2024/AF_gpcrdb_state.json\",\"r\"))\n",
    "        structures.append({\"value\":\"AlphaFold2\",\"chain\": \"A\",  \"state\":states_AF_json[uniprotID], \"offset\":  0, \"gaps\": [], \"resolution\": \"\", \"method\": \"Predicted\", \"url\": af_gpcrdb, \"reference\":\"GPCRdb\", \"date\":\"2024\"})\n",
    "    else:\n",
    "        #AlphaFold2 DB\n",
    "        structures.append({\"value\":\"AlphaFold2\",\"chain\": \"A\",  \"state\":\"Undetermined\", \"offset\":  0, \"gaps\": [], \"resolution\": \"\", \"method\": \"Predicted\", \"url\": f\"https://alphafold.ebi.ac.uk/files/AF-{uniprotID}-F1-model_v4.pdb\", \"reference\":\"AFDB\", \"date\":\"2022\"})\n",
    "\n",
    "    #AF multistate active\n",
    "    #check if gpcrdb has updated model \n",
    "    if os.path.exists(f\"../examples/3Dstructures/AFms_gpcrdb_2024/Active/{uniprotID}.pdb\"):\n",
    "        af_ms_active = f\"file:///examples/3Dstructures/AFms_gpcrdb_2024/Active/{uniprotID}.pdb\"\n",
    "        structures.append({\"value\":\"AlphaFold2-Multistate Active\",\"chain\": \"A\", \"state\":\"Active\", \"offset\":  0, \"gaps\": [], \"resolution\": \"\", \"method\": \"Predicted\", \"url\": af_ms_active, \"reference\":\"AlphaFold multistate\", \"date\":\"2024\"})\n",
    "    else:\n",
    "        #AlphaFold multistate. Don't have a AF ms for every GPCR (only humans). Need to check if file exist:\n",
    "        if os.path.exists(\"../examples/3Dstructures/AFms_2023/Active/{uniprotID}.pdb\"):\n",
    "            af_ms_active = f\"file:///examples/3Dstructures/AFms_2023/Active/{uniprotID}.pdb\"\n",
    "            structures.append({\"value\":\"AlphaFold2-Multistate Active\",\"chain\": \"A\", \"state\":\"Active\", \"offset\":  0, \"gaps\": [], \"resolution\": \"\", \"method\": \"Predicted\", \"url\": af_ms_active, \"reference\":\"AlphaFold multistate\", \"date\":\"2023\"})\n",
    "\n",
    "    #AF multistate inactive \n",
    "    #check if gpcrdb has updated model \n",
    "    if os.path.exists(f\"../examples/3Dstructures/AFms_gpcrdb_2024/Inactive/{uniprotID}.pdb\"):\n",
    "\n",
    "        af_ms_inactive = f\"file:///examples/3Dstructures/AFms_gpcrdb_2024/Inactive/{uniprotID}.pdb\"\n",
    "        structures.append({\"value\":\"AlphaFold2-Multistate Inactive\",\"chain\": \"A\", \"state\":\"Inactive\", \"offset\":  0, \"gaps\": [], \"resolution\": \"\", \"method\": \"Predicted\", \"url\": af_ms_inactive, \"reference\":\"AlphaFold multistate\", \"date\":\"2024\"})\n",
    "    else:\n",
    "        #AlphaFold multistate. Don't have a AF ms for every GPCR (only humans). Need to check if file exist:\n",
    "        if os.path.exists(f\"../examples/3Dstructures/AFms_2023/Inactive/{uniprotID}.pdb\"):\n",
    "            af_ms_inactive = f\"file:///examples/3Dstructures/AFms_2023/Inactive/{uniprotID}.pdb\"\n",
    "            structures.append({\"value\":\"AlphaFold2-Multistate Inactive\",\"chain\": \"A\", \"state\":\"Inactive\", \"offset\":  0, \"gaps\": [], \"resolution\": \"\", \"method\": \"Predicted\", \"url\": af_ms_inactive, \"reference\":\"AlphaFold multistate\", \"date\":\"2023\"})\n",
    "\n",
    "    return structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_interacting_residues(interacting_residues_list,binders_list,pdbs):\n",
    "    # Reprocessing the data with the updated binders list\n",
    "    interactions = {}\n",
    "\n",
    "    for i, residue_groups in enumerate(interacting_residues_list):\n",
    "        binders = binders_list[i]\n",
    "        pdb_id = pdbs[i]\n",
    "\n",
    "        for j,residue_group in enumerate(residue_groups):\n",
    "            for residue in residue_group:\n",
    "                if residue not in interactions:\n",
    "                    interactions[residue] = [[binders[j],pdb_id]]\n",
    "                else:\n",
    "                    interactions[residue].append([binders[j],pdb_id])\n",
    "\n",
    "    contacts_list =[]\n",
    "    for residue,info in interactions.items():\n",
    "        pdbs = []\n",
    "        binders = []\n",
    "        for data in info:\n",
    "            if not data[0] is None:\n",
    "                binders.append(data[0])\n",
    "                pdbs.append(data[1])\n",
    "        binders=list(set(binders))\n",
    "        if len(binders)>0:\n",
    "            contacts_list.append({\n",
    "                \"start\": residue,\n",
    "                \"end\": residue,\n",
    "                \"type\": \", \".join(binders),  # Ensuring unique binders\n",
    "                \"description\": f\"Inferred from {', '.join(pdbs)}.\",\n",
    "                \"reference\": \"https://www.ebi.ac.uk/pdbe/pisa/\"\n",
    "            })\n",
    "\n",
    "    return contacts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q9BXA5\n",
      "8JPN\n",
      "8JPN\n",
      "8JPP\n",
      "8JPP\n",
      "8WOG\n",
      "8WOG\n",
      "8WP1\n",
      "8WP1\n",
      "8YKV\n",
      "8YKV\n",
      "8YKW\n",
      "8YKW\n",
      "8YKX\n",
      "8YKX\n",
      "[('TM1', (12, 44)), ('TM2', (57, 73)), ('TM3', (92, 125)), ('TM4', (136, 157)), ('TM5', (181, 216)), ('TM6', (228, 259)), ('TM7', (269, 284)), ('TM8', (287, 297)), ('TM9', (300, 308))]\n",
      "{'TM1': [[21, 25], [50, 50]], 'TM2': [[57, 57], [84, 85]], 'TM3': [[91, 94], [125, 125]], 'TM4': [[136, 136], [159, 160]], 'TM5': [[182, 185], [215, 217]], 'TM6': [[223, 228], [256, 257]], 'TM7': [[272, 274], [296, 297]]}\n",
      "{'TM1': (21, 50), 'TM2': (57, 84), 'TM3': (92, 125), 'TM4': (136, 159), 'TM5': (182, 216), 'TM6': (228, 257), 'TM7': (272, 296)}\n"
     ]
    }
   ],
   "source": [
    "for uniprotID, prot_seq_aligned in entry_uniprotID_seq.items():\n",
    "    prot_seq = prot_seq_aligned.replace(\"-\",\"\")\n",
    "    requestURL = f\"https://rest.uniprot.org/uniprotkb/{uniprotID}.json\"\n",
    "    r = requests.get(requestURL, headers={ \"Accept\" : \"application/json\"})\n",
    "\n",
    "    if not r.ok:\n",
    "        r.raise_for_status()\n",
    "        sys.exit()\n",
    "\n",
    "    uniprot_json = json.loads(r.text)\n",
    "\n",
    "    class_ = 'A' #always class A for now\n",
    "    abbreviated_name = uniprot_json[\"uniProtkbId\"]\n",
    "    names = []\n",
    "    names.append({\"value\":uniprot_json['proteinDescription']['recommendedName']['fullName']['value'], \"reference\":\"UniProt\"})\n",
    "    try:\n",
    "        if 'alternativeNames' in uniprot_json['proteinDescription']:\n",
    "            for i in range(len(uniprot_json['proteinDescription']['alternativeNames'])):\n",
    "                names.append({\"value\":uniprot_json['proteinDescription']['alternativeNames'][i]['fullName']['value'],\"reference\":\"UniProt\"})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if \"a\"==\"a\":\n",
    "    # if uniprotID == \"P02699\":\n",
    "    # if not os.path.exists(f'../examples/json_entries/new_mammals_3/{abbreviated_name.upper()}.json'):\n",
    "        print(uniprotID)\n",
    "        species_scientific = uniprot_json['organism']['scientificName']\n",
    "        if species_scientific in species_dict:\n",
    "            species = species_dict[species_scientific]\n",
    "        \n",
    "        #find classfication based on human classification in GPCRdb\n",
    "        #find human ortholog\n",
    "        family = \"\"\n",
    "        subclass_ligand = \"\"\n",
    "        subclass_phylo = \"\"\n",
    "        try:\n",
    "            if listGPCRdb_df[listGPCRdb_df['Uniprot ID'] == uniprotID][\"Phylogenetically-based\"].values[0] == \"A-other\":\n",
    "                if not abbreviated_name.endswith(\"HUMAN\"):\n",
    "                    abbreviated_name_human = (abbreviated_name.split('_')[0]+\"_\"+\"HUMAN\").lower()\n",
    "                    uniprot_id_human = listGPCRdb_df[listGPCRdb_df['Name'] == abbreviated_name_human]['Uniprot ID'].values[0] \n",
    "            else:\n",
    "                uniprot_id_human = uniprotID\n",
    "            family = listGPCRdb_df[listGPCRdb_df['Uniprot ID'] == uniprot_id_human][\"Subclass\"].values[0].rstrip() #need to change this to interpro API for those not on GPCRdb\n",
    "            if \"Class A\" in family:\n",
    "                family = family.replace(\"Class A \",\"\")\n",
    "                family = family[0].upper()+family[1:]\n",
    "            if \"receptors\" in family:\n",
    "                family = family.replace(\"receptors\",\"\").rstrip()\n",
    "            elif \"receptor\" in family:\n",
    "                family = family.replace(\"receptor\",\"\").rstrip()\n",
    "            subclass_ligand = listGPCRdb_df[listGPCRdb_df['Uniprot ID'] == uniprot_id_human][\"Ligand-based\"].values[0].rstrip() #need to change so it works for all mammals (put same as what we have for humans? What with those not on GPCRdb?)\n",
    "            if \"receptors\" in subclass_ligand:\n",
    "                subclass_ligand = subclass_ligand.replace(\"receptors\",\"\").rstrip()\n",
    "            elif \"receptor\" in subclass_ligand:\n",
    "                subclass_ligand = subclass_ligand.replace(\"receptor\",\"\").rstrip()\n",
    "            subclass_phylo = listGPCRdb_df[listGPCRdb_df['Uniprot ID'] == uniprot_id_human][\"Phylogenetically-based\"].values[0]\n",
    "            if \"A-\" in subclass_phylo:\n",
    "                subclass_phylo = subclass_phylo.split('-')[1].rstrip()\n",
    "                subclass_phylo = subclass_phylo[0].upper()+subclass_phylo[1:]\n",
    "        except:\n",
    "            if any(\"olfactory\" in entry[\"value\"].lower() for entry in names): #many of the olfactory humans are not in GPCRdb so no classification. We took the olf human ones from GROSS\n",
    "                family = \"Olfactory\"\n",
    "                subclass_ligand = \"Olfactory\"\n",
    "                subclass_phylo = \"Olfactory\"\n",
    "            else:\n",
    "                print(\"Problem with classification (family,subfamily...)\")\n",
    "\n",
    "        #GtoP or gpcrdb_name or pharmacological name\n",
    "        pharma_name = get_pharma_name(uniprotID,abbreviated_name)\n",
    "        \n",
    "        #Gprot and Barr coupling data from GPCRdb\n",
    "        try:\n",
    "            Gprot_coupling_data,Barr_coupling_data=coupling_Gprot_Barr(uniprotID)\n",
    "        except:\n",
    "            Gprot_coupling_data,Barr_coupling_data=[],[]\n",
    "        \n",
    "        #get structures: pdb id, chain, state and offset\n",
    "        #From the PDB (exp structures)\n",
    "        structures,interacting_residues_list, binders_list, PDBs = retrieve_pdb_dsbonds_interactions(uniprot_json,uniprotID,prot_seq)\n",
    "        structures = retrieve_predicted_models(structures,uniprotID)\n",
    "\n",
    "        #Variations Uniprot\n",
    "        requestURL = f\"https://www.ebi.ac.uk/proteins/api/variation/{uniprotID}\"\n",
    "\n",
    "        r = requests.get(requestURL, headers={ \"Accept\" : \"application/json\"})\n",
    "        if not r.ok:\n",
    "            variations_uniprot_json = None\n",
    "        else:\n",
    "            variations_uniprot_json = json.loads(r.text)\n",
    "\n",
    "        #Pharmacological info from GPCRdb mutants (API service)\n",
    "        requestURL = f\"https://gpcrdb.org/services/mutants/{abbreviated_name.lower()}\"\n",
    "        try:\n",
    "            r = requests.get(requestURL, headers={ \"Accept\" : \"application/json\"})\n",
    "            pharmaco_data_json = json.loads(r.text)\n",
    "        except:\n",
    "            pharmaco_data_json = None\n",
    "\n",
    "        #name endogenous ligands\n",
    "        ligands = []\n",
    "        for value in endogenous_ligands:\n",
    "            if value[\"receptor\"] == abbreviated_name.lower():\n",
    "                # ligands.append({\"value\":  value[\"ligand_name\"].replace(\"&\", \"\").replace(\";\", \"\").replace(\"<sub>\",\"_\"), \"reference\": \"GPCRdb\"})\n",
    "                ligands.append({\"value\":  value[\"ligand_name\"].replace(\"&\", \"\").replace(\";\", \"\").replace(\"<sup>\",\"\").replace(\"</sup>\",\"\").replace(\"<sub>\",\"\").replace(\"</sub>\",\"\"), \"reference\": \"GPCRdb\"})\n",
    "\n",
    "        seen_values = set()\n",
    "        ligands = [item for item in ligands if item[\"value\"] not in seen_values and not seen_values.add(item[\"value\"])]\n",
    "    \n",
    "        microswitches_literature = motifs_microswitches_literature(MSA,uniprotID)\n",
    "\n",
    "        #Retrieve the mutagenesis information from Uniprot related to the entry\n",
    "        if variations_uniprot_json:\n",
    "            mutations_Uniprot = retrieve_mutagenesis_info_Uniprot(variations_uniprot_json,uniprotID)\n",
    "        else:\n",
    "            mutations_Uniprot = []\n",
    "\n",
    "        #Pharmacological data - mutants info from GPCRdb API\n",
    "        if pharmaco_data_json:\n",
    "            pharmaco = retrieve_pharmaco_info_GPCRdb(pharmaco_data_json)\n",
    "        else:\n",
    "            pharmaco = []\n",
    "\n",
    "        #If no conformational biosensor just put confo_biosensor = []\n",
    "        confo_biosensor = []\n",
    "\n",
    "        #find chimeras this parent is involved in\n",
    "        #find the cutting points used for this parent in all designs that we know so we can learn form that\n",
    "        chimeras,all_regions_cutting_pts,related_chimeras=retrieve_involvement_natural_chimeric_design(uniprotID,abbreviated_name,prot_seq,chimeric_design_df)\n",
    "        \n",
    "        cutting_point_values = [] #leave empty if it's a natural\n",
    "        \n",
    "        #retrieve all limits of the secondary structure elements\n",
    "        allregions = compute_dssp_TM_regions(uniprotID,MSA,type_gpcr = \"natural\")\n",
    "\n",
    "        #Add BW numbering based on limits TM regions and x.50 position\n",
    "        BW_numbering=computeBW_numbering(uniprotID,abbreviated_name,allregions,MSA)\n",
    "\n",
    "        #retrieve the residues interacting with ligand/Gprot/Nb/Ab in PDB and link it to region\n",
    "        #Add manually extra IC and EC contacts\n",
    "        #should follow the following structure: list regrouping all dictionaries with 1 dict per contact\n",
    "        #in dictionary: {\"start\":,\"end\",\"type\",\"description\",\"reference\"}\n",
    "        #for the EC contacts the types can be \"orthosteric\",\"allosteric\",\"VHH EC\"\n",
    "        #for the IC contacts the types can be \"G-protein\",\"VHH IC\"\n",
    "        # manual_ICs,manual_ECs = translate_interacting_residues_IC_EC(interacting_residues,binders,PDBs,allregions)\n",
    "        contacts =gather_interacting_residues(interacting_residues_list,binders_list,PDBs)\n",
    "\n",
    "        #Features uniprot PTM, binding site uniptor\n",
    "        features = {}\n",
    "        ligand_BS_uniprot,PTMs_uniprot,disulfide_bonds_uniprot = features_uniprot(uniprot_json,uniprotID)\n",
    "\n",
    "        #Scop3P phosphorylations\n",
    "        PTMs_scop3P=retrieve_PTM_Scop3P(uniprotID)\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        #Abbreviated name\n",
    "        info[\"Abbreviated name\"] = [{\"value\": abbreviated_name.upper(), \"reference\": \"UniProt\"}]\n",
    "\n",
    "        #pharma name\n",
    "        info[\"Pharmacological name\"] = [{\"value\": pharma_name, \"reference\": \"GPCRdb\"}]\n",
    "\n",
    "        #Name\n",
    "        info[\"Name(s)\"] = names\n",
    "\n",
    "        #Uniprot ID\n",
    "        info[\"Uniprot ID\"] = [{\"value\": uniprotID, \"reference\": \"UniProt\"}]\n",
    "\n",
    "        #Species\n",
    "        info[\"Organism\"] =  [{\"value\":species, \"reference\": \"UniProt\"}]\n",
    "\n",
    "        #Class\n",
    "        info[\"Class\"] = [{\"value\":class_, \"reference\": \"GPCRdb\"}]\n",
    "\n",
    "        #Family\n",
    "        info[\"Family\"] = [{\"value\": family, \"reference\": \"GPCRdb\"}]\n",
    "\n",
    "        #Subclass\n",
    "        #Phylogenetically based & Ligand based\n",
    "        info[\"Subclass\"] = {\"Phylogenetically based\": [{\"value\": subclass_phylo, \"reference\": \"10.1124/mol.63.6.1256\"}],\n",
    "                            \"Ligand based\": [{\"value\":subclass_ligand, \"reference\": \"GPCRdb\"}]}\n",
    "\n",
    "        #Endogenous ligand \n",
    "        info[\"Endogenous ligand\"]=ligands\n",
    "\n",
    "        #Gport and Barr coupling data\n",
    "        info[\"G-protein coupling\"]=Gprot_coupling_data\n",
    "        info[\"Beta-arrestin coupling\"] = Barr_coupling_data\n",
    "\n",
    "        #Structures\n",
    "        info[\"Structures\"] = structures\n",
    "\n",
    "        #Info related to chimeric design\n",
    "        info[\"Conformational biosensor\"] = confo_biosensor\n",
    "        info[\"Involvement in chimeric design\"] = chimeras\n",
    "        info[\"Cutting point values\"] = cutting_point_values\n",
    "        info[\"Known cutting points and designs\"] = {\"Known cutting points\":all_regions_cutting_pts,\"Known designs\":related_chimeras}\n",
    "\n",
    "        features['Microswitches'] = microswitches_literature\n",
    "\n",
    "        #remove duplicate PTMs, keep Scop3P ref\n",
    "        PTMs_tot = PTMs_uniprot + PTMs_scop3P\n",
    "        PTMs_tot_unique = remove_duplicates(PTMs_tot,uniprotID,\"start\",descriminator2=\"description\")\n",
    "\n",
    "        features['PTMs'] = PTMs_tot_unique\n",
    "        features['Disulfide bonds'] = disulfide_bonds_uniprot\n",
    "        features['Mutagenesis'] = mutations_Uniprot\n",
    "        features['Pharmacological mutagenesis'] = pharmaco\n",
    "\n",
    "        #remove duplicate contacts, keep uniprot\n",
    "        # Contacts_EC_tot = manual_ECs + ligand_BS_uniprot\n",
    "        # Contacts_EC_unique = remove_duplicates(Contacts_EC_tot,uniprotID,\"start\")\n",
    "\n",
    "        features[\"Contacts\"] = contacts+ligand_BS_uniprot\n",
    "\n",
    "        info[\"Features\"] = features\n",
    "\n",
    "        #Sequence\n",
    "        info[\"Sequence\"] = [{\"value\":prot_seq, \"reference\": \"UniProt\"}]\n",
    "\n",
    "        #BW numbering\n",
    "        info[\"BWnumbering\"] = [{\"value\":BW_numbering, \"reference\": \"MSA\"}] \n",
    "\n",
    "        #Secondary structure info\n",
    "        info[\"Limits regions\"] = allregions\n",
    "\n",
    "        #Gather info that could be useful for chimeric design\n",
    "        known_info = []\n",
    "        if len(confo_biosensor) > 0:\n",
    "            known_info.append({\"value\": \"Confo biosensor\"})\n",
    "        if len(chimeras) > 0:\n",
    "            known_info.append({\"value\": \"Parent chimera\"})\n",
    "            info[\"entryType\"] =\"parent\"\n",
    "        else:\n",
    "            info[\"entryType\"] =\"natural\"\n",
    "\n",
    "        info[\"Known info chimeric design\"] = known_info\n",
    "\n",
    "        json.dump(info, open(f'../examples/json_entries/{abbreviated_name.upper()}.json', 'w'), indent=2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
